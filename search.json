[{"title":"歌多的人得來個專屬歌單","url":"/posts/5414d98e.html","content":"\n# [Shirley](https://music.163.com/playlist?id=8773564776&userid=622029058 \"Shirley\")\n> &emsp;&emsp;啊啊啊，大老王和老関一仙一妖！\n\n{% meting \"8773564776\" \"netease\" \"playlist\" \"autoplay \" \"mutex:true\" \"listmaxheight:500px\" \"preload:auto\" \"theme:#ad7a86\"  \"loop:all\" \"order:random\" %}\n\n# [英皇姐妹花](https://music.163.com/playlist?id=9539208867&userid=622029058 \"英皇姐妹花\")\n> &emsp;&emsp;容祖兒與張靜萱，正所谓“英皇姐妹花，永遠不分家”！\n\n# [如果我是陳奕迅](https://music.163.com/playlist?id=9539223621&userid=622029058 \"如果我是陳奕迅\")\n# [厨子回來唱歌吧](https://music.163.com/playlist?id=8815115832&userid=622029058 \"厨子回來唱歌吧\")\n> &emsp;&emsp;大陳小謝也想放在一起的，但是歌也太多了，那就各自爲營吧。\n\n# [浦铭心与董折](https://music.163.com/playlist?id=9539209029&userid=622029058 \"浦铭心与董折\")\n> &emsp;&emsp;Juno 和 Kay媽一起出更多的企劃吧，求求你們了！","tags":["王菲","關淑怡","容祖兒","張敬軒","陳奕迅","謝霆鋒","麥浚龍","謝安琪"],"categories":["playlist"]},{"title":"心水粤语歌合集","url":"/posts/b15178c5.html","content":"\n# [天王天后](https://music.163.com/playlist?id=8793501671&userid=622029058 \"天王天后\")\n\n# [粵語女聲](https://music.163.com/playlist?id=8774302199&userid=622029058 \"粵語女聲\")\n\n# [粵語男聲](https://music.163.com/playlist?id=8777194507&userid=622029058 \"粵語男聲\")\n\n# [粵語組合](https://music.163.com/playlist?id=8777116105&userid=622029058 \"粵語組合\")\n\n# [系列X部曲](https://music.163.com/playlist?id=8774473006&userid=622029058 \"系列X部曲\")\n{% meting \"8774473006\" \"netease\" \"playlist\" \"autoplay \" \"mutex:true\" \"listmaxheight:500px\" \"preload:auto\" \"theme:#ad7a86\"  \"loop:all\" \"order:random\" %}\n\n# [獨唱不和合唱](https://music.163.com/playlist?id=8774354711&userid=622029058 \"獨唱不和合唱\")","tags":["天王天后","粵語系列","男聲女聲","組合樂隊","對唱合唱"],"categories":["playlist"]},{"title":"PostgreSQL 压力测试方案","url":"/posts/1eac6ba4.html","content":"\n<!-- block -->\n> &emsp;&emsp;按照领导要求，最近任务重点聚焦于集群性能，就再次把 PostgreSQL 这块任务分给了我。那好吧，我只能再次从各个网站上找找合适的方案了。\n<!-- block -->\n\n# pgbench\n## 新建数据库\n```bash\nPGPASSWORD=Teco@135 psql -U postgres -h 10.244.88.146\ncreate database pgbench;\n```\n## 初始化数据\n```bash\n# 5000w条记录\nPGPASSWORD=Teco@135 pgbench -h 10.244.88.146 -U postgres -i -F 100 -s 500 pgbench\n```\n\n## 压测命令\n```bash\n# 1分钟100客户端\nPGPASSWORD=Teco@135 pgbench -h 10.244.88.146 -U postgres -c 100 -S -T 60 pgbench\n```\n\n```bash\n# 2分钟300客户端\nPGPASSWORD=Teco@135 pgbench -h 10.244.88.146 -U postgres -c 300 -S -T 120 pgbench\n```\n\n## 简单总结\n> PgSQL自带，简单易用，但需要自己写脚本\n\n# sysbench\n[https://github.com/akopytov/sysbench.git](https://github.com/akopytov/sysbench.git)\n## 安装\n```bash\nyum/apt/dnf install sysbench -y\n```\n## 新建数据库\n```bash\nPGPASSWORD=Teco@135 psql -U postgres -h 10.244.88.146\ncreate database sysbench;\n```\n## 初始化数据\n```bash\n# 创建2张表，每张表个100万行，8线程\nsysbench \\\n  --threads=8 \\\n  --time=30 \\\n  --report-interval=1 \\\n  select_random_ranges \\\n  --db-driver=pgsql \\\n  --pgsql-host=10.244.88.146 \\\n  --pgsql-port=5432 \\\n  --pgsql-user=postgres \\\n  --pgsql-password=Teco@135 \\\n  --pgsql-db=sysbench \\\n  --tables=2 \\\n  --table-size=1000000 \\\n  prepare\n```\n\n## 压力读\n```bash\n# 压测2分钟，每隔10s打印信息，8线程，范围查\nsysbench \\\n  --threads=8 \\\n  --time=120 \\\n  --report-interval=10 \\\n  select_random_ranges \\\n  --db-driver=pgsql \\\n  --pgsql-host=10.244.88.146 \\\n  --pgsql-port=5432 \\\n  --pgsql-user=postgres \\\n  --pgsql-password=Teco@135 \\\n  --pgsql-db=sysbench \\\n  --tables=2 \\\n  --table-size=1000000 \\\n  run\n```\n\n## 压力删\n```bash\n# 压测2分钟，每隔10s打印信息，8线程，删除\nsysbench \\\n  --threads=8 \\\n  --time=120 \\\n  --report-interval=10 \\\n  oltp_delete \\\n  --db-driver=pgsql \\\n  --pgsql-host=10.244.88.146 \\\n  --pgsql-port=5432 \\\n  --pgsql-user=postgres \\\n  --pgsql-password=Teco@135 \\\n  --pgsql-db=sysbench \\\n  --tables=2 \\\n  --table-size=1000000 \\\n  run\n```\n\n## 清理压测数据\n```bash\nsysbench \\\n  --threads=8 \\\n  --time=120 \\\n  --report-interval=10 \\\n  oltp_delete \\\n  --db-driver=pgsql \\\n  --pgsql-host=10.244.88.146 \\\n  --pgsql-port=5432 \\\n  --pgsql-user=postgres \\\n  --pgsql-password=Teco@135 \\\n  --pgsql-db=sysbench \\\n  --tables=2 \\\n  --table-size=1000000 \\\n  cleanup\n```\n\n## 简单总结\n> 安装和使用简单，命令行操作即可，学习成本较低\n> 除了数据库之外，还可以测试CPU/MEM/DISK等\n\n# BenchmarkSQL\n[https://benchmarksql.readthedocs.io/](https://benchmarksql.readthedocs.io/)\n## 下载编译\n[https://zhuanlan.zhihu.com/p/670725309](https://zhuanlan.zhihu.com/p/670725309)\n## 新建数据库\n```bash\nPGPASSWORD=Teco@135 psql -U postgres -h 10.244.88.146\ncreate database benchmarksql;\n```\n## 修改配置\n[Properties file - BenchmarkSQL](https://benchmarksql.readthedocs.io/en/latest/PROPERTIES/#benchmarksql-properties-files)\n```bash\ncd /data/benchmarksql/benchmarksql-master/target/run\ncp sample.postgresql.properties demo.pgsql.properties\nvim demo.pgsql.properties\n```\n```properties\ndb=postgres\ndriver=org.postgresql.Driver\napplication=Generic\nconn=jdbc:postgresql://10.244.88.146:5432/benchmarksql\nuser=postgres\npassword=Teco@135\n```\n## 初始化数据\n```bash\n./runDatabaseBuild.sh demo.pgsql.properties\n```\n\n## 压力测试\n```bash\n./runBenchmark.sh demo.pgsql.properties\n```\n\n## 清理压测数据\n```bash\n./runDatabaseDestroy.sh demo.pgsql.properties\n```\n\n## 修改测试参数\n```properties\nsutThreads=16\nmaxDeliveryBGThreads=12\nmaxDeliveryBGPerWarehouse=1\nrunMins=1\nrampupMins=10\nrampupSUTMins=5\nrampupTerminalMins=5\nreportIntervalSecs=10\nrestartSUTThreadProbability=0.0\n```\n## 生成压测报告\n```bash\n# 需要 python3，安装 numpy 和 matplotlib，最后生成 html 文件\n./generateReport.py --resultdir=my_result_2024-01-19_144212/\n```\n## 简单总结\n> - 有其他依赖项，且配置项较多\n> - 功能较为全面，有现成脚本直接测试\n\n\n","tags":["数据库","压力测试","PostgreSQL"],"categories":["devops"]},{"title":"ETCD压力测试方案","url":"/posts/a8fa8272.html","content":"\n<!-- block -->\n> &emsp;&emsp;按照领导要求，最近任务重点聚焦于集群性能，就把 etcd 这块任务分给了我。那好吧，我只能感谢指路原帖：https://blog.csdn.net/CrazyLoser000/article/details/131315187\n<!-- block -->\n\n# 编译安装benchmark工具\n[参考官方github](https://github.com/etcd-io/etcd/tree/main/tools/benchmark)\n# 载入etcd集群环境变量\n```bash\nSSL_PATH=\"/etc/kubernetes/ssl\"\nETCD_CA_CERT=\"${SSL_PATH}/ca.pem\"\nETCD_CERT=\"${SSL_PATH}/etcd.pem\"\nETCD_KEY=\"${SSL_PATH}/etcd-key.pem\"\nHOST_1=https://10.8.20.28:2379\nHOST_2=https://10.8.20.32:2379\nHOST_3=https://10.8.20.38:2379\n```\n\n# 验证写入\n```bash\nYOUR_KEY=foo\n\nETCDCTL_API=3 etcdctl --endpoints=${HOST_1},${HOST_2},${HOST_3} --cacert=\"${ETCD_CA_CERT}\" --cert=\"${ETCD_CERT}\" --key=\"${ETCD_KEY}\" put $YOUR_KEY bar\n```\n\n# 获取leader\n```bash\nsource etcd.env\n\nETCDCTL_API=3 etcdctl -w table \\\n  --cacert=\"${ETCD_CA_CERT}\" --cert=\"${ETCD_CERT}\" --key=\"${ETCD_KEY}\" \\\n  --endpoints=${HOST_1},${HOST_2},${HOST_3} endpoint status\n```\n# 写测试\n```bash\nbenchmark \\\n  --endpoints=${HOST_1} \\\n  --cacert=\"${ETCD_CA_CERT}\" --cert=\"${ETCD_CERT}\" --key=\"${ETCD_KEY}\" \\\n  --target-leader --conns=1 --clients=1 \\\n  put --key-size=8 --sequential-keys --total=10000 --val-size=256\n```\n\n```bash\nbenchmark \\\n  --endpoints=${HOST_1} \\\n  --cacert=\"${ETCD_CA_CERT}\" --cert=\"${ETCD_CERT}\" --key=\"${ETCD_KEY}\" \\\n  --target-leader --conns=100 --clients=1000 \\\n  put --key-size=8 --sequential-keys --total=100000 --val-size=256\n```\n\n```bash\nbenchmark \\\n  --endpoints=${HOST_1},${HOST_2},${HOST_3} \\\n  --cacert=\"${ETCD_CA_CERT}\" --cert=\"${ETCD_CERT}\" --key=\"${ETCD_KEY}\" \\\n  --conns=100 --clients=1000 \\\n  put --key-size=8 --sequential-keys --total=100000 --val-size=256\n```\n# 读测试\n```bash\nbenchmark \\\n  --endpoints=${HOST_1},${HOST_2},${HOST_3} \\\n  --cacert=\"${ETCD_CA_CERT}\" --cert=\"${ETCD_CERT}\" --key=\"${ETCD_KEY}\" \\\n  --conns=1 --clients=1 \\\n  range $YOUR_KEY --consistency=l --total=10000\n```\n\n```bash\nbenchmark \\\n  --endpoints=${HOST_1},${HOST_2},${HOST_3} \\\n  --cacert=\"${ETCD_CA_CERT}\" --cert=\"${ETCD_CERT}\" --key=\"${ETCD_KEY}\" \\\n  --conns=1 --clients=1 \\\n  range $YOUR_KEY --consistency=s --total=10000\n```\n\n```bash\nbenchmark \\\n  --endpoints=${HOST_1},${HOST_2},${HOST_3} \\\n  --cacert=\"${ETCD_CA_CERT}\" --cert=\"${ETCD_CERT}\" --key=\"${ETCD_KEY}\" \\\n  --conns=100 --clients=1000 \\\n  range $YOUR_KEY --consistency=l --total=100000\n```\n\n```bash\nbenchmark \\\n  --endpoints=${HOST_1},${HOST_2},${HOST_3} \\\n  --cacert=\"${ETCD_CA_CERT}\" --cert=\"${ETCD_CERT}\" \\\n  --key=\"${ETCD_KEY}\" --conns=100 --clients=1000 \\\n  range $YOUR_KEY --consistency=s --total=100000\n```\n","tags":["ETCD","数据库","压力测试"],"categories":["devops"]},{"title":"K8S使用问题","url":"/posts/f5b43e90.html","content":"\n<!-- block -->\n> &emsp;&emsp;恍恍惚惚 `K8S`也用了不少时间了，之前遇到一些问题也没有记录下来，每次都是百度解决，觉得有些可惜和浪费时间，索性借机整理一下吧，不定期更新。\n<!-- block -->\n\n# hostname导致的calico网络问题\n\n## 问题记录\ncalico在部署过程中注意提前规划主机名称，如果默认为主机名称localhost，会导致coredns不可用\n\n## 影响范围\ncalico节点正常启动，coredns域名无法解析\n\n## 解决过程\n1. 通过k8s查看pod中通过域名进行访问时，发现无法解析域名\n2. node-local-dns提示10.244.102.218:53: i/o timeout\n3. 通过更改其他域名服务器问题也无法解决\n4. calicoctl get nodes命令查看节点只有一个名称localhost，判断为主机名称导致的该问题\n5. 更改主机名称，将calico配置文件的主机名称和hostname保持一致\n6. 删除etcd保存的localhost数据，重启calico即可\n```bash\netcdctl get / --prefix --keys-only | grep localhost\netcdctl get / --prefix --keys-only | grep localhost | xargs -I {} etcdctl del {}\n```\n\n# CoreDNS无法启动\n\n## 问题记录\ncoredns无法启动成功，无报错日志,健康检查出现503错误\n\n## 影响范围\n所有服务无法通过域名进行访问\n\n## 解决过程:\n1. 查看日志发现没有任何报错，只有INFO级别的日志\n[INFO] plugin/ready: Still waiting on: \"kubernetes\"\n2. 看日志是连接k8s集群没有成功，于是从以下几个方面排查<br>\n (1) 权限问题，查看关于coredns的ClusterRole，权限配置都正确<br>\n (2) 排查/etc/resolve.conf的问题，在不同的nameserver下依然出现该问题<br>\n (3) 将日志级别调整到debug依然打印不出错误信息\n3. 突然想到只打印两行日志，pod就被kill掉是不是有资源限制导致，于是查看果然资源中内存限制到了300Mi\n4. 将限制调整为1024Mi成功启动服务","tags":["K8S","容器","CNI插件","容器网络"],"categories":["devops"]},{"title":"K8S使用Multus-cni实现pod第二网络","url":"/posts/bcaebdb7.html","content":"\n<!-- block -->\n> &emsp;&emsp;最近因工作需要，需要在 `K8S` 中让 `pod` 除了默认 `cni` （`flannel` 或者 `calico` ）之外，使用 第二网络通信，调研了一下  `Multus-cni`这个插件，配合 rdma ，记录一下使用过程。\n<!-- block -->\n\n# 安装 RDMA\n> &emsp;&emsp;按照官方 [k8s-rdma-shared-dev-plugin](https://github.com/Mellanox/k8s-rdma-shared-dev-plugin) 文档操作即可，一般没有什么问题，我这边的改动，是把配置文件中的 selector 改成了 \"drivers\": [\"mlx5_core\"]，避免不同节点 IB网卡名称不同的问题（也试过修改 vendors ,但是没有成功）\n\n# 安装 Multus-cni\n> &emsp;&emsp;按照官方  [multus-cni](https://github.com/k8snetworkplumbingwg/multus-cni/) 文档操作即可，一般也没有什么问题<br>\n> &emsp;&emsp;除了安装 multus-cni 外，还需要装一个 `macvlan` 的可执行文件，参考项目 [cni](https://github.com/containernetworking/cni)\n\n# 验证是否成功\n\n```bash\n# 可以看到自动生成网络配置\nls -l /etc/cni/net.d\n\n-rw------- 1 root root  954 Oct 30 17:26 00-multus.conf\n-rw-r--r-- 1 root root  858 Oct 30 17:26 10-calico.conflist\n-rw------- 1 root root 3045 Oct 30 17:26 calico-kubeconfig\ndrwxr-xr-x 2 root root   54 Oct 30 17:26 calico-tls\ndrwxr-xr-x 2 root root   31 Oct 30 17:26 multus.d\n```\n\n```bash\n# 第二网络会以第一网络为基础（即k8s默认cni，flannel或者calico或其他）\ncat /etc/cni/net.d/00-multus.conf\n\n{\n        \"cniVersion\": \"0.3.1\",\n        \"name\": \"multus-cni-network\",\n        \"type\": \"multus\",\n        \"capabilities\": {\"bandwidth\":true,\"portMappings\":true},\n        \"cniConf\": \"/host/etc/cni/multus/net.d\",\n        \"kubeconfig\": \"/etc/cni/net.d/multus.d/multus.kubeconfig\",\n        \"delegates\": [\n                {\"cniVersion\":\"0.3.1\",\"name\":\"k8s-pod-network\",\"plugins\":[{\"etcd_ca_cert_file\":\"/etc/kubernetes/ssl/ca.pem\",\"etcd_cert_file\":\"/etc/calico/ssl/calico.pem\",\"etcd_endpoints\":\"https://10.8.20.1:2379,https://10.8.20.2:2379,https://10.8.20.5:2379\",\"etcd_key_file\":\"/etc/calico/ssl/calico-key.pem\",\"ipam\":{\"type\":\"calico-ipam\"},\"kubernetes\":{\"kubeconfig\":\"/etc/cni/net.d/calico-kubeconfig\"},\"log_file_path\":\"/var/log/calico/cni/cni.log\",\"log_level\":\"info\",\"mtu\":1500,\"policy\":{\"type\":\"k8s\"},\"type\":\"calico\"},{\"capabilities\":{\"portMappings\":true},\"snat\":true,\"type\":\"portmap\"},{\"capabilities\":{\"bandwidth\":true},\"type\":\"bandwidth\"}]}\n        ]\n}\n```\n\n# 使用第二网络\n\n> &emsp;&emsp;默认的 NetworkAttachmentDefinition 中直接配置了 ip range，但是这样会导致 不同节点上两个 pod 的第二网络会使用同一个 ip，具体可以查看我提的 [issue](https://github.com/k8snetworkplumbingwg/multus-cni/issues/1150)，所以我这边采用了配置文件的方式，手动配置了每个节点第二网络的 ip range。\n\n## 自定义网络接口\n\n```yaml\napiVersion: \"k8s.cni.cncf.io/v1\"\nkind: NetworkAttachmentDefinition\nmetadata:\n  # pod中引入网路接口名称\n  name: macvlan-conf\n  # 在哪个名称空间下创建，建议pod在哪使用就在哪个名称空间下创建\n  namespace: kube-system \n```\n\n## 应用网络接口\n\n```yaml\nkubectl apply -f macvlan-conf.yml\n```\n\n## 在每一个节点上自定义网络接口的IP池\n\n```bash\nmkdir /etc/cni/multus/net.d/ -p\n\ncd /etc/cni/multus/net.d/\n\nvim macvlan.conf\n\n{\n  \"cniVersion\": \"0.3.1\",\n  \"type\": \"macvlan\",\n  # name和上一步网络接口的名称一致\n  \"name\": \"macvlan-conf\",\n  # 主机上网卡名称，\n  \"master\": \"ens3\",\n  \"mode\": \"bridge\",\n  \"ipam\": {\n      \"type\": \"host-local\",\n      \"ranges\": [\n          [ {\n               \"subnet\": \"11.10.0.0/16\",\n               \"rangeStart\": \"11.10.1.10\",\n               \"rangeEnd\": \"11.10.1.220\"\n          } ]\n      ]\n  }\n}\n```\n\n> 注意：每一个节点的配置文件中 rangeStart 和 rangeEnd 需要配置成不同的网段，例如 节点1配置的是 11.10.1.xxx，节点2上配置成 11.10.2.xxx，可跟随节点数量以此类推\n\n## 验证pod是否能使用第二网络\n\n创建 test-pod.yml\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: mofed-test-ctr\n  # 通过注解的方式，来使用第二网络的自定义接口，并设置网卡名称\n  annotations:\n    k8s.v1.cni.cncf.io/networks: kube-system/macvlan-conf@ib0,kube-system/macvlan-conf@ib1\nspec:\n  restartPolicy: OnFailure\n  containers:\n  - image: mellanox/rping-test\n    name: mofed-test-ctr\n    imagePullPolicy: IfNotPresent\n    securityContext:\n      capabilities:\n        add: [ \"IPC_LOCK\" ]\n    resources:\n    # 上一步rdma插件部署中使用的资源名称\n      limits:\n        rdma/cx5_bond_shared_devices_a: 1\n      requests:\n        rdma/cx5_bond_shared_devices_a: 1\n    command:\n    - sh\n    - -c\n    - |\n      ls -l /dev/infiniband /sys/class/infiniband /sys/class/net\n      sleep 1000000\n```\n\n应用测试 pod\n\n```yaml\nkubectl apply -f test-pod.yml\n```\n\n进入 `pod` 中使用 `ifconfig` 可以看到有两个网卡和 ip\n\n# 可能出现的问题\n\n## POD中无法添加第二网络\n\n> &emsp;&emsp;`multus` 的二进制 `cni` 插件一定要在 `kubelet` 使用的 `cni` 插件目录下(默认都是在 `/opt/cni/bin` ，而使用 `kubeasz` 部署的 `k8s`，用的是 `/opt/kube/bin` 这个目录，因此发现了该问题)，否则 `pod` 虽然能够正常启动，但是无法使用第二网络（该日志会在 `kubelet` 中显示）,\n\n## 缺少 macvlan\n> &emsp;&emsp; 如果启动 `pod` 时出现缺少 `macvlan` 的日志，如前文所说下载安装 https://github.com/containernetworking/cni/releases 里的可执行文件即可\n\n## 网卡名称问题\n\n> &emsp;&emsp;创建的 `NetworkAttachmentDefinition` 的网卡名称需要在节点下存在，否则的话会提示网络设备找不到。","tags":["K8S","CNI插件","容器网络"],"categories":["devops"]},{"title":"Jenkins流水线脚本","url":"/posts/bbf2ea7a.html","content":"<!-- block -->\n>&emsp;&emsp;作为一名合格的运维工程师，使用自动化工具来实现应用的 `CI/CD` 必不可少。在实际工作中，一般使用 `Jenkins` 并撰写相应的 `pipeline` 来完成此类工作。针对不同的情况，流水线的组合也不尽相同，例如前后端编译方式不一样、一个 `git` 地址里实际包含多个项目等等。对此，我主要总结成几个案例以供分析（流水线中一些 `git` 项目地址和账户凭据以 xxxxx 代替）。\n<!-- block -->\t\n\n# 案例一：构建java后端项目\n\n```groovy\npipeline {\n\tagent any \n\n    // 一些常用选项\n    options {\n      // 构建过程显示时间戳\n      timestamps()\n      // 构建事件保留策略\n      buildDiscarder logRotator(\n        artifactDaysToKeepStr: '30', \n        artifactNumToKeepStr: '5', \n        daysToKeepStr: '3', \n        numToKeepStr: '3'\n      )\n  \t}\n  \n    // 声明一些变量，这样针对其他类似项目修改变量即可\n    environment {\n        // git相关，拉取项目代码的凭据需要提前配置\n        GIT_BRANCH = 'master'\n        GIT_CR_ID = 'xxxxxxx'  \n        GIT_URL = 'http://xxx.git'\n        // 项目名称，不一定和git中一致，主要是为了区分\n        APP_NAME = 'xxx'\n        APP_SPACE = '/opt'\n    }\n   \n    stages {\n        stage('初始化工作目录') {\n            steps {\n                echo \"STEP1: 初始化工作目录\"\n                deleteDir()\n            }\n        }\n        \n        stage('拉取项目代码') {\n            steps {\n                echo \"STEP2: 拉取项目代码\"\n                git branch: \"${GIT_BRANCH}\", credentialsId: \"${GIT_CR_ID}\", url: \"${GIT_URL}\"   \n            }\n        }\n      \n        stage('项目编译') {\n                echo \"STEP3: 项目编译\"\n                sh 'mvn -U -am clean package -Dmaven.test.skip=true -DskipTests=true'\n        }\n      \n        stage('准备项目包') {\n            steps {\n                echo \"STEP4: 准备项目包\"\n                // 获取maven编译出来的war包并放到指定目录（主要是因为本人强迫症，不喜欢一个目录里有不必要的东西）\n                sh '''\n                    VERFILE=$(ls ${WORKSPACE}/target -A | grep war | grep -v original)\n                    if [ ! -d ${APP_SPACE}/${GIT_BRANCH} ]; then mkdir -p ${APP_SPACE}/${GIT_BRANCH} ; fi\n                    cp ${WORKSPACE}/target/${VERFILE} ${APP_SPACE}/${GIT_BRANCH}/${APP_NAME}.war\n                '''\n            }\n        }\n      \n        stage('生成镜像') {\n            steps {\n                echo \"STEP5: 生成镜像\"\n                sh '''\n                    cd ${APP_SPACE}/${GIT_BRANCH}\n                    IMGAETAG=hub.xxxx.cn/${APP_NAME}:${BUILD_NUMBER}\n                    docker build -t ${IMGAETAG} -f ${APP_NAME}_dockerfile .\n                    cat /root/hubyozocloud | docker login -u xxx hub.xxxx.cn --password-stdin\n                    docker push ${IMGAETAG}\n                    docker rmi ${IMGAETAG}\n                '''\n            }\n        }\n    }\n    \n    // 配置钉钉通知\n    post{\n        always{\n            wrap([$class: 'BuildUser']) {\n                buildDescription \"Build Person: ${BUILD_USER}\\nBuild Task: ${JOB_NAME}\"\n            }            \n        }\n        success{\n            dingtalk (\n                // 机器人ID\n                robot: 'xxxxxxxxxxxxxxx',\n                type: 'LINK',\n                // 如果不@所有人的话，配置指定@的人\n                at: ['xxxxxxxxxx'],\n                atAll: false,\n                title: 'Test',\n                text: ['构建成功！'],\n                // 可以配置Jenkins地址\n                messageUrl: 'xxxxxxxx',\n                picUrl: 'https://img2.baidu.com/it/u=1641501037,1168798543&fm=15&fmt=auto&gp=0.jpg'\n            )\n        }\n        failure{\n            dingtalk (\n                // 机器人ID\n                robot: 'xxxxxxxxxxxxxxx',\n                type: 'LINK',\n                // 如果不@所有人的话，配置指定@的人\n                at: ['xxxxxxxxxx'],\n                atAll: false,\n                title: 'Test',\n                text: ['构建失败！'],\n                // 可以配置Jenkins地址\n                messageUrl: 'xxxxxxxx',\n                picUrl: 'https://img2.baidu.com/it/u=1641501037,1168798543&fm=15&fmt=auto&gp=0.jpg'\n            )\n        }\n    }  \n}\n```\n\n## 总结说明\n\n> 这是本人刚接触 `jenkins` 时候撰写的 `pipeline` (一些配置项可以看注释)，主要是根据前人留下来的模版，尝试添加 `docker build`（实际写得很粗糙），并且把可以在界面上配置的一些选项和钉钉通知也转换在脚本里（这样就完全可以不依赖界面配置，但实际也没有必要，纯粹是早期为了学习）\n\n# 案例二：构建node前端项目\n\n```groovy\npipeline {\n    // 指定jenkins从节点\n    agent {\n        label 'agent slave1'\n    }\n\n    // 还是习惯声明一些常用并可以修改的变量\n    environment {\n        // git相关\n        GIT_URL = 'xxxxxxx'\n        GIT_CR_ID = 'xxxxx'\n        GIT_BRANCH_TAG = 'develop'\n        // 服务相关\n        APP_NAME = 'xxx'\n        APP_ALL=\"a,b,c,d\"\n        // 镜像相关\n        HUB_URL = 'xxxxxx'\n        PROJECT_NAME = 'project'\n        ARCH = 'x86_64'\n        ENV = 'uat'\n        // 部署/更新相关\n        HOST = '192.168.20.10'\n        BASE_PATH = '/opt'\n    }\n   \n    stages {\n        stage('ClearDir') {\n            steps {\n                deleteDir()\n            }\n        }\n        \n        stage('Get Code') {\n            steps {\n                script {\n                    sh 'rm -rf build-tmp'\n                    // 涉及到不同git项目，写了个循环\n                    for ( app_name in APP_ALL.tokenize(\",\") ){\n                        checkout([$class: 'GitSCM', \n                            branches: [[name: \"${GIT_BRANCH_TAG}\"]], \n                            doGenerateSubmoduleConfigurations: false, \n                            extensions: [[$class: 'RelativeTargetDirectory',relativeTargetDir: \"${app_name}\"]], \n                            submoduleCfg: [], \n                            userRemoteConfigs: [[credentialsId: GIT_CR_ID, url: GIT_URL+ \"${app_name}\" +'.git']]\n                        ])\n                    }    \n                }       \n            }\n        }\n\n        stage('Compile') {\n            steps {\n                script {\n                    sh 'mkdir build-tmp'\n                    // 同理循环，编译多个前端项目包\n                    for ( app_name in APP_ALL.tokenize(\",\") ){\n                        dir( \"${WORKSPACE}/\" + app_name ){\n                            // 使用docker编译前端项目\n                            docker.image('node:16.17.0-alpine3.16').inside('-v /etc/hosts:/etc/hosts -v /home/build/.yarnrc:/.yarnrc -v /home/build/.docmiddle_npmrc:/.npmrc -v /home/build/.docmiddle_yarn:/.cache/yarn  --network host') {\n                                sh '''\n                                  npm config set registry https://registry.npmmirror.com/\n                                  yarn config set registry https://registry.npmmirror.com/\n                                  yarn install\n                                  yarn build\n                                '''\n                            }\n                            // 生成项目包\n                            sh 'cp -ar dist '+ app_name\n                            sh 'tar zcpf ' + app_name + '.tar.gz '+ app_name +'/'\n                            sh 'mv '+ app_name + '.tar.gz ../build-tmp/'\n                        }                   \n                    }\n                }\n            }\n        }\n\n        stage('Build') {\n            steps {\n                // 将多个静态资源包构建一个前端镜像\n                script {\n                    dir( \"${WORKSPACE}/build-tmp\"){\n                        docker.withRegistry(\"https://${HUB_URL}\", '7cd81781-9c42-479c-9ed2-4e26d272520c') {\n                            def customImage=docker.build(\"${HUB_URL}/${PROJECT_NAME}/${ARCH}/${ENV}/${APP_NAME}:${GIT_BRANCH_TAG}\")\n                            customImage.push()\n                        }\n                        sh 'docker rmi ${HUB_URL}/${PROJECT_NAME}/${ARCH}/${ENV}/${APP_NAME}:${GIT_BRANCH_TAG}'\n                    }\n                }\n            }\n        }\n\n        stage('Update') {\n            steps {\n                // 撰写playbook并使用ansible插件更新项目\n                script {\n                    app_image = '${HUB_URL}/${PROJECT_NAME}/${ARCH}/${ENV}/${APP_NAME}:${GIT_BRANCH_TAG}'\n                    Map<String,String> extra_vars = [ 'host': '${HOST}', 'app_name': '${APP_NAME}', 'app_image': app_image, 'base_path': '${BASE_PATH}' ]\n                    ansiblePlaybook(\n                        playbook: '/etc/ansible/scripts/docker_image.yml', \n                        inventory: '/etc/ansible/hosts', \n                        extraVars: extra_vars\n                    )\n                }\n            }\n        }\n    }\n}\n```\n\n## 总结说明\n\n> 这个前端项目的镜像由4个静态资源组成，分别在4个 `git` 地址上 ，在拉取代码阶段用循环拼接不同的项目地址分别拉取；\n>\n> 编译阶段为了避免在 `jenkins` 本地安装及配置  `nodejs` , 使用对应镜像在容器中编译成静态资源，并且映射相关依赖目录，加快下载依赖的速度，最后打包并压缩成  `gz` 包；\n>\n> 构建镜像时候使用  `docker` 插件，使用 withRegistry 方法  `build` 并  `push` ；未防止机器上镜像过多，在  `push` 之后进行删除（纯粹也是学习，和直接使用  `docker build` 或者  `docker push` 的命令 一样）；\n>\n> 更新阶段学习使用  `ansible` 插件，提前写了对应的playbook，当时服务是用 `docker-compose`  编排的，更新服务只要更新镜像即可，就写了个 py脚本替换 yml 里的 image 字段，再重启服务\n\n# 案例三：构建golang后端项目\n\n```groovy\ntimestamps {\n  podTemplate(\n    containers: [\n      containerTemplate(name: 'sonar', image: 'sonar:v4.8.0', command: 'sleep', args: '99d'),\n      containerTemplate(name: 'golang', image: 'golang:1.20', command: 'sleep', args: '99d'),\n      containerTemplate(name: 'kubectl', image: 'kubectl:v1', command: 'sleep', args: '99d')\n    ],\n    volumes: [\n      hostPathVolume(hostPath: '/etc/docker', mountPath: '/etc/docker'),\n      hostPathVolume(hostPath: '/usr/bin/docker', mountPath: '/usr/bin/docker'),\n      hostPathVolume(hostPath: '/etc/resolv.conf', mountPath: '/etc/resolv.conf'),\n      hostPathVolume(hostPath: '/var/run/docker.sock', mountPath: '/var/run/docker.sock')\n    ],\n    envVars: [\n      envVar(key: 'GOPROXY', value: 'https://goproxy.cn,direct'),\n      envVar(key: 'GONOPROXY', value: 'xxxxxx'),\n      envVar(key: 'GONOSUMDB', value: 'xxxxxx'),\n      envVar(key: 'GOPRIVATE', value: 'xxxxxx'),\n      envVar(key: 'GOINSECURE', value: 'xxxxxx'),\n    ]\n  ){\n    node(POD_LABEL) {\n      // 祖传的一些变量\n      git_cr_id='xxxx'\n      git_url='xxxxx'\n      harbor_url = 'xxxx'\n      project = 'xxx'\n      arch = 'x86_64'\n      apps = 'xxxxxx'\n        \n      stage('Git') {\n        checkout([$class: 'GitSCM', \n            branches: [[name: \"${target}\"]], \n            extensions: [[$class: 'CloneOption',shallow: true,depth: 1,timeout: 30]], \n            userRemoteConfigs: [[credentialsId: git_cr_id, url: git_url]]\n        ])\n\n        image_tag=sh(returnStdout: true, script: 'git rev-parse --short HEAD').trim()\n\n        container('sonar'){\n          stage('Sonar'){\n            for ( app in apps.tokenize(\",\") ){\n              dir(\"/home/jenkins/agent/workspace/${JOB_NAME}/app/\"+app){\n                withSonarQubeEnv('sonarqubeserver') {\n                  withCredentials([usernamePassword(credentialsId: 'sonar-scan', passwordVariable: 'snoarPSW', usernameVariable: 'snoarUSR')]) {\n                    sh \"sonar-scanner -Dsonar.projectKey=${app} -Dsonar.projectName=${app}  -Dsonar.login=$snoarUSR -Dsonar.password=$snoarPSW\"\n                  }\n                }\n              }\n            }\n          }\n        }\n\n        container('golang') {\n          stage('Make') {\n            for ( app in apps.tokenize(\",\") ){\n              dir(\"/home/jenkins/agent/workspace/${JOB_NAME}/app/${app}\"){\n                sh 'git config --global user.name \"xxxxxx\"'\n                sh 'git config --global user.email \"xxxxxx\"'\n                sh \"git config --global --add safe.directory /home/jenkins/agent/workspace/${JOB_NAME}\"\n                sh 'git config --global url.\"git@xxxxxx:\".insteadof \"http://xxxxxx/\"'\n                sh 'make linux-build'\n              }\n            }\n          }\n\n          stage('Image'){\n            for ( app in apps.tokenize(\",\") ){\n              dir(\"/home/jenkins/agent/workspace/${JOB_NAME}/app/${app}\"){\n                image = \"${harbor_url}/${project}/${arch}/${app}:${image_tag}\"\n                build_image(image)\n              }\n            }\n          }\n        }  \n\n        container('kubectl') {\n          stage('Deploy'){\n            sh \"mkdir -p ~/.kube\"\n              \n            if (target == 'rc') {\n              withCredentials([string(credentialsId: 'k8s-uat-admin', variable: 'K8S_UAT_CONF')]) {\n                sh 'echo $K8S_UAT_CONF | base64 -d > ~/.kube/config'\n              }\n            }else{\n              withCredentials([string(credentialsId: 'k8s-dev-admin', variable: 'K8S_DEV_CONF')]) {\n                sh 'echo $K8S_DEV_CONF | base64 -d > ~/.kube/config'\n              }\n            }\n\n            for ( app in apps.tokenize(\",\") ){\n              dir(\"/home/jenkins/agent/workspace/${JOB_NAME}/app/${app}\"){\n                if (target =='develop' || target =='rc'){\n                  sh \"helm upgrade -i -n tecoai --set image.tag=${image_tag} --set labels.branch=${target} ${app} ./helm/\"\n                }else{\n                  sh \"sed -i 's/${app}/${app}-${target}/g' helm/Chart.yaml\"\n                  sh \"helm upgrade -i -n tecoai --set image.tag=${image_tag} --set labels.branch=${target} --set app=${app}-${target} ${app}-${target} ./helm/\"\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n}\n\ndef build_image(String image){\n    docker.withRegistry(\"https://${harbor_url}\", 'harbor') {\n        def customImage=docker.build(\"${image}\")\n        customImage.push()\n    }\n    sh \"docker rmi ${image}\"\n}\n```\n\n## 总结说明\n\n> 这个是最近比较折腾的一个流水线，说下总体的思路：\n>\n> 使用 `Jenkins` 的 `Kubernetes` 插件，在已有的 `K8S` 集群中添加一个云节点（需要提前在 `Jenkins` 里配置的，具体可以看插件的说明）；\n>\n> 这个云节点会和 `podTemplate` 里写的多个容器组成一个 pod 运行 （podTemplate里的语法也可以使用最直接的yaml），例如此处创建了3个 `container`；\n>\n> 代码拉取： 使用 `checkout` 来实现浅拷贝，加快拉取速度，以 `commit id` 作为之后镜像的 `tag`\n>\n> 代码扫描：使用 `pod` 中的 `sonar` 容器（sonor镜像需要提前准备）完成扫描；\n>\n> 代码编译：使用 `pod` 中的 `golang` 容器，利用研发在项目中撰写的 `Makefile`  ，执行 `make linux-build` 生成可执行文件（因为 `go build` 中依赖其他项目中的公共库，所以这里加了一些其他的 `git config`，在 `podTemplate` 的 `envVars` 中也添加了一些编译需要的环境变量 ）; 这个 `golang` 容器的镜像为了能够免密拉取公共库的包，配置了 `git` 中 `ssh` 的秘钥；\n>\n> 构建镜像：这个和之前一样，使用 `Jenkins` 的 `docker` 插件完成，稍微不同的就是单独封装成了一个方法，需要时候直接调用；为了避免 `docker build`  时上下文过大，添加 `.dockerignore` 文件；另外，这一步使用 `pod` 中的 `golang` 容器，在 `podTemplate` 的 `volumes` 中也添加 hostpath 实现 `docker in docker` ；\n>\n> 更新服务：使用 `pod`中的 `kubectl` 容器，根据不同分支选择对应 K8S 集群环境（不同的 `kubeconfig` 需要提前在 `Jenkins` 里配置），使用helm部署或者更新应用。\n>\n> 之前的流水线都是声明式的，通过 `script` 来写一些脚本，这次干脆直接使用脚本式；在这个项目中实际包含了两个应用，也用到了循环分别进到不同的目录构建和部署（需要和研发沟通）；除此之外，之前都是明确分支或者标签的，所以在一开始写了个 `GIT_BRANCH` 或者 `GIT_BRANCH_TAG` 的变量，而在这个流水线里拉取的分支变量是 `target` ，考虑到和触发器有关就在 Jenkins 的页面上配置了：这里的逻辑是   `rc` 分支部署到测试环境供测试人员测试，`develop` 分支部署到开发环境供开发人员自测，而 `develop` 是可能有多个研发完成不同特性分支 `merge` 上去的，这个单独的特性分支 `push` 之后，也会通过 `helm` 部署一个特性的 `release` 应用以供研发自测特性功能。\n\n***还需说明的一点是，个人认为流水线一般只负责到更新就完成了，至于更新是否成功或者如果失败是否需要回滚，可能需要研发排查代码或者运维排查配置其他问题，没有必要包含在流水线内。***\n\n---\n\n&emsp;&emsp;可以看到本人在不同阶段的流水线中用了很多插件，实际上对于 `DevOps` 工具而言，应该尽可能只使用它最核心的功能。插件是有学习成本的，增加了阅读和编写难度的同时，也可能导致其他问题：比如最后一个案例中所有编译和构建都在 pod 中，虽然提高了隔离性，但不管是前端还是后端的依赖都需要重新拉取，影响了构建速度，如果再使用 `hostpath` 挂载目录，就需要考虑固定某一节点，这样反而复杂了。 \n&emsp;&emsp;业务繁多，任何流水线都有优化的空间。说句有用的废话，适合自己的才是最好的。\n","tags":["CI/CD","自动化"],"categories":["devops"]},{"title":"後搖","url":"/posts/15d5b004.html","content":"<!-- block -->\n>&emsp;&emsp;後搖後搖，一起抱抱搖滾！\n<!-- block -->\n{% meting \"9543514113\" \"netease\" \"playlist\" \"autoplay \" \"mutex:true\" \"listmaxheight:500px\" \"preload:auto\" \"theme:#ad7a86\"  \"loop:all\" \"order:random\" %}","tags":["文雀","搖滾","後搖"],"categories":["playlist"]},{"title":"Prometheus常用配置汇总","url":"/posts/6fe7255f.html","content":"<!-- block -->\n>&emsp;&emsp;在之前的 [K8S常用yaml汇总](https://limerencist.github.io/posts/ef247c7f.html) 中的 Prometheus 部分，为了快速部署应用，并没有添加常用的target和rules配置，然而在实际工作中，任务的服务发现和告警规则必不可少，本文就 target 和 rules 也整理了一些常用的配置，做到开箱即用。\n<!-- block -->\n\n# Target\n\n```yaml\nglobal:\n  scrape_interval:     15s\n  evaluation_interval: 15s\n  external_labels:\n    cluster: prometheus-ha\n    replica: $(POD_NAME)\nrule_files:\n- /etc/prometheus/rules/*.yml\nalerting:\n  alertmanagers:\n  - static_configs:\n    - targets: [\"alertmanager:9093\"]\nscrape_configs:\n- job_name: 'prometheus'\n  static_configs:\n    - targets: ['localhost:9090']\n- job_name: 'alertmanager'\n  static_configs:\n    - targets: ['alertmanager:9093']\n- job_name: 'node-exporter'\n  kubernetes_sd_configs:\n  - role: node\n  relabel_configs:\n  - source_labels: [__address__]\n    regex: '(.*):10250'\n    replacement: '${1}:9100'\n    target_label: __address__\n    action: replace\n  - source_labels: [__meta_kubernetes_node_name]\n    action: replace\n    target_label: node      \n  - action: labelmap\n    regex: __meta_kubernetes_node_label_(.+)\n  - source_labels: [__meta_kubernetes_node_address_InternalIP]\n    action: replace\n    target_label: ip\n- job_name: 'blackbox-exporter'\n  metrics_path: /probe\n  params:\n    module: [http_2xx]  # Look for a HTTP 200 response.\n  static_configs:\n    - targets:\n      - https://prometheus.io    # Target to probe with http.\n  relabel_configs:\n    - source_labels: [__address__]\n      target_label: __param_target\n    - source_labels: [__param_target]\n      target_label: instance\n    - target_label: __address__\n      replacement: blackbox-exporter:9115  # The blackbox exporter's real hostname:port.\n- job_name: \"kube-state-metrics\" \n  static_configs: \n    - targets: [\"kube-state-metrics:8080\"]\n- job_name: 'kube-nodes-cadvisor'\n  honor_timestamps: true\n  scrape_interval: 30s\n  scrape_timeout: 10s\n  metrics_path: /metrics\n  scheme: https\n  kubernetes_sd_configs:\n  - role: node\n  bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token\n  tls_config:\n    ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n    insecure_skip_verify: true\n  relabel_configs:\n  - separator: ;\n    regex: __meta_kubernetes_node_label_(.+)\n    replacement: $1\n    action: labelmap\n  - separator: ;\n    regex: (.*)\n    target_label: __metrics_path__\n    replacement: /metrics/cadvisor\n    action: replace\n# kubelet\n- job_name: 'kube-kubelet'\n  scheme: https\n  tls_config:\n    ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n  bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token\n  kubernetes_sd_configs:\n  - role: node\n  relabel_configs:\n  - action: labelmap\n    regex: __meta_kubernetes_node_label_(.+)\n  - target_label: __address__\n    replacement: kubernetes.default.svc:443\n  - source_labels: [__meta_kubernetes_node_name]\n    regex: (.+)\n    target_label: __metrics_path__\n    replacement: /api/v1/nodes/${1}/proxy/metrics\n# apiserver\n- job_name: 'kube-apiservers'\n  kubernetes_sd_configs:\n  - role: endpoints\n  scheme: https\n  tls_config:\n    ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n  bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token\n  relabel_configs:\n  - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]\n    action: keep\n    regex: default;kubernetes;https\n# 添加controller和scheduler的任务之前，需要保证启动参数中有--bind-address=0.0.0.0，以及创建了对应的svc名称\n- job_name: 'kube-controller-manager'\n  kubernetes_sd_configs:\n  - role: endpoints\n  scheme: https\n  tls_config:\n    insecure_skip_verify: true\n  authorization:\n    credentials_file: /var/run/secrets/kubernetes.io/serviceaccount/token\n  relabel_configs:\n  - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]\n    action: keep\n    regex: kube-system;kube-controller-manager;https\n- job_name: 'kube-scheduler'\n  kubernetes_sd_configs:\n  - role: endpoints\n  scheme: https\n  tls_config:\n    insecure_skip_verify: true\n  authorization:\n    credentials_file: /var/run/secrets/kubernetes.io/serviceaccount/token\n  relabel_configs:\n  - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]\n    action: keep\n    regex: kube-system;kube-scheduler;https\n```\n\n# Rules\n\n```yaml\ngroups:\n- name: hostStatsAlert\n  rules:\n  - alert: hostCpuUsageAlert\n    expr: sum(avg without (cpu)(irate(node_cpu{mode!='idle'}[5m]))) by (instance) > 0.85\n    for: 5m\n    labels:\n      severity: warning\n    annotations:\n      summary: \"{{ $labels.instance }} CPU usage above 85% (current value: {{ $value }}%)\"\n  - alert: hostMemUsageAlert\n    expr: (node_memory_MemTotal - node_memory_MemAvailable)/node_memory_MemTotal > 0.85\n    for: 5m\n    labels:\n      severity: warning\n    annotations:\n      summary: \"{{ $labels.instance }} MEM usage above 85% (current value: {{ $value }}%)\"\n  - alert: OutOfInodes\n    expr: node_filesystem_free{fstype=\"overlay\",mountpoint =\"/\"} / node_filesystem_size{fstype=\"overlay\",mountpoint =\"/\"} * 100 < 10\n    for: 5m\n    labels:\n      severity: warning\n    annotations:\n      summary: \"Out of inodes (instance {{ $labels.instance }})\"\n      description: \"Disk is almost running out of available inodes (< 10% left) (current value: {{ $value }})\"\n  - alert: OutOfDiskSpace\n    expr: node_filesystem_free{fstype=\"overlay\",mountpoint =\"/rootfs\"} / node_filesystem_size{fstype=\"overlay\",mountpoint =\"/rootfs\"} * 100 < 10\n    for: 5m\n    labels:\n      severity: warning\n    annotations:\n      summary: \"Out of disk space (instance {{ $labels.instance }})\"\n      description: \"Disk is almost full (< 10% left) (current value: {{ $value }})\"\n  - alert: UnusualNetworkThroughputIn\n    expr: sum by (instance) (irate(node_network_receive_bytes[2m])) / 1024 / 1024 > 100\n    for: 5m\n    labels:\n      severity: warning\n    annotations:\n      summary: \"Unusual network throughput in (instance {{ $labels.instance }})\"\n      description: \"Host network interfaces are probably receiving too much data (> 100 MB/s) (current value: {{ $value }})\"\n  - alert: UnusualNetworkThroughputOut\n    expr: sum by (instance) (irate(node_network_transmit_bytes[2m])) / 1024 / 1024 > 100\n    for: 5m\n    labels:\n      severity: warning\n    annotations:\n      summary: \"Unusual network throughput out (instance {{ $labels.instance }})\"\n      description: \"Host network interfaces are probably sending too much data (> 100 MB/s) (current value: {{ $value }})\"\n  - alert: UnusualDiskReadRate\n    expr: sum by (instance) (irate(node_disk_bytes_read[2m])) / 1024 / 1024 > 50\n    for: 5m\n    labels:\n      severity: warning\n    annotations:\n      summary: \"Unusual disk read rate (instance {{ $labels.instance }})\"\n      description: \"Disk is probably reading too much data (> 50 MB/s) (current value: {{ $value }})\"\n  - alert: UnusualDiskWriteRate\n    expr: sum by (instance) (irate(node_disk_bytes_written[2m])) / 1024 / 1024 > 50\n    for: 5m\n    labels:\n      severity: warning\n    annotations:\n      summary: \"Unusual disk write rate (instance {{ $labels.instance }})\"\n      description: \"Disk is probably writing too much data (> 50 MB/s) (current value: {{ $value }})\"\n  - alert: UnusualDiskReadLatency\n    expr: rate(node_disk_read_time_ms[1m]) / rate(node_disk_reads_completed[1m]) > 100\n    for: 5m\n    labels:\n      severity: warning\n    annotations:\n      summary: \"Unusual disk read latency (instance {{ $labels.instance }})\"\n      description: \"Disk latency is growing (read operations > 100ms) (current value: {{ $value }})\"\n  - alert: UnusualDiskWriteLatency\n    expr: rate(node_disk_write_time_ms[1m]) / rate(node_disk_writes_completedl[1m]) > 100\n    for: 5m\n    labels:\n      severity: warning\n    annotations:\n      summary: \"Unusual disk write latency (instance {{ $labels.instance }})\"\n      description: \"Disk latency is growing (write operations > 100ms) (current value: {{ $value }})\"\n- name: http_status\n  rules:\n  - alert: ProbeFailed\n    expr: probe_success == 0\n    for: 1m\n    labels:\n      severity: error\n    annotations:\n      summary: \"Probe failed (instance {{ $labels.instance }})\"\n      description: \"Probe failed (current value: {{ $value }})\"\n  - alert: StatusCode\n    expr: probe_http_status_code <= 199 OR probe_http_status_code >= 400\n    for: 1m\n    labels:\n      severity: error\n    annotations:\n      summary: \"Status Code (instance {{ $labels.instance }})\"\n      description: \"HTTP status code is not 200-399 (current value: {{ $value }})\"\n  - alert: SslCertificateWillExpireSoon\n    expr: probe_ssl_earliest_cert_expiry - time() < 86400 * 30\n    for: 5m\n    labels:\n      severity: warning\n    annotations:\n      summary: \"SSL certificate will expire soon (instance {{ $labels.instance }})\"\n      description: \"SSL certificate expires in 30 days (current value: {{ $value }})\"\n  - alert: SslCertificateHasExpired\n    expr: probe_ssl_earliest_cert_expiry - time()  <= 0\n    for: 5m\n    labels:\n      severity: error\n    annotations:\n      summary: \"SSL certificate has expired (instance {{ $labels.instance }})\"\n      description: \"SSL certificate has expired already (current value: {{ $value }})\"\n  - alert: BlackboxSlowPing\n    expr: probe_icmp_duration_seconds > 2\n    for: 5m\n    labels:\n      severity: warning\n    annotations:\n      summary: \"Blackbox slow ping (instance {{ $labels.instance }})\"\n      description: \"Blackbox ping took more than 2s (current value: {{ $value }})\"\n  - alert: BlackboxSlowRequests\n    expr: probe_http_duration_seconds > 2 \n    for: 5m\n    labels:\n      severity: warning\n    annotations:\n      summary: \"Blackbox slow requests (instance {{ $labels.instance }})\"\n      description: \"Blackbox request took more than 2s (current value: {{ $value }})\"\n  - alert: PodCpuUsagePercent\n    expr: sum(sum(label_replace(irate(container_cpu_usage_seconds_total[1m]),\"pod\",\"$1\",\"container_label_io_kubernetes_pod_name\", \"(.*)\"))by(pod) / on(pod) group_right kube_pod_container_resource_limits_cpu_cores *100 )by(container,namespace,node,pod,severity) > 80\n    for: 5m\n    labels:\n      severity: warning\n    annotations:\n      summary: \"Pod cpu usage percent has exceeded 80% (current value: {{ $value }}%)\"\n  - alert: PodMemoryUsagePercent\n    expr: sum(label_replace(container_memory_working_set_bytes,\"pod\",\"$1\",\"container_label_io_kubernetes_pod_name\", \"(.*)\") / on(pod) group_left kube_pod_container_resource_limits_memory_bytes{pod!~\"prometheus.*|filebeat.*\"})by(pod) * 100 > 80\n    for: 5m\n    labels:\n      severity: warning\n    annotations:\n      summary: \"Pod emory usage percent has exceeded 80% (current value: {{ $value }}%)\"\n- name: kube-state-metrics\n  rules:\n  - alert: KubeStateMetricsListErrors\n    annotations:\n      description: kube-state-metrics is experiencing errors at an elevated rate in list operations. This is likely causing it to not be able to expose metrics about Kubernetes objects correctly or at all.\n      summary: kube-state-metrics is experiencing errors in list operations.\n    expr: |\n      (sum(rate(kube_state_metrics_list_total{job=\"kube-state-metrics\",result=\"error\"}[5m]))\n        /\n      sum(rate(kube_state_metrics_list_total{job=\"kube-state-metrics\"}[5m])))\n      > 0.01\n    for: 15m\n    labels:\n      severity: critical\n  - alert: KubeStateMetricsWatchErrors\n    annotations:\n      description: kube-state-metrics is experiencing errors at an elevated rate in watch operations. This is likely causing it to not be able to expose metrics about Kubernetes objects correctly or at all.\n      summary: kube-state-metrics is experiencing errors in watch operations.\n    expr: |\n      (sum(rate(kube_state_metrics_watch_total{job=\"kube-state-metrics\",result=\"error\"}[5m]))\n        /\n      sum(rate(kube_state_metrics_watch_total{job=\"kube-state-metrics\"}[5m])))\n      > 0.01\n    for: 15m\n    labels:\n      severity: critical\n  - alert: KubeStateMetricsShardingMismatch\n    annotations:\n      description: kube-state-metrics pods are running with different --total-shards configuration, some Kubernetes objects may be exposed multiple times or not exposed at all.\n      summary: kube-state-metrics sharding is misconfigured.\n    expr: |\n      stdvar (kube_state_metrics_total_shards{job=\"kube-state-metrics\"}) != 0\n    for: 15m\n    labels:\n      severity: critical\n  - alert: KubeStateMetricsShardsMissing\n    annotations:\n      description: kube-state-metrics shards are missing, some Kubernetes objects are not being exposed.\n      summary: kube-state-metrics shards are missing.\n    expr: |\n      2^max(kube_state_metrics_total_shards{job=\"kube-state-metrics\"}) - 1\n        -\n      sum( 2 ^ max by (shard_ordinal) (kube_state_metrics_shard_ordinal{job=\"kube-state-metrics\"}) )\n      != 0\n    for: 15m\n    labels:\n      severity: critical\n```\n\n","tags":["K8S","监控告警"],"categories":["devops"]},{"title":"伍佰老大","url":"/posts/4f9469f2.html","content":"<!-- block -->\n>&emsp;&emsp;想我 伍佰老大 和 China Blue 必须得有个专属歌单！\n<!-- block -->\n{% meting \"8513433371\" \"netease\" \"playlist\" \"autoplay \" \"mutex:true\" \"listmaxheight:500px\" \"preload:auto\" \"theme:#ad7a86\"  \"loop:all\" \"order:random\" %}","tags":["伍佰","China Blue","搖滾"],"categories":["playlist"]},{"title":"K8S常用yaml汇总","url":"/posts/ef247c7f.html","content":"<!-- block -->\n>&emsp;&emsp;由于工作需要，K8S 总有服务是需要重复部署的，例如监控、日志等，每此都从网上整理很麻烦，而且往往有错误或者因 K8S 版本原因需要修改。虽然使用 helm 或者 operator 方式部署更为方便，但都是经过别人封装的东西，且万变不离 yaml。本文旨在记录一些常用的组件 yaml，做到开箱即用——至少在我本地直接 kubectl apply -f 就可以运行。诚然，有不规范或不严谨的地方在所难免，往各位海涵。\n<!-- block -->\n\n## 说明\n1. 涉及到的数据存储都依赖于 `StorageClass` ，需要 `K8S` 集群内提前创建。\n2. 使用的镜像如不加 `tag` ，即使用 `latest` 镜像，以供使用者有需要添加制定版本的 tag。\n3. 个人习惯创建资源的顺序为 `RBAC` → `Configmap/Secret` → `Deployment/Statefulset/Deamonset` → `Service` ，实际可按需使用 `Ingress`。（看到文章说 `svc` 先于 `deploy` 等工作负载创建会比较好，个人觉得从逻辑上 `svc` 的端口是从工作负载里声明的，没有工作负载也没有对应的 `svc`）\n\n# 通用类\n\n## Grafana\n\n### 01-grafana-pvc.yaml\n\n```yaml\n## 除K8S属性如namespace外的修改点\n# 1、storage 存储大小\n# 2、storageclass 名称\n\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: grafana-data\n  namespace: public\nspec:\n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage: 10Gi\n  storageClassName: <storageclass_name>\n```\n\n### 02-grafana-deploy.yaml\n\n```yaml\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: grafana\n  namespace: public\n  labels:\n    app: grafana\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: grafana\n  template:\n    metadata:\n      labels:\n        app: grafana\n    spec:\n      securityContext:\n        fsGroup: 472\n        supplementalGroups:\n          - 0\n      containers:\n        - name: grafana\n          image: grafana/grafana\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http-grafana\n              containerPort: 3000\n              protocol: TCP\n          readinessProbe:\n            failureThreshold: 3\n            httpGet:\n              path: /robots.txt\n              port: 3000\n              scheme: HTTP\n            initialDelaySeconds: 10\n            periodSeconds: 30\n            successThreshold: 1\n            timeoutSeconds: 2\n          livenessProbe:\n            failureThreshold: 3\n            initialDelaySeconds: 30\n            periodSeconds: 10\n            successThreshold: 1\n            tcpSocket:\n              port: 3000\n            timeoutSeconds: 1\n          resources:\n            requests:\n              cpu: 250m\n              memory: 750Mi\n          volumeMounts:\n            - name: grafana-data-volume\n              mountPath: /var/lib/grafana\n      volumes:\n        - name: grafana-data-volume\n          persistentVolumeClaim:\n            claimName: grafana-data\n```\n\n### 03-grafana-svc.yaml\n\n```yaml\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: grafana\n  namespace: public\n  labels:\n    app: grafana\nspec:\n  type: NodePort\n  ports:\n    - name: grafana-port\n      port: 3000\n      protocol: TCP\n      targetPort: http-grafana\n      nodePort: 31001\n  selector:\n    app: grafana\n```\n\n---\n\n## Minio\n\n> 因 `Minio` 简单，此处以 `minio` 作为后续用到的对象存储，有其他对象存储的可以略过。\n\n### 01-minio-pvc.yaml\n\n```yaml\n## 除K8S属性如namespace外的修改点\n# 1、storage 存储大小\n# 2、storageclass 名称\n\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: minio-data\n  namespace: public\nspec:\n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage: 100Gi\n  storageClassName: <storageclass_name>\n```\n\n### 02-minio-deploy.yaml\n\n```yaml\n## 除K8S属性如namespace外的修改点\n# 1、console-address minio控制台端口\n# 2、MINIO_ROOT_USER/MINIO_ROOT_PASSWORD 登录minio控制台的账号密码\n\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: minio\n  namespace: public\n  labels:\n    app: minio\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: minio\n  template:\n    metadata:\n      labels:\n        app: minio\n    spec:\n      containers:\n        - name: minio\n          image: minio/minio\n          imagePullPolicy: IfNotPresent\n          args:\n            - server\n            - /data\n            - '--console-address'\n            - ':9001'\n          env:\n            - name: MINIO_ROOT_USER\n              value: \"admin\"\n            - name: MINIO_ROOT_PASSWORD\n              value: \"admin123\"\n          ports:\n            - name: http-minio\n              containerPort: 9000\n              protocol: TCP\n            - name: http-console\n              containerPort: 9001\n              protocol: TCP\n          volumeMounts:\n            - name: minio-data-volume\n              mountPath: /data\n      volumes:\n        - name: minio-data-volume\n          persistentVolumeClaim:\n            claimName: minio-data\n```\n\n### 03-minio-svc.yaml\n\n```yaml\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: minio\n  namespace: public\n  labels:\n    app: minio\nspec:\n  type: ClusterIP\n  ports:\n    - name: minio-port\n      port: 9000\n      protocol: TCP\n      targetPort: http-minio\n    - name: console-port\n      port: 9001\n      protocol: TCP\n      targetPort: http-console\n  selector:\n    app: minio\n```\n\n## MySQL\n\n### 01-mysql-cm.yaml\n\n> 注：`MySQL 8.0` 和 `5.7` 的部分配置项不兼容\n\n```yaml\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: mysql-config\n  namespace: public\n  labels:\n    app: mysql\ndata:\n  my.cnf: |-\n    [mysqld]\n    pid-file = /var/run/mysqld/mysqld.pid\n    socket = /var/run/mysqld/mysqld.sock\n    datadir = /var/lib/mysql\n    secure-file-priv = NULL\n    lower_case_table_names = 1\n    bind_address=0.0.0.0\n\n    # Custom config should go here\n    !includedir /etc/mysql/conf.d/\n\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: mysql-custom-config\n  namespace: public\n  labels:\n    app: mysql\ndata:\ndata:\n  my_custom.cnf: |-\n    [mysqld]\n    max_connections = 2000\n    connect_timeout = 3600\n    max_prepared_stmt_count = 65535\n    key_buffer_size = 256M\n    sort_buffer_size = 1M\n    read_buffer_size = 1M\n    read_rnd_buffer_size = 16M\n    join_buffer_size = 1M\n    thread_cache_size = 32\n    interactive_timeout = 86400\n    wait_timeout = 86400\n    table_open_cache = 2048\n    open_files_limit = 65535\n    back_log = 600\n    max_connections = 6000\n    max_connect_errors = 6000\n    max_allowed_packet = 32M\n    tmp_table_size = 256M\n    max_heap_table_size = 512M\n    bulk_insert_buffer_size = 64M\n    myisam_sort_buffer_size = 128M\n    myisam_max_sort_file_size = 10G\n    external-locking = FALSE\n    default-storage-engine = InnoDB\n    thread_stack = 256K\n    transaction_isolation = READ-COMMITTED\n    explicit_defaults_for_timestamp = 1\n    binlog_format = row\n    binlog-ignore-db = information_schema\n    binlog-ignore-db = performance_schema\n    binlog-ignore-db = sys\n    max_allowed_packet=16M\n    bind_address=0.0.0.0\n    character_set_server=utf8mb4\n    collation_server=utf8mb4_general_ci\n\n    [client]\n    default_character_set=utf8mb4\n```\n\n### 02-mysql-sts.yaml\n\n```yaml\n---\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mysql\n  namespace: public\n  labels:\n    app: mysql\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mysql\n  serviceName: mysql\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: mysql\n    spec:\n      containers:\n        - name: mysql\n          image: mysql:8.0.32\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 3306\n              protocol: TCP\n          env:\n            - name: MYSQL_ROOT_PASSWORD\n              value: tecoai123\n          resources: {}\n          volumeMounts:\n            - name: mysql-data\n              mountPath: /var/lib/mysql\n            - name: mysql-config\n              mountPath: /etc/mysql/conf.d/my.cnf\n              subPath: my.cnf\n            - name: mysql-custom-config\n              mountPath: /etc/mysql/conf.d/my_custom.cnf\n              subPath: my_custom.cnf\n      volumes:\n        - name: mysql-config\n          configMap:\n            name: mysql-config\n        - name: mysql-custom-config\n          configMap:\n            name: mysql-custom-config\n  volumeClaimTemplates:\n    - kind: PersistentVolumeClaim\n      apiVersion: v1\n      metadata:\n        name: mysql-data\n      spec:\n        accessModes:\n          - ReadWriteMany\n        resources:\n          requests:\n            storage: 10Gi\n        storageClassName: <storageclass_name>\n```\n\n### 03-mysql-svc.yaml\n\n```yaml\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: mysql\n  namespace: public\n  labels:\n    app: mysql\nspec:\n  clusterIP: None\n  ports:\n    - name: http\n      port: 3306\n      protocol: TCP\n      targetPort: http\n  selector:\n    app: mysql\n```\n\n## Redis\n\n### 01-redis-cm.yaml\n\n```yaml\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: redis-config\n  namespace: public\n  labels:\n    app: redis\ndata:\n  redis.conf: |-\n    bind 0.0.0.0\n    protected-mode yes\n    port 6379\n    tcp-backlog 511\n    timeout 0\n    tcp-keepalive 300\n    supervised no\n    loglevel notice\n    databases 16\n    always-show-logo yes\n    save 900 1\n    save 300 10\n    save 60 10000\n    stop-writes-on-bgsave-error yes\n    rdbcompression yes\n    rdbchecksum yes\n    dbfilename dump.rdb\n    dir /data\n    replica-serve-stale-data yes\n    replica-read-only yes\n    repl-diskless-sync no\n    repl-diskless-sync-delay 5\n    repl-disable-tcp-nodelay no\n    replica-priority 100\n    requirepass tecoai123\n    lazyfree-lazy-eviction no\n    lazyfree-lazy-expire no\n    lazyfree-lazy-server-del no\n    replica-lazy-flush no\n    appendonly no\n    appendfilename \"appendonly.aof\"\n    appendfsync everysec\n    no-appendfsync-on-rewrite no\n    auto-aof-rewrite-percentage 100\n    auto-aof-rewrite-min-size 64mb\n    aof-load-truncated yes\n    aof-use-rdb-preamble yes\n    lua-time-limit 5000\n    slowlog-log-slower-than 10000\n    slowlog-max-len 128\n    latency-monitor-threshold 0\n    notify-keyspace-events \"\"\n    hash-max-ziplist-entries 512\n    hash-max-ziplist-value 64\n    list-max-ziplist-size -2\n    list-compress-depth 0\n    set-max-intset-entries 512\n    zset-max-ziplist-entries 128\n    zset-max-ziplist-value 64\n    hll-sparse-max-bytes 3000\n    stream-node-max-bytes 4096\n    stream-node-max-entries 100\n    activerehashing yes\n    client-output-buffer-limit normal 0 0 0\n    client-output-buffer-limit replica 256mb 64mb 60\n    client-output-buffer-limit pubsub 32mb 8mb 60\n    hz 10\n    dynamic-hz yes\n    aof-rewrite-incremental-fsync yes\n    rdb-save-incremental-fsync yes\n```\n\n### 02-redis-sts.yaml\n\n```yaml\n---\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis\n  namespace: public\n  labels:\n    app: redis\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis\n  serviceName: redis\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: redis\n    spec:\n      containers:\n        - name: redis\n          image: redis:6.2.11-alpine3.17\n          imagePullPolicy: IfNotPresent\n          command:\n            - redis-server\n            - /etc/redis/redis.conf\n          ports:\n            - name: http\n              containerPort: 6379\n              protocol: TCP\n          volumeMounts:\n            - name: redis-data\n              mountPath: /data\n            - name: redis-config\n              mountPath: /etc/redis/redis.conf\n              subPath: redis.conf\n      volumes:\n        - name: redis-config\n          configMap:\n            name: redis-config\n            defaultMode: 420\n  volumeClaimTemplates:\n    - kind: PersistentVolumeClaim\n      apiVersion: v1\n      metadata:\n        name: redis-data\n      spec:\n        accessModes:\n          - ReadWriteMany\n        resources:\n          requests:\n            storage: 10Gi\n        storageClassName: <storageclass_name>\n```\n\n### 03-redis-svc.yaml\n\n```yaml\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: redis\n  namespace: public\n  labels:\n    app: redis\nspec:\n  clusterIP: None\n  ports:\n    - name: http\n      port: 6379\n      protocol: TCP\n      targetPort: http\n  selector:\n    app: redis\n```\n\n# 日志\n\n> 相对于重量级日志分析的 `ELK(F)` ，这里使用更为轻量的 `Loki` 仅仅作为日志的查看工具，并对接 `minio` 。\n\n## Loki\n\n### 01-loki-rbac.yaml\n\n```yaml\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: loki\n  namespace: logging\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: loki\n  namespace: logging\nrules:\n  - apiGroups:\n      - extensions\n    resourceNames:\n      - loki\n    resources:\n      - podsecuritypolicies\n    verbs:\n      - use\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: loki\n  namespace: logging\nsubjects:\n  - kind: ServiceAccount\n    name: loki\n    namespace: logging\nroleRef:\n  kind: Role\n  name: loki\n  apiGroup: rbac.authorization.k8s.io\n```\n\n### 02-loki-cm.yaml\n\n```yaml\n## 除K8S属性如namespace外的修改点\n# 1、shared_store 存储类型\n# 2、s3 对象存储的aksk及url\n\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: loki-config\n  namespace: logging\n  labels:\n    app: loki\ndata:\n  loki.yaml: |\n    auth_enabled: false\n    server:\n      http_listen_port: 3100\n    ingester:\n      lifecycler:       #配置ingester的生命周期，以及在哪里注册以进行发现\n        ring:\n          kvstore:\n            store: inmemory      # 用于ring的后端存储，支持consul、etcd、inmemory\n          replication_factor: 1      # 写入和读取的ingesters数量，至少为1（为了冗余和弹性，默认情况下为3）\n      chunk_idle_period: 3m      # 如果块没有达到最大的块大小，那么在刷新之前，块应该在内存中不更新多长时间\n      chunk_block_size: 262144\n      chunk_retain_period: 1m      # 块刷新后应该在内存中保留多长时间\n      max_transfer_retries: 0      # Number of times to try and transfer chunks when leaving before falling back to flushing to the store. Zero = no transfers are done.\n    schema_config:      # 配置从特定时间段开始应该使用哪些索引模式\n      configs:\n      - from: 2020-10-24      # 创建索引的日期。如果这是唯一的schema_config，则使用过去的日期，否则使用希望切换模式时的日期\n        store: boltdb-shipper      # 索引使用哪个存储，如：cassandra, bigtable, dynamodb，或boltdb\n        object_store: filesystem      # 用于块的存储，如：gcs, s3， inmemory, filesystem, cassandra，如果省略，默认值与store相同\n        schema: v11\n        index:      # 配置如何更新和存储索引\n          prefix: index_      # 所有周期表的前缀\n          period: 24h      # 表周期\n    storage_config:      # 为索引和块配置一个或多个存储\n      boltdb_shipper:\n        active_index_directory: /data/loki/boltdb-shipper-active\n        cache_location: /data/loki/boltdb-shipper-cache\n        cache_ttl: 24h\n        # shared_store: filesystem\n        shared_store: s3\n      filesystem:\n        directory: /data/loki/chunks\n      aws:\n        s3: s3://access_key:secret_key@<minio_svc_name>(.<minio_namspace_name>.svc.cluster.local):<minio_svc_port>/<bucket_name>\n        s3forcepathstyle: true\n    limits_config:\n      enforce_metric_name: false\n      reject_old_samples: true      # 旧样品是否会被拒绝\n      reject_old_samples_max_age: 168h      # 拒绝旧样本的最大时限\n    chunk_store_config:      # 配置如何缓存块，以及在将它们保存到存储之前等待多长时间\n      max_look_back_period: 0s      #限制查询数据的时间，默认是禁用的，这个值应该小于或等于table_manager.retention_period中的值\n    table_manager:\n      retention_deletes_enabled: true      # 日志保留周期开关，用于表保留删除\n      retention_period: 48h       # 日志保留周期，保留期必须是索引/块的倍数\n    compactor:\n      working_directory: /data/loki/boltdb-shipper-compactor\n      # shared_store: filesystem\n      shared_store: s3\n      compaction_interval: 5m\n```\n\n### 03-loki-sts.yaml\n\n```yaml\n## 除K8S属性如namespace外的修改点\n# 1、storage 存储大小\n# 2、storageclass 名称\n\n---\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: loki\n  namespace: logging\n  labels:\n    app: loki\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: loki\n  serviceName: loki\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: loki\n    spec:\n      serviceAccountName: loki\n      securityContext:\n          fsGroup: 10001\n          runAsGroup: 10001\n          runAsNonRoot: true\n          runAsUser: 10001\n      initContainers: []\n      containers:\n        - name: loki\n          image: grafana/loki\n          imagePullPolicy: IfNotPresent\n          args:\n            - -config.file=/etc/loki/loki.yaml\n          ports:\n            - name: http-loki\n              containerPort: 3100\n              protocol: TCP\n          volumeMounts:\n            - name: loki-config-volume\n              mountPath: /etc/loki\n            - name: loki-data\n              mountPath: /data\n            - name: loki-wal\n              mountPath: /wal\n          livenessProbe:\n            httpGet:\n              path: /ready\n              port: http-loki\n              scheme: HTTP\n            initialDelaySeconds: 45\n            timeoutSeconds: 1\n            periodSeconds: 10\n            successThreshold: 1\n            failureThreshold: 3\n          readinessProbe:\n            httpGet:\n              path: /ready\n              port: http-loki\n              scheme: HTTP\n            initialDelaySeconds: 45\n            timeoutSeconds: 1\n            periodSeconds: 10\n            successThreshold: 1\n            failureThreshold: 3\n          securityContext:\n            readOnlyRootFilesystem: true\n      terminationGracePeriodSeconds: 4800\n      volumes:\n        - name: loki-config-volume\n          configMap:\n            defaultMode: 420\n            name: loki-config\n  volumeClaimTemplates:\n    - metadata:\n        name: loki-data\n        labels:\n          app: loki\n      spec:\n        accessModes:\n          - ReadWriteMany\n        storageClassName: <storageclass_name>\n        resources:\n          requests:\n            storage: \"10Gi\"\n    - metadata:\n        name: loki-wal\n        labels:\n          app: loki\n      spec:\n        accessModes:\n          - ReadWriteMany\n        storageClassName: <storageclass_name>\n        resources:\n          requests:\n            storage: \"5Gi\"\n```\n\n### 04-loki-svc.yaml\n\n```yaml\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: loki\n  namespace: logging\n  labels:\n    app: loki\nspec:\n  type: ClusterIP\n  ports:\n    - name: loki-port\n      port: 3100\n      protocol: TCP\n      targetPort: http-loki\n  selector:\n    app: loki\n```\n\n## Promtail\n\n### 01-promtail-rbac.yaml\n\n```yaml\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: promtail\n  namespace: logging\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: promtail\n  namespace: logging\nrules:\n  - apiGroups:\n      - \"\"\n    resources:\n      - nodes\n      - nodes/proxy\n      - services\n      - endpoints\n      - pods\n    verbs:\n      - get\n      - list\n      - watch\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: promtail\n  namespace: logging\nsubjects:\n  - kind: ServiceAccount\n    name: promtail\n    namespace: logging\nroleRef:\n  kind: ClusterRole\n  name: promtail\n  apiGroup: rbac.authorization.k8s.io\n```\n\n### 02-protmail-cm.yaml\n\n```yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: promtail-config\n  namespace: logging\n  labels:\n    app: promtail\ndata:\n  promtail.yaml: |\n    client:      # 配置Promtail如何连接到Loki的实例\n      backoff_config:      # 配置当请求失败时如何重试请求给Loki\n        max_period: 5m\n        max_retries: 10\n        min_period: 500ms\n      batchsize: 1048576      # 发送给Loki的最大批次大小(以字节为单位)\n      batchwait: 1s      # 发送批处理前等待的最大时间（即使批次大小未达到最大值）\n      external_labels: {}      # 所有发送给Loki的日志添加静态标签\n      timeout: 10s      # 等待服务器响应请求的最大时间\n    positions:\n      filename: /run/promtail/positions.yaml\n    server:\n      http_listen_port: 3101\n    target_config:\n      sync_period: 10s\n    scrape_configs:\n    - job_name: kubernetes-pods-name\n      pipeline_stages:\n        - docker: {}\n      kubernetes_sd_configs:\n        - role: pod\n      relabel_configs:\n      - source_labels:\n        - __meta_kubernetes_pod_label_name\n        target_label: __service__\n      - source_labels:\n        - __meta_kubernetes_pod_node_name\n        target_label: __host__\n      - action: drop\n        regex: ''\n        source_labels:\n        - __service__\n      - action: labelmap\n        regex: __meta_kubernetes_pod_label_(.+)\n      - action: replace\n        replacement: $1\n        separator: /\n        source_labels:\n        - __meta_kubernetes_namespace\n        - __service__\n        target_label: job\n      - action: replace\n        source_labels:\n        - __meta_kubernetes_namespace\n        target_label: namespace\n      - action: replace\n        source_labels:\n        - __meta_kubernetes_pod_name\n        target_label: pod\n      - action: replace\n        source_labels:\n        - __meta_kubernetes_pod_container_name\n        target_label: container\n      - replacement: /var/log/pods/*$1/*.log\n        separator: /\n        source_labels:\n        - __meta_kubernetes_pod_uid\n        - __meta_kubernetes_pod_container_name\n        target_label: __path__\n    - job_name: kubernetes-pods-app\n      pipeline_stages:\n        - docker: {}\n      kubernetes_sd_configs:\n      - role: pod\n      relabel_configs:\n      - action: drop\n        regex: .+\n        source_labels:\n        - __meta_kubernetes_pod_label_name\n      - source_labels:\n        - __meta_kubernetes_pod_label_app\n        target_label: __service__\n      - source_labels:\n        - __meta_kubernetes_pod_node_name\n        target_label: __host__\n      - action: drop\n        regex: ''\n        source_labels:\n        - __service__\n      - action: labelmap\n        regex: __meta_kubernetes_pod_label_(.+)\n      - action: replace\n        replacement: $1\n        separator: /\n        source_labels:\n        - __meta_kubernetes_namespace\n        - __service__\n        target_label: job\n      - action: replace\n        source_labels:\n        - __meta_kubernetes_namespace\n        target_label: namespace\n      - action: replace\n        source_labels:\n        - __meta_kubernetes_pod_name\n        target_label: pod\n      - action: replace\n        source_labels:\n        - __meta_kubernetes_pod_container_name\n        target_label: container\n      - replacement: /var/log/pods/*$1/*.log\n        separator: /\n        source_labels:\n        - __meta_kubernetes_pod_uid\n        - __meta_kubernetes_pod_container_name\n        target_label: __path__\n    - job_name: kubernetes-pods-direct-controllers\n      pipeline_stages:\n        - docker: {}\n      kubernetes_sd_configs:\n      - role: pod\n      relabel_configs:\n      - action: drop\n        regex: .+\n        separator: ''\n        source_labels:\n        - __meta_kubernetes_pod_label_name\n        - __meta_kubernetes_pod_label_app\n      - action: drop\n        regex: '[0-9a-z-.]+-[0-9a-f]{8,10}'\n        source_labels:\n        - __meta_kubernetes_pod_controller_name\n      - source_labels:\n        - __meta_kubernetes_pod_controller_name\n        target_label: __service__\n      - source_labels:\n        - __meta_kubernetes_pod_node_name\n        target_label: __host__\n      - action: drop\n        regex: ''\n        source_labels:\n        - __service__\n      - action: labelmap\n        regex: __meta_kubernetes_pod_label_(.+)\n      - action: replace\n        replacement: $1\n        separator: /\n        source_labels:\n        - __meta_kubernetes_namespace\n        - __service__\n        target_label: job\n      - action: replace\n        source_labels:\n        - __meta_kubernetes_namespace\n        target_label: namespace\n      - action: replace\n        source_labels:\n        - __meta_kubernetes_pod_name\n        target_label: pod\n      - action: replace\n        source_labels:\n        - __meta_kubernetes_pod_container_name\n        target_label: container\n      - replacement: /var/log/pods/*$1/*.log\n        separator: /\n        source_labels:\n        - __meta_kubernetes_pod_uid\n        - __meta_kubernetes_pod_container_name\n        target_label: __path__\n    - job_name: kubernetes-pods-indirect-controller\n      pipeline_stages:\n        - docker: {}\n      kubernetes_sd_configs:\n      - role: pod\n      relabel_configs:\n      - action: drop\n        regex: .+\n        separator: ''\n        source_labels:\n        - __meta_kubernetes_pod_label_name\n        - __meta_kubernetes_pod_label_app\n      - action: keep\n        regex: '[0-9a-z-.]+-[0-9a-f]{8,10}'\n        source_labels:\n        - __meta_kubernetes_pod_controller_name\n      - action: replace\n        regex: '([0-9a-z-.]+)-[0-9a-f]{8,10}'\n        source_labels:\n        - __meta_kubernetes_pod_controller_name\n        target_label: __service__\n      - source_labels:\n        - __meta_kubernetes_pod_node_name\n        target_label: __host__\n      - action: drop\n        regex: ''\n        source_labels:\n        - __service__\n      - action: labelmap\n        regex: __meta_kubernetes_pod_label_(.+)\n      - action: replace\n        replacement: $1\n        separator: /\n        source_labels:\n        - __meta_kubernetes_namespace\n        - __service__\n        target_label: job\n      - action: replace\n        source_labels:\n        - __meta_kubernetes_namespace\n        target_label: namespace\n      - action: replace\n        source_labels:\n        - __meta_kubernetes_pod_name\n        target_label: pod\n      - action: replace\n        source_labels:\n        - __meta_kubernetes_pod_container_name\n        target_label: container\n      - replacement: /var/log/pods/*$1/*.log\n        separator: /\n        source_labels:\n        - __meta_kubernetes_pod_uid\n        - __meta_kubernetes_pod_container_name\n        target_label: __path__\n    - job_name: kubernetes-pods-static\n      pipeline_stages:\n        - docker: {}\n      kubernetes_sd_configs:\n      - role: pod\n      relabel_configs:\n      - action: drop\n        regex: ''\n        source_labels:\n        - __meta_kubernetes_pod_annotation_kubernetes_io_config_mirror\n      - action: replace\n        source_labels:\n        - __meta_kubernetes_pod_label_component\n        target_label: __service__\n      - source_labels:\n        - __meta_kubernetes_pod_node_name\n        target_label: __host__\n      - action: drop\n        regex: ''\n        source_labels:\n        - __service__\n      - action: labelmap\n        regex: __meta_kubernetes_pod_label_(.+)\n      - action: replace\n        replacement: $1\n        separator: /\n        source_labels:\n        - __meta_kubernetes_namespace\n        - __service__\n        target_label: job\n      - action: replace\n        source_labels:\n        - __meta_kubernetes_namespace\n        target_label: namespace\n      - action: replace\n        source_labels:\n        - __meta_kubernetes_pod_name\n        target_label: pod\n      - action: replace\n        source_labels:\n        - __meta_kubernetes_pod_container_name\n        target_label: container\n      - replacement: /var/log/pods/*$1/*.log\n        separator: /\n        source_labels:\n        - __meta_kubernetes_pod_annotation_kubernetes_io_config_mirror\n        - __meta_kubernetes_pod_container_name\n        target_label: __path__\n```\n\n### 03-promtail-ds.yaml\n\n```yaml\n## 除K8S属性如namespace外的修改点\n#1. client.url  loki地址\n#2. volume docker挂载路径\n\n---\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: promtail\n  namespace: logging\n  labels:\n    app: promtail\nspec:\n  selector:\n    matchLabels:\n      app: promtail\n  template:\n    metadata:\n      labels:\n        app: promtail\n    spec:\n      serviceAccountName: promtail\n      containers:\n        - name: promtail\n          image: grafana/promtail\n          imagePullPolicy: IfNotPresent\n          args:\n            - -config.file=/etc/promtail/promtail.yaml\n            - -client.url=http://loki:3100/loki/api/v1/push\n          env:\n            - name: HOSTNAME\n              valueFrom:\n                fieldRef:\n                  apiVersion: v1\n                  fieldPath: spec.nodeName\n          ports:\n            - name: http-promtail\n              containerPort: 3101\n              protocol: TCP\n          volumeMounts:\n            - name: loki-config-volume\n              mountPath: /etc/promtail\n            - name: run\n              mountPath: /run/promtail\n            - name: docker\n              mountPath: /data/docker/containers\n              readOnly: true\n            - name: pods\n              mountPath: /var/log/pods\n              readOnly: true\n          securityContext:\n            readOnlyRootFilesystem: true\n            runAsGroup: 0\n            runAsUser: 0\n          readinessProbe:\n            failureThreshold: 5\n            httpGet:\n              path: /ready\n              port: http-promtail\n              scheme: HTTP\n            initialDelaySeconds: 10\n            periodSeconds: 10\n            successThreshold: 1\n            timeoutSeconds: 1\n      tolerations:\n        - effect: NoSchedule\n          key: node-role.kubernetes.io/master\n          operator: Exists\n      volumes:\n        - name: loki-config-volume\n          configMap:\n            defaultMode: 420\n            name: promtail-config\n        - name: run\n          hostPath:\n            path: /run/promtail\n            type: \"\"\n        - name: docker\n          hostPath:\n            path: /data/docker/containers\n        - name: pods\n          hostPath:\n            path: /var/log/pods\n```\n\n> 部署完成后需要在 `Grafana` 中添加 `Loki` 数据源，添加地址为 `<loki_svc_name>(.<loki_namspace_name>.svc.clster.local):<loki_svc_port>` 。\n\n# 监控\n\n> 除了普通 `Prometheus` 之外，对于多个 `Prometheus`，添加 `Thanos` 方案。\n\n## Alertmanager\n\n### 01-alertmanager-cm.yaml\n\n```yaml\n## 除K8S属性如namespace外的修改点\n# 1、邮箱smtp等账户信息\n\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: alertmanager-config\n  namespace: monitoring\ndata:\n  alertmanager.yml: |-\n    # 发送邮箱配置\n    global:\n      resolve_timeout: 1m\n      #用于发送邮件的邮箱的SMTP服务器地址+端口\n      smtp_smarthost: 'smtp.163.com:25'\n      # 从哪个邮箱发送报警\n      smtp_from: '<mail_address>'\n      # 发送邮箱的认证用户，不是邮箱名\n      smtp_auth_username: '<user_name>'\n      # 发送邮箱的授权码而不是登录密码\n      smtp_auth_password: 'poxuwotjhdbybdfb'\n      smtp_require_tls: false\n    route:\n      group_by: [alertname]\n      group_wait: 10s\n      group_interval: 10s\n      repeat_interval: 10m\n      receiver: public\n    # 接收邮箱配置\n    receivers:\n    - name: 'public'\n      # 接收邮箱地址\n      email_configs:\n      - to: '<mail_address>'\n        send_resolved: true\n```\n\n### 02-alertmanager-pvc.yaml\n\n```yaml\n## 除K8S属性如namespace外的修改点\n# 1、storage 存储大小\n# 2、storageclass 名称\n\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: alertmanager-data\n  namespace: monitoring\nspec:\n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage: 10Gi\n  storageClassName: <storageclass_name>\n```\n\n### 03-alertmanager-deploy.yaml\n\n```yaml\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: alertmanager\n  namespace: monitoring\n  labels:\n    app: alertmanager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: alertmanager\n  template:\n    metadata:\n      labels:\n        app: alertmanager\n    spec:\n      containers:\n        - name: alertmanager\n          image: prom/alertmanager\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n            - --config.file=/etc/config/alertmanager.yml\n            - --storage.path=/data\n            - --cluster.advertise-address=0.0.0.0:9093\n          ports:\n            - name: http-alert\n              containerPort: 9093\n              protocol: TCP\n          readinessProbe:\n            httpGet:\n              path: /#/status\n              port: 9093\n            initialDelaySeconds: 30\n            timeoutSeconds: 30\n          resources:\n            requests:\n              cpu: 100m\n              memory: 100Mi\n            limits:\n              cpu: 500m\n              memory: 2500Mi\n          volumeMounts:\n            - name: alert-config-volume\n              mountPath: /etc/config\n            - name: alert-data-volume\n              mountPath: \"/data\"\n      volumes:\n        - name: alert-config-volume\n          configMap:\n            name: alertmanager-config\n        - name: alert-data-volume\n          persistentVolumeClaim:\n            claimName: alertmanager-data\n```\n\n### 04-alertmanager-svc.yaml\n\n```yaml\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: alertmanager\n  namespace: monitoring\n  labels:\n    app: alertmanager\nspec:\n  type: ClusterIP\n  ports:\n    - name: alertmanager-port\n      port: 9093\n      protocol: TCP\n      targetPort: http-alert\n  selector:\n    app: alertmanager\n```\n\n## Metrics\n\n### Node-Exporter\n\n#### 01-node-exporter-ds.yaml\n\n```yaml\n---\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: node-exporter\n  namespace: monitoring\n  labels:\n    app: node-exporter\nspec:\n  selector:\n    matchLabels:\n      app: node-exporter\n  template:\n    metadata:\n      labels:\n        app: node-exporter\n    spec:\n      hostPID: true\n      hostIPC: true\n      hostNetwork: true\n      containers:\n      - name: node-exporter\n        image: prom/node-exporter\n        imagePullPolicy: IfNotPresent\n        args:\n          - --path.procfs\n          - /host/proc\n          - --path.sysfs\n          - /host/sys\n          - --path.rootfs\n          - /host/root\n          - --collector.filesystem.ignored-mount-points\n          - '\"^/(sys|proc|dev|host|etc)($|/)\"'\n          - \"--collector.processes\"\n        ports:\n          - name: http-node\n            containerPort: 9100\n            protocol: TCP\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n          limits:\n            cpu: 2000m\n            memory: 2Gi\n        securityContext:\n          privileged: true\n        volumeMounts:\n          - name: dev\n            mountPath: /host/dev\n          - name: proc\n            mountPath: /host/proc\n          - name: sys\n            mountPath: /host/sys\n          - name: rootfs\n            mountPath: /host/root\n      tolerations:\n        - key: \"node-role.kubernetes.io/master\"\n          operator: \"Exists\"\n          effect: \"NoSchedule\"\n      volumes:\n        - name: proc\n          hostPath:\n            path: /proc\n        - name: dev\n          hostPath:\n            path: /dev\n        - name: sys\n          hostPath:\n            path: /sys\n        - name: rootfs\n          hostPath:\n            path: /\n```\n\n#### 02-node-exporter-svc.yaml（可以省略）\n\n```yaml\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: node-exporter\n  namespace: monitoring\n  labels:\n    name: node-exporter\nspec:\n  type: ClusterIP\n  ports:\n    - name: node-exporter-port\n      port: 9100\n      protocol: TCP\n      targetPort: http-node\n  selector:\n    app: node-exporter\n```\n\n### Blackbox-Exporter\n\n#### 01-blackbox-exporter-cm.yaml\n\n```yaml\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: blackbox-exporter-config\n  namespace: monitoring\n  labels:\n    app: blackbox-exporter\ndata:\n  blackbox.yml: |-\n    modules:\n      http_2xx:\n        prober: http\n        timeout: 10s\n        http:\n          valid_http_versions: [\"HTTP/1.1\", \"HTTP/2\"]\n          valid_status_codes: []\n          method: GET\n          preferred_ip_protocol: \"ip4\"\n      http_post_2xx:\n        prober: http\n        timeout: 10s\n        http:\n          valid_http_versions: [\"HTTP/1.1\", \"HTTP/2\"]\n          method: POST\n          preferred_ip_protocol: \"ip4\"\n      tcp_connect:\n        prober: tcp\n        timeout: 10s\n      icmp:\n        prober: icmp\n        timeout: 10s\n        icmp:\n          preferred_ip_protocol: \"ip4\"\n      dns:\n        prober: dns\n        dns:\n          transport_protocol: \"tcp\"\n          preferred_ip_protocol: \"ip4\"\n          query_name: \"kubernetes.default.svc.cluster.local\"\n```\n\n#### 02-blackbox-exporter-deploy.yaml\n\n```yaml\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: blackbox-exporter\n  namespace: monitoring\n  labels:\n    app: blackbox-exporter\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: blackbox-exporter\n  template:\n    metadata:\n      labels:\n        app: blackbox-exporter\n    spec:\n      containers:\n      - name: blackbox-exporter\n        image: prom/blackbox-exporter\n        imagePullPolicy: IfNotPresent\n        args:\n          - --config.file=/etc/blackbox_exporter/blackbox.yml\n          - --web.listen-address=:9115\n        ports:\n          - name: http-blackbox\n            containerPort: 9115\n            protocol: TCP\n        readinessProbe:\n          tcpSocket:\n            port: 9115\n          initialDelaySeconds: 10\n          timeoutSeconds: 5\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 100m\n          limits:\n            memory: 60Mi\n            cpu: 200m\n        volumeMounts:\n          - name: blackbox-config-volume\n            mountPath: /etc/blackbox_exporter\n      volumes:\n        - name: blackbox-config-volume\n          configMap:\n            name: blackbox-exporter-config\n```\n\n#### 03-blackbox-exporter-svc.yaml\n\n```yaml\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: blackbox-exporter\n  namespace: monitoring\n  labels:\n    name: blackbox-exporter\nspec:\n  type: ClusterIP\n  ports:\n    - name: blackbox-exporter-port\n      port: 9115\n      protocol: TCP\n      targetPort: http-blackbox\n  selector:\n    app: blackbox-exporter\n```\n\n### Kube-State-Metrics\n\n#### 01-kube-state-metrics-rbac.yaml\n\n```yaml\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: kube-state-metrics\n  namespace: monitoring\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: kube-state-metrics\n  namespace: monitoring\nrules:\n  - apiGroups:\n      - \"\"\n    resources:\n      - configmaps\n      - secrets\n      - nodes\n      - pods\n      - services\n      - serviceaccounts\n      - resourcequotas\n      - replicationcontrollers\n      - limitranges\n      - persistentvolumeclaims\n      - persistentvolumes\n      - namespaces\n      - endpoints\n    verbs:\n      - list\n      - watch\n  - apiGroups:\n      - apps\n    resources:\n      - statefulsets\n      - daemonsets\n      - deployments\n      - replicasets\n    verbs:\n      - list\n      - watch\n  - apiGroups:\n      - batch\n    resources:\n      - cronjobs\n      - jobs\n    verbs:\n      - list\n      - watch\n  - apiGroups:\n      - autoscaling\n    resources:\n      - horizontalpodautoscalers\n    verbs:\n      - list\n      - watch\n  - apiGroups:\n      - authentication.k8s.io\n    resources:\n      - tokenreviews\n    verbs:\n      - create\n  - apiGroups:\n      - authorization.k8s.io\n    resources:\n      - subjectaccessreviews\n    verbs:\n      - create\n  - apiGroups:\n      - policy\n    resources:\n      - poddisruptionbudgets\n    verbs:\n      - list\n      - watch\n  - apiGroups:\n      - certificates.k8s.io\n    resources:\n      - certificatesigningrequests\n    verbs:\n      - list\n      - watch\n  - apiGroups:\n      - discovery.k8s.io\n    resources:\n      - endpointslices\n    verbs:\n      - list\n      - watch\n  - apiGroups:\n      - storage.k8s.io\n    resources:\n      - storageclasses\n      - volumeattachments\n    verbs:\n      - list\n      - watch\n  - apiGroups:\n      - admissionregistration.k8s.io\n    resources:\n      - mutatingwebhookconfigurations\n      - validatingwebhookconfigurations\n    verbs:\n      - list\n      - watch\n  - apiGroups:\n      - networking.k8s.io\n    resources:\n      - networkpolicies\n      - ingressclasses\n      - ingresses\n    verbs:\n      - list\n      - watch\n  - apiGroups:\n      - coordination.k8s.io\n    resources:\n      - leases\n    verbs:\n      - list\n      - watch\n  - apiGroups:\n      - rbac.authorization.k8s.io\n    resources:\n      - clusterrolebindings\n      - clusterroles\n      - rolebindings\n      - roles\n    verbs:\n      - list\n      - watch\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: state-metrics\n  namespace: monitoring\nsubjects:\n- kind: ServiceAccount\n  name: kube-state-metrics\n  namespace: monitoring\nroleRef:\n  kind: ClusterRole\n  name: kube-state-metrics\n  apiGroup: rbac.authorization.k8s.io\n```\n\n#### 02-kube-state-metrics-deploy.yaml\n\n```yaml\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kube-state-metrics\n  namespace: monitoring\n  labels:\n    app: kube-state-metrics\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: kube-state-metrics\n  template:\n    metadata:\n      labels:\n        app: kube-state-metrics\n    spec:\n      automountServiceAccountToken: true\n      serviceAccountName: kube-state-metrics\n      containers:\n      - name: kube-state-metrics\n        image: kubesphere/kube-state-metrics:v2.8.0\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        ports:\n          - name: http-metrics\n            containerPort: 8080\n            protocol: TCP\n          - name: telemetry\n            containerPort: 8081\n            protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n              - ALL\n          readOnlyRootFilesystem: true\n          runAsUser: 65534\n```\n\n#### 03-kube-state-metrics-svc.yaml\n\n```yaml\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: kube-state-metrics\n  namespace: monitoring\n  labels:\n    app: kube-state-metrics\nspec:\n  type: ClusterIP\n  ports:\n    - name: metrics-port\n      port: 8080\n      protocol: TCP\n      targetPort: http-metrics\n    - name: telemetry\n      port: 8081\n      protocol: TCP\n      targetPort: telemetry\n  selector:\n    app: kube-state-metrics\n```\n\n## Prometheus\n\n### <a id=\"rbac\">01-prometheus-rbac.yaml</a>\n\n```yaml\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: prometheus\n  namespace: monitoring\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: prometheus\n  namespace: monitoring\nrules:\n  - apiGroups:\n      - \"\"\n    resources:\n      - nodes\n      - services\n      - endpoints\n      - pods\n      - nodes/proxy\n    verbs:\n      - get\n      - list\n      - watch\n  - apiGroups:\n      - \"extensions\"\n    resources:\n      - ingresses\n    verbs:\n      - get\n      - list\n      - watch\n  - apiGroups:\n      - \"\"\n    resources:\n      - configmaps\n      - nodes/metrics\n    verbs:\n      - get\n  - nonResourceURLs:\n      - /metrics\n    verbs:\n      - get\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: prometheus\n  namespace: monitoring\nsubjects:\n  - kind: ServiceAccount\n    name: prometheus\n    namespace: monitoring\nroleRef:\n  kind: ClusterRole\n  name: prometheus\n  apiGroup: rbac.authorization.k8s.io\n```\n\n### 02-prometheus-cm.yaml\n\n```yaml\n## 除K8S属性如namespace外的修改点\n# 1、target url地址\n\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: prometheus-config\n  namespace: monitoring\ndata:\n  prometheus.yml: |\n    global:\n      scrape_interval:     15s\n      evaluation_interval: 15s\n    rule_files:\n      - /etc/prometheus/rules/*.yml\n    alerting:\n      alertmanagers:\n      \t- static_configs:\n        \t- targets: [\"alertmanager:9093\"]\n    scrape_configs:\n  \t\t- job_name: 'prometheus'\n    \t\tstatic_configs:\n    \t\t\t- targets: ['localhost:9090']\n      - job_name: 'alertmanager'\n    \t\tstatic_configs:\n      \t\t- targets: ['alertmanager:9093']\n```\n\n### <a id=\"rules\">03-prometheus-rules-cm.yaml</a>\n\n```yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: prometheus-rules-config\n  namespace: monitoring\ndata:\n  rules.yml: |\n  groups:\n \t\t- name: instance\n   \t\trules:\n   \t\t\t- alert: InstanceDown # 告警名称\n     \t\t\texpr: up == 0 # 告警的判定条件，参考Prometheus高级查询来设定\n     \t\t\tfor: 10s # 满足告警条件持续时间多久后，才会发送告警\n     \t\t\tlabels: #标签项\n      \t\t\tseverity: error\n     \t\t\tannotations: # 解析项，详细解释告警信息\n      \t\t\tsummary: \"{{$labels.instance}}: has been down\"\n      \t\t\tdescription: \"{{$labels.instance}}: job {{$labels.job}} has been down \"\n```\n\n### 04-prometheus-pvc.yaml\n\n```yaml\n## 除K8S属性如namespace外的修改点\n# 1、storage 存储大小\n# 2、storageclass 名称\n\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: prometheus-data\n  namespace: monitoring\nspec:\n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage: 10Gi\n  storageClassName: <storageclass_name>\n```\n\n### 05-prometheus-deploy.yaml\n\n```yaml\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prometheus\n  namespace: monitoring\n  labels:\n    app: prometheus\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prometheus\n  template:\n    metadata:\n      labels:\n        app: prometheus\n    spec:\n      serviceAccountName: prometheus\n      containers:\n        - name: prometheus\n          image: prom/prometheus\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n            - --config.file=/etc/prometheus/config_out/prometheus.yaml\n            - --storage.tsdb.path=/prometheus/data\n            - --storage.tsdb.retention=24h\n            - --web.enable-lifecycle\n          ports:\n            - name: http-p8s\n              containerPort: 9090\n              protocol: TCP\n          resources:\n            requests:\n              cpu: 100m\n              memory: 100Mi\n            limits:\n              cpu: 500m\n              memory: 2500Mi\n          volumeMounts:\n            - name: prometheus-config-volume\n              mountPath: /etc/prometheus/config_out\n            - name: prometheus-rules-volume\n              mountPath: /etc/prometheus/rules\n            - name: prometheus-data-volume\n              mountPath: /prometheus/data\n      volumes:\n        - name: prometheus-config-volume\n          configMap:\n            name: prometheus-config\n        - name: prometheus-rules-volume\n          configMap:\n            name: prometheus-rules-config\n        - name: prometheus-data-volume\n          persistentVolumeClaim:\n            claimName: prometheus-data\n```\n\n### 06-prometheus-svc.yaml\n\n```yaml\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: prometheus\n  namespace: monitoring\n  labels:\n    app: prometheus\nspec:\n  type: NodePort\n  ports:\n    - name: prometheus-port\n      port: 9090\n      protocol: TCP\n      targetPort: http-p8s\n      nodePort: 31003\n  selector:\n    app: prometheus\n```\n\n## Thanos\n\n> 本文采用 sidecar 模式，把 prometheus 和 sider 服务部署在一起，query 服务提供跟 prometheus 一样的查询界面，store 服务储存到 minio，compact 服务压缩数据文件，因为 prometheus 已经配置 rule 告警规则，故 thanos 的 rules 服务不在部署，仅供参考。（如果部署 rules 的话，可以在 query 界面中既包含 prometheus 本身的告警规则，也可以看到 rules 服务的规则），详情参考 [Thanos-io/kube-thanso](https://github.com/thanos-io/kube-thanos/tree/main/examples/all/manifests)\n\n### 01-prometheus-rbac.yaml\n\n> 和 [prometheus 的 rbac](#rbac) 保持一致即可\n\n### 02-prometheus-tp-cm.yaml\n\n> 可以由稍后的 `05-prometheus-sidecar-sts.yaml` 看出，prometheus 仅提供 `后缀为tmpl的模版配置文件` 由 sidecar 解析后生成 `后缀为yaml的标准配置文件` ，所以此 configmap 只是文件名不同，配置项也和 prometheus 的一致。\n\n```yaml\n## 除K8S属性如namespace外的修改点\n# 1、target url地址\n\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: prometheus-config-tmpl\n  namespace: monitoring\n  labels:\n    app: prometheus\ndata:\n  prometheus.yaml.tmpl: |\n    global:\n      scrape_interval:     15s\n      evaluation_interval: 15s\n    rule_files:\n      - /etc/prometheus/rules/*.yml\n    alerting:\n      alertmanagers:\n      \t- static_configs:\n        \t- targets: [\"alertmanager:9093\"]\n    scrape_configs:\n  \t\t- job_name: 'prometheus'\n    \t\tstatic_configs:\n    \t\t\t- targets: ['localhost:9090']\n      - job_name: 'alertmanager'\n    \t\t- static_configs:\n      \t\t- targets: ['alertmanager:9093']\n```\n\n### 03-prometheus-rules-cm.yaml\n\n> 如果使用 prometheus 自己告警规则而不是 thanos-rules 的话，和 [prometheus 的 rules](#rules) 保持一致即可。\n\n### 04-thanos-storage-secret.yaml\n\n```yaml\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: thanos-objectstorage\n  namespace: monitoring\ntype: Opaque\nstringData:\n  objectstorage.yaml: |\n    type: S3\n    config:\n      endpoint: \"<minio_svc_name>(.<minio_namspace_name>.svc.cluster.local):<minio_svc_port>\"\n      bucket: \"<bucket_name>\"\n      access_key: \"\"\n      secret_key: \"\"\n      insecure: true\n```\n\n### 05-prometheus-sidecar-sts.yaml\n\n```yaml\n---\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: prometheus-sidecar\n  namespace: monitoring\n  labels:\n    app: prometheus-sidecar\nspec:\n  replicas: 2\n  serviceName: prometheus-sidebar-headless\n  podManagementPolicy: Parallel\n  selector:\n    matchLabels:\n      app: prometheus-sidecar\n  template:\n    metadata:\n      labels:\n        app: prometheus-sidecar\n    spec:\n      serviceAccountName: prometheus\n      containers:\n        - name: prometheus\n          image: prom/prometheus\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n            - --config.file=/etc/prometheus/config_out/prometheus.yaml\n            - --storage.tsdb.path=/prometheus/data\n            - --storage.tsdb.retention=24h\n            - --web.enable-lifecycle\n            - --storage.tsdb.no-lockfile\n            - --storage.tsdb.min-block-duration=2h\n            - --storage.tsdb.max-block-duration=2h\n          ports:\n            - name: http-p8s\n              containerPort: 9090\n              protocol: TCP\n          livenessProbe:\n            failureThreshold: 6\n            httpGet:\n              path: /-/healthy\n              port: http-p8s\n              scheme: HTTP\n            periodSeconds: 5\n            successThreshold: 1\n            timeoutSeconds: 3\n          readinessProbe:\n            failureThreshold: 120\n            httpGet:\n              path: /-/ready\n              port: http-p8s\n              scheme: HTTP\n            periodSeconds: 5\n            successThreshold: 1\n            timeoutSeconds: 3\n          volumeMounts:\n            - name: prometheus-config-volume\n              mountPath: /etc/prometheus/config_out\n            - name: prometheus-rules-volume\n              mountPath: /etc/prometheus/rules\n            - name: prometheus-data-volume\n              mountPath: /prometheus/data\n        - name: thanos-sidecar\n          image: quay.io/thanos/thanos:v0.30.2\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n            - sidecar\n            - --log.level=debug\n            - --tsdb.path=/prometheus/data\n            - --prometheus.url=http://127.0.0.1:9090\n            - --grpc-address=0.0.0.0:10901\n            - --http-address=0.0.0.0:10902\n            - --objstore.config-file=/etc/thanos/objectstorage.yaml\n            - --reloader.config-file=/etc/prometheus/config/prometheus.yaml.tmpl\n            - --reloader.config-envsubst-file=/etc/prometheus/config_out/prometheus.yaml\n            - --reloader.rule-dir=/etc/prometheus/rules/\n          env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n          ports:\n            - name: grpc\n              containerPort: 10901\n              protocol: TCP\n            - name: http\n              containerPort: 10902\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              port: http\n              path: /-/healthy\n          readinessProbe:\n            httpGet:\n              port: http\n              path: /-/ready\n          volumeMounts:\n            - name: prometheus-config-tmpl-volume\n              mountPath: /etc/prometheus/config\n            - name: prometheus-config-volume\n              mountPath: /etc/prometheus/config_out\n            - name: prometheus-rules-volume\n              mountPath: /etc/prometheus/rules\n            - name: thanos-objectstorage\n              mountPath: /etc/thanos/objectstorage.yaml\n              subPath: objectstorage.yaml\n            - name: prometheus-data-volume\n              mountPath: /prometheus/data\n      securityContext:\n        fsGroup: 2000\n        runAsNonRoot: true\n        runAsUser: 1000\n      volumes:\n        - name: prometheus-config-tmpl-volume\n          configMap:\n            name: prometheus-config-tmpl\n        - name: prometheus-config-volume\n          emptyDir: {}\n        - name: prometheus-rules-volume\n          configMap:\n            name: prometheus-rules-config\n        - name: thanos-objectstorage\n          secret:\n            secretName: thanos-objectstorage\n  volumeClaimTemplates:\n    - metadata:\n        name: prometheus-data-volume\n        labels:\n          app: prometheus\n      spec:\n        accessModes:\n          - ReadWriteMany\n        storageClassName: <storageclass_name>\n        resources:\n          requests:\n            storage: 20Gi\n```\n\n### 06-prometheus-sidecar-svc.yaml\n\n```yaml\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: prometheus-sidecar-headless\n  namespace: monitoring\n  labels:\n    app: prometheus-sidecar\nspec:\n  clusterIP: None\n  ports:\n    - name: grpc\n      port: 10901\n      targetPort: grpc\n    - name: http\n      port: 10902\n      targetPort: http\n  selector:\n    app: prometheus-sidecar\n```\n\n### 07-thanos-store-sts.yaml\n\n```yaml\n---\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: thanos-store\n  namespace: monitoring\n  labels:\n    app: thanos-store\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: thanos-store\n  serviceName: thanos-store-headless\n  podManagementPolicy: Parallel\n  template:\n    metadata:\n      labels:\n        app: thanos-store\n    spec:\n      containers:\n        - name: thanos-store\n          image: quay.io/thanos/thanos:v0.30.2\n          imagePullPolicy: IfNotPresent\n          args:\n            - store\n            - --log.level=debug\n            - --data-dir=/var/thanos/store\n            - --grpc-address=0.0.0.0:10901\n            - --http-address=0.0.0.0:10902\n            - --objstore.config-file=/etc/thanos/objectstorage.yaml\n          ports:\n            - name: grpc\n              containerPort: 10901\n              protocol: TCP\n            - name: http\n              containerPort: 10902\n              protocol: TCP\n          livenessProbe:\n            failureThreshold: 8\n            httpGet:\n              path: /-/healthy\n              port: http\n              scheme: HTTP\n            periodSeconds: 30\n            timeoutSeconds: 1\n          readinessProbe:\n            failureThreshold: 20\n            httpGet:\n              path: /-/ready\n              port: http\n              scheme: HTTP\n            periodSeconds: 5\n          resources:\n            limits:\n              cpu: 0.42\n              memory: 420Mi\n            requests:\n              cpu: 0.123\n              memory: 123Mi\n          terminationMessagePolicy: FallbackToLogsOnError\n          volumeMounts:\n            - mountPath: /var/thanos/store\n              name: thanos-store-data\n              readOnly: false\n            - name: thanos-objectstorage\n              mountPath: /etc/thanos/objectstorage.yaml\n              subPath: objectstorage.yaml\n      terminationGracePeriodSeconds: 120\n      volumes:\n        - name: thanos-objectstorage\n          secret:\n            secretName: thanos-objectstorage\n  volumeClaimTemplates:\n    - metadata:\n        name: thanos-store-data\n        labels:\n          app: thanos-store\n      spec:\n        accessModes:\n          - ReadWriteMany\n        storageClassName: <storageclass_name>\n        resources:\n          requests:\n            storage: 10Gi\n```\n\n### 08-thanos-store-svc.yaml\n\n```yaml\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: thanos-store-headless\n  namespace: monitoring\n  labels:\n    app: thanos-store\nspec:\n  clusterIP: None\n  ports:\n    - name: grpc\n      port: 10901\n      protocol: TCP\n      targetPort: grpc\n    - name: store\n      port: 10902\n      protocol: TCP\n      targetPort: http\n  selector:\n    app: thanos-store\n```\n\n### 09-thanos-compact-sts.yaml\n\n```yaml\n---\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: thanos-compact\n  namespace: monitoring\n  labels:\n    app: thanos-compact\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: thanos-compact\n  serviceName: thanos-compact-headless\n  template:\n    metadata:\n      labels:\n        app: thanos-compact\n    spec:\n      containers:\n        - name: thanos-compact\n          image: quay.io/thanos/thanos:v0.30.2\n          args:\n            - compact\n            - --wait\n            - --log.level=info\n            - --log.format=logfmt\n            - --objstore.config-file=/etc/thanos/objectstorage.yaml\n            - --data-dir=/var/thanos/compact\n            - --debug.accept-malformed-index\n            - --retention.resolution-raw=90d\n            - --retention.resolution-5m=180d\n            - --retention.resolution-1h=360d\n          ports:\n            - name: http\n              containerPort: 10902\n              protocol: TCP\n          livenessProbe:\n            failureThreshold: 4\n            httpGet:\n              path: /-/healthy\n              port: http\n              scheme: HTTP\n            periodSeconds: 30\n          readinessProbe:\n            failureThreshold: 20\n            httpGet:\n              path: /-/ready\n              port: http\n              scheme: HTTP\n            periodSeconds: 5\n          resources:\n            limits:\n              cpu: 0.42\n              memory: 420Mi\n            requests:\n              cpu: 0.123\n              memory: 123Mi\n          terminationMessagePolicy: FallbackToLogsOnError\n          volumeMounts:\n            - name: thanos-compact-data\n              mountPath: /var/thanos/compact\n              readOnly: false\n            - name: thanos-objectstorage\n              mountPath: /etc/thanos/objectstorage.yaml\n              subPath: objectstorage.yaml\n      terminationGracePeriodSeconds: 120\n      volumes:\n        - name: thanos-objectstorage\n          secret:\n            secretName: thanos-objectstorage\n  volumeClaimTemplates:\n    - metadata:\n        name: thanos-compact-data\n        labels:\n          app: thanos-compact\n      spec:\n        accessModes:\n          - ReadWriteOnce\n        storageClassName: <storageclass_name>\n        resources:\n          requests:\n            storage: 100Gi\n```\n\n### 10-thanos-compact-svc.yaml\n\n```yaml\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: thanos-compact-headless\n  namespace: monitoring\n  labels:\n    app: thanos-compact\nspec:\n  clusterIP: None\n  ports:\n    - name: http\n      port: 10902\n      protocol: TCP\n      targetPort: http\n  selector:\n    app: thanos-compact\n```\n\n### 11-thanos-query-deploy.yaml\n\n> 由于 `query` 需要连接 `sidecar` 还有 `store`（部署 `rules`  的话也要），在这里我最后部署 `query` 服务，虽然先部署 `query` 它也会自动去重连 `store` 或者 `rules`\n\n```yaml\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: thanos-query\n  namespace: monitoring\n  labels:\n    app: thanos-query\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: thanos-query\n  template:\n    metadata:\n      labels:\n        app: thanos-query\n    spec:\n      containers:\n        - name: thanos-query\n          image: quay.io/thanos/thanos:v0.30.2\n          imagePullPolicy: IfNotPresent\n          args:\n            - query\n            - --grpc-address=0.0.0.0:10901\n            - --http-address=0.0.0.0:9090\n            - --log.level=debug\n            - --log.format=logfmt\n            - --query.replica-label=prometheus_replica\n            - --query.replica-label=rule_replica\n            - --store=dnssrv+_grpc._tcp.prometheus-sidecar-headless\n            - --store=dnssrv+_grpc._tcp.thanos-store-headless\n            - --query.timeout=5m\n            - --query.lookback-delta=15m\n            - --query.auto-downsampling\n          ports:\n            - name: http-query\n              containerPort: 9090\n              protocol: TCP\n            - name: grpc\n              containerPort: 10901\n              protocol: TCP\n          livenessProbe:\n            failureThreshold: 4\n            httpGet:\n              path: /-/healthy\n              port: http-query\n              scheme: HTTP\n            periodSeconds: 30\n          readinessProbe:\n            failureThreshold: 20\n            httpGet:\n              path: /-/ready\n              port: http-query\n              scheme: HTTP\n            periodSeconds: 5\n```\n\n### 12-thanos-query-svc.yaml\n\n```yaml\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: thanos-query\n  namespace: monitoring\n  labels:\n    app: thanos-query\nspec:\n  type: NodePort\n  ports:\n    - name: query-port\n      port: 9090\n      protocol: TCP\n      targetPort: http-query\n      nodePort: 31990\n    - name: grpc\n      port: 10901\n      protocol: TCP\n      targetPort: grpc\n      nodePort: 31991\n  selector:\n    app: thanos-query\n```\n\n> 以下 rules 服务为可选项\n\n### 13-thanos-rules-cm.yaml\n\n```yaml\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: thanos-rules-config\n  namespace: monitoring\n  labels:\n    name: thanos-rules\ndata:\n  record.rules.yaml: |-\n    groups:\n    - name: k8s.rules\n      rules:\n      - expr: |\n          sum(rate(container_cpu_usage_seconds_total{job=\"cadvisor\", image!=\"\", container!=\"\"}[5m])) by (namespace)\n        record: namespace:container_cpu_usage_seconds_total:sum_rate\n      - expr: |\n          sum(container_memory_usage_bytes{job=\"cadvisor\", image!=\"\", container!=\"\"}) by (namespace)\n        record: namespace:container_memory_usage_bytes:sum\n      - expr: |\n          sum by (namespace, pod, container) (\n            rate(container_cpu_usage_seconds_total{job=\"cadvisor\", image!=\"\", container!=\"\"}[5m])\n          )\n        record: namespace_pod_container:container_cpu_usage_seconds_total:sum_rate\n```\n\n### 14-thanos-rules-sts.yaml\n\n```yaml\n---\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: thanos-rule\n  namespace: monitoring\n  labels:\n    app: thanos-rule\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: thanos-rule\n  serviceName: thanos-rule-headless\n  podManagementPolicy: Parallel\n  template:\n    metadata:\n      labels:\n        app: thanos-rule\n    spec:\n      containers:\n        - name: thanos-rule\n          image: quay.io/thanos/thanos:v0.30.2\n          imagePullPolicy: IfNotPresent\n          args:\n            - rule\n            - --log.level=info\n            - --log.format=logfmt\n            - --grpc-address=0.0.0.0:10901\n            - --http-address=0.0.0.0:10902\n            - --rule-file=/etc/thanos/rules/*rules.yaml\n            - --objstore.config-file=/etc/thanos/objectstorage.yaml\n            - --data-dir=/var/thanos/rule\n            - --label=rule_replica=\"$(NAME)\"\n            - --alert.label-drop=\"rule_replica\"\n            - --tsdb.retention=48h\n            - --tsdb.block-duration=2h\n            - --query=dnssrv+_http._tcp.thanos-query\n          env:\n          - name: NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          ports:\n            - name: grpc\n              containerPort: 10901\n              protocol: TCP\n            - name: http\n              containerPort: 10902\n              protocol: TCP\n          livenessProbe:\n            failureThreshold: 24\n            httpGet:\n              path: /-/healthy\n              port: http\n              scheme: HTTP\n            periodSeconds: 5\n          readinessProbe:\n            failureThreshold: 18\n            httpGet:\n              path: /-/ready\n              port: http\n              scheme: HTTP\n            initialDelaySeconds: 10\n            periodSeconds: 5\n          resources:\n            limits:\n              cpu: 0.42\n              memory: 420Mi\n            requests:\n              cpu: 0.123\n              memory: 123Mi\n          terminationMessagePolicy: FallbackToLogsOnError\n          volumeMounts:\n            - name: thanos-rules-data\n              mountPath: /var/thanos/rule\n              readOnly: false\n            - name: thanos-objectstorage\n              mountPath: /etc/thanos/objectstorage.yaml\n              subPath: objectstorage.yaml\n            - name: thanos-rules-config-volume\n              mountPath: /etc/thanos/rules\n      volumes:\n        - name: thanos-objectstorage\n          secret:\n            secretName: thanos-objectstorage\n        - name: thanos-rules-config-volume\n          configMap:\n            name: thanos-rules-config\n  volumeClaimTemplates:\n    - metadata:\n        name: thanos-rules-data\n        labels:\n          app: thanos-rule\n      spec:\n        accessModes:\n          - ReadWriteMany\n        storageClassName: <storageclass_name>\n        resources:\n          requests:\n            storage: 100Gi\n```\n\n### 15-thanos-rules-svc.yaml\n\n```yaml\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: thanos-rule-headless\n  namespace: monitoring\n  labels:\n    app: thanos-rule\nspec:\n  clusterIP: None\n  ports:\n    - name: grpc\n      port: 10901\n      protocol: TCP\n      targetPort: grpc\n    - name: http\n      port: 10902\n      protocol: TCP\n      targetPort: http\n  selector:\n    app: thanos-rule\n```\n","tags":["K8S","容器","监控告警","日志收集"],"categories":["devops"]},{"title":"K8S部署方式简介","url":"/posts/4fc885b8.html","content":"<!-- block -->\n>&emsp;&emsp;如今 Kubernetes 可谓是最为火热的云原生技术，即使没有用过，也一定听过这个名词。而对于运维而言，已经基本成为行业要求的刚需。不要求精通（毕竟任何技术都有一定的要求），但必须掌握基本原理、部署及基本使用。之前也写过 K8S(Kubernetes 单词首尾之前还有 8 个字母，类似还有 Prometheus 的 P8S)的部署手册，自我学习的两三年过去，对此有了更深的一些体会，整理如下。\n<!-- block -->\n\n### 原生方式\n\n#### [Minikube](https://minikube.sigs.k8s.io/docs/start/)/[Kind](https://kind.sigs.k8s.io/docs/user/quick-start/)\n\n此类方式只适合学习环境快速接触 K8S，了解即可。\n\n#### 二进制\n\n安装部署的主要过程如下：\n\n- 初始化机器\n\n- 生成证书及完成自签\n\n- Etcd 节点通过二进制文件（系统服务）部署 etcd\n\n- Master 节点通过二进制文件（系统服务）部署 Apiserver、Controller、Schedule 等组件\n\n- Node 节点通过二进制文件（系统服务）部署 Docker、Kubelet、kube-proxy 等组件\n\n- 添加 CNI 插件\n\n  详细的部署过程可以参考拙文 [通过二进制方式部署 Kubernetes 集群](https://limerencist.github.io/posts/89a5c5a5.html)\n\n#### [Kubeadm](https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/install-kubeadm/)\n\n安装部署的主要过程如下：\n\n- 初始化机器\n\n- 所有节点先安装 Docker、Kubeadm、Kubelet 等组件\n\n- 使用 Kubeadm 初始化，自动添加证书，使用容器化部署 Etcd、Apiserver、Controller 等组件（这一步其实也需要区分节点类型，只不过都是 Kubeadm 简单化了）\n\n- 添加 CNI 插件\n\n  详细的部署过程可以参考拙文 [通过 Kubeadm 部署 Kubernetes 单主多从集群](https://limerencist.github.io/posts/50bdf01d.html) 和 [通过 Kubeadm 部署 Kubernetes 高可用集群](https://limerencist.github.io/posts/43012f97.html)\n\n  **_需要注意的是，K8S 从 V1.24 版本开始移除默认对 Docker 的支持，使用 Containerd 的方式，写这几篇部署文档时候还没有出现这种情况，仍以 Docker 为主，后续看个人情况是否进行完成，所以仅供参考_**\n\n#### 总结\n\n> - 通过 Kubeadm 可以简单、快速地搭建 K8S 集群，易于添加/删除节点，但是封装成容器对应用者屏蔽一些底层的逻辑。\n>\n> - 通过二进制部署比较麻烦，却更好地了解每一个步骤及组件的作用，有利于后续出问题时的排查。\n>\n> - 对于集群高可用即多个主节点的情况，也需要手动完成额外操作。\n\n### 优化方式\n\n对原生的方式，个人或团队进行优化（比如国内镜像地址）和自动化封装，包含且不限于基于二进制的 _[Kubeasz](https://github.com/easzlab/kubeasz)_ 和基于 Kubeadm 的 _[Kubeadm HA](https://github.com/TimeBye/kubeadm-ha)_\n\n> 此类工具一般具有以下特点：\n>\n> - 支持离线化部署\n>\n> - 利用 ansible-playbook 实现自动化，既可以一键安装，也可以安装各个组件\n>\n> - 组件可选化，比如 Docker/Containerd，Calico/Flannel 等\n>\n> - 适配多个操作系统和 CPU 架构\n>\n> - 较为新的版本，高于 V1.20、V1.22\n\n### 企业级应用\n\n在优化原生方式的基础上，再加以可视化、热插拔组件等额外功能的企业级应用，包括切不限于以下几种：\n\n|                   | [Rancher](https://www.rancher.com/) | [Kuboard](https://www.kuboard.cn/)                                      | [KubeCube](https://www.kubecube.io/) | [KubeSphere](https://kubesphere.io/zh/)                                                        | [KubeOperator](https://kubeoperator.io/)             |\n| ----------------- | ----------------------------------- | ----------------------------------------------------------------------- | ------------------------------------ | ---------------------------------------------------------------------------------------------- | ---------------------------------------------------- |\n| 部署 K8S 方式     | 可视化 Rancher<br>RKE 发行版        | 可视化 [Kuboard-Spray](https://www.kuboard.cn/install/install-k8s.html) | 命令行脚本                           | 命令行 [kubekey](https://kubesphere.io/zh/docs/v3.3/installing-on-linux/introduction/kubekey/) | 可视化 [KubeOperator](https://kubeoperator.io/docs/) |\n| Dashboard         | Rancher                             | Kuboard                                                                 | KubeCube                             | KubeSphere                                                                                     | KubePi                                               |\n| 是否支持 Arm 架构 |                                     | 支持                                                                    | 支持                                 | 不支持                                                                                         | 支持                                                 |\n| 离线化部署程度    | 需要自建私库及准备镜像              | 提供部分离线资源包                                                      |                                      | 需要自建私库及准备镜像                                                                         | 提供完整的离线安装包                                 |\n| 支持 K8S 版本     |                                     | v1.23.1~v1.25.6                                                         | v1.18.20 ~ v1.23.5                   | v1.19~v1.24                                                                                    | v1.18.3~v1.22.12                                     |\n| 开源许可证        | Apache2.0                           | 部署工具 Apache2.0                                                      | Apache2.0                            | Apache2.0                                                                                      | Apache2.0                                            |\n\n#### 总结\n\n> 此类工作一般具有以下特点：\n>\n> - 开箱即用\n> - 提供可视化的安装界面，降低部署难度\n>\n> - 部署完成后默认自带或可以安装对应的 Dashboard，Dashboard 支持管理多集群\n>\n> - 需要考虑开源性及是否商用\n\n### 封装容器\n\n可以看出，从原生方式到企业级应用，部署 K8S 的过程，是一步一步封装、简化、丰富的。那么，容器化的思想更近一步，把这个 K8S 集群当做一个整体，便是 [sealos](https://www.sealyun.com/zh-Hans/) 和 [sealor](http://sealer.cool/v0.8.6/zh/) 主要做的事情。省去了大部分的步骤和配置，以镜像的方式，只需几条命令便可以构建一个 K8S 集群。\n\n不过，越是傻瓜的操作，意味着内部封装的程度越高，越不容易接触到里面的核心。不出问题还好，出了问题又不清楚原理，简直无从下手。况且这种方式远没有基于/改造原生方式成熟，慎用。\n\n### 云服务\n\n除了自己搭建 K8S 环境外，也可以使用云厂商提供的服务，且不需要提前规划服务器节点(之前的方式都需要区分节点角色)。总的来说大差不差，都是云厂商有底层的硬件和技术支撑，花钱买服务就行了。\n\n#### [阿里 ACK](https://www.aliyun.com/product/kubernetes?spm=5176.19720258.J_3207526240.33.e93976f4aVYqP6)\n\n#### [华为 CCE](https://www.huaweicloud.com/product/cce.html)\n\n#### [腾讯 TKE](https://cloud.tencent.com/product/tke)\n\n---\n\n&emsp;&emsp;在我学习 K8S 集群的初期，常常看到 `开发 Kubeadm，运维二进制` 的说法。不论以何种方式，部署只是第一步，学习之路，道阻且长。从这几种方式来看，技术都是一步一步完善的，善于利用并借鉴已经成熟的东西，亦不失为一种好的学习方法。","tags":["K8S","容器"],"categories":["devops"]},{"title":"Band","url":"/posts/e8ed0dd5.html","content":"<!-- block -->\n>&emsp;&emsp;话不多说，就是喜欢的一些乐队。\n<!-- block -->\n\n# [樂隊集合1](https://music.163.com/playlist?id=8050230658&userid=622029058 \"樂隊集合1\")\n{% meting \"8050230658\" \"netease\" \"playlist\" \"autoplay \" \"mutex:true\" \"listmaxheight:500px\" \"preload:auto\" \"theme:#ad7a86\"  \"loop:all\" \"order:random\" %}\n\n# [樂隊集合2](https://music.163.com/playlist?id=9538035967&userid=622029058 \"樂隊集合2\")\n","tags":["万能青年旅店","搖滾","告五人","八仙飯店","聲無哀樂","一奏器樂派"],"categories":["playlist"]},{"title":"Hexo常用插件汇总","url":"/posts/5438079.html","content":"<!-- block -->\n>&emsp;&emsp;在上一篇 [搭建 Hexo 博客](https://limerencist.github.io/c1d5d64c/) 中，发布 `Github Page` 时用到了 `hexo-deployer-git` 插件，这里再介绍一些其他插件。可能大多数主题里已经包含了，本着“知其所以然”的态度，也稍稍探究一下。\n<!-- block -->\n>&emsp;&emsp;以下插件主要分为即用型和改造型，前者下载插件加以简单配置后就可以直接使用；后者还需要修改前端页面代码的内容，我还没有这个实力，推荐给比较硬核的玩家，或者直接使用已经包含插件的主题。\n\n# 即用型\n\n## 更换 markdown 渲染器\n\n更换渲染器的原因，是我在按照语法编辑文档后，发现页面没有显示对应的效果，而是源代码或者标签，比如缩进两字符、文本右对齐、字体颜色等。参考大佬的博客 [Hexo 多种 Markdown 渲染器对比分析](https://zsyyblog.com/b73ceb85.html) 后，选用了 `hexo-renderer-markdown-it-plus` 渲染器，完美实现预期的效果。\n\n```bash\nnpm un hexo-renderer-marked --save\nnpm i hexo-renderer-markdown-it-plus --save\n```\n\n## 优化文章路径\n\nhexo 默认的文章永久链接包含时间和标题，我认为 url 里还是尽量少出现中文和特殊字符，使用 `hexo-abbrlink` 可以生成带有数字的链接来规避。\n\n```bash\nnpm i hexo-abbrlink --save\n```\n\n在\\_config.yml 中修改\n\n```yml\npermalink: :abbrlink/\n# or\npermalink: posts/:abbrlink.html\n```\n\n在\\_config.yml 中加入\n\n```yml\n# abbrlink config\nabbrlink:\n  alg: crc32      #support crc16(default) and crc32\n  rep: hex        #support dec(default) and hex\n  drafts: false   #(true)Process draft,(false)Do not process draft. false(default)\n  # Generate categories from directory-tree\n  # depth: the max_depth of directory-tree you want to generate, should > 0\n  auto_category:\n     enable: true  #true(default)\n     depth:        #3(default)\n     over_write: false\n  auto_title: false #enable auto title, it can auto fill the title by path\n  auto_date: false #enable auto date, it can auto fill the date by time today\n  force: false #enable force mode,in this mode, the plugin will ignore the cache, and calc the abbrlink for every post even it already had abbrlink. This only updates abbrlink rather than other front variables.\n```\n\n详细配置参考官方文档 [GitHub - rozbo/hexo-abbrlink: create one and only link for every post for hexo](https://github.com/rozbo/hexo-abbrlink)\n\n## 播放音乐\n\n音乐爱好者必备插件，既可以使用单独页面播放歌单，也可以在文章里播放单曲。\n\n```bash\nnpm i hexo-tag-aplayer --save\n```\n\n在 `_config.yml` 中加入\n\n```yml\n# hexo-tag-aplayer\naplayer:\n  meting: true\n```\n\n在 md 文章中加入\n\n```html\n<!-- Simple example (id, server, type)  -->\n{% meting \"60198\" \"netease\" \"playlist\" %}\n```\n\n详细配置参考官方文档 [GitHub - MoePlayer/hexo-tag-aplayer: Embed aplayer in Hexo posts/pages](https://github.com/MoePlayer/hexo-tag-aplayer)\n\n## 压缩资源\n\n压缩 css/html/图片 等资源\n\n```bash\nnpm i hexo-all-minifier --save\n```\n\n在 `_config.yml` 中加入\n\n```yml\n# hexo-all-minifier\nall_minifier: true\n```\n\n## 摘要\n\n```bash\nnpm i hexo-auto-excerpt --save\n```\n\n与上面自动生成摘要相比，以下这个插件可以自定义摘要的内容，更加自由\n\n```bash\nnpm i hexo-excerpt-block  --save\n```\n\n在文章中加入\n\n```html\n<!-- block -->\n这里是摘要\n<!-- block -->\n```\n\n# 改造型\n\n## 本地全局搜索\n\n```bash\nnpm i hexo-generator-searchdb --save\n```\n\n在 `_config.yml` 中加入\n\n```yml\nsearch:\n  path: search.xml\n  field: all\n  content: true\n  format: html\n```\n\n具体如何使用移步 [GitHub - next-theme/hexo-generator-searchdb: 🔍 Seach data generator plugin for Hexo.](https://github.com/next-theme/hexo-generator-searchdb)\n\n## 统计字数与时长\n\n```bash\nnpm i hexo-word-counter --save\n```\n\n在 `_config.yml` 中加入\n\n```yml\n# hexo-word-counter\nsymbols_count_time:\n  symbols: true\n  time: true\n  total_symbols: false\n  total_time: false\n  exclude_codeblock: false\n  awl: 4\n  wpm: 275\n  suffix: \"mins.\"\n```\n\n具体如何使用移步 [GitHub - next-theme/hexo-word-counter: ⏱️ Word count and time to read of articles for Hexo.](https://github.com/next-theme/hexo-word-counter)\n\n##","tags":["Hexo"],"categories":["devops"]},{"title":"搭建Hexo博客","url":"/posts/c1d5d64c.html","content":"<!-- block -->\n>&emsp;&emsp;博客的95%终于搭好了，剩下的5%估計又要拖延……\n>&emsp;&emsp;搭博客的原因：一部分是最近比较闲，有时间可以玩一玩，看到大佬都有自己的博客，也顺便调研了一下各个开源框架，最终敲定了 `Hexo` ——相比较其它傻瓜式操作，`Hexo` 显得比较硬核，能体现出我是个搞技术的；其次是足夠成熟，用的人也多，有问题容易解決；再就是有花里胡哨的主題，一年365天一天换一套都不是問題。小弟不才，先总结下使用 `Hexo` 搭建博客的主要过程，也方便新手入个门。\n<!-- block -->\n\n# git\n\n## 安装\n\n```bash\napt/yum/dnf install git -y\n```\n\n# nodejs\n\n## 下载\n\n```bash\nwget https://nodejs.org/dist/v18.12.1/node-v18.12.1-linux-x64.tar.xz\n```\n\n## 解压\n\n```bash\ntar xf node-v18.12.1-linux-x64.tar.xz -C /opt\n```\n\n## 配置环境变量\n\n```bash\necho \"PATH=\\$PATH:/opt/node-v18.12.1-linux-x64\" >> /etc/profile\n. /etc/profile\n```\n\n## 添加淘宝源\n\n```bash\nnpm config set registry https://registry.npmmirror.com\n```\n\n# hexo\n\n## 安装脚手架\n\n```bash\nnpm i -g hexo-cli\n```\n\n## 建站\n\n```bash\ncd /opt\nhexo init hexo\nnpm i\n```\n\n## 参考文档修改\\_config.yml\n\n[配置 | Hexo](https://hexo.io/zh-cn/docs/configuration)\n\n## 修改模板字段\n\n```bash\nvim scaffolds/post.md 添加以下内容\n    categories:\n```\n\n## 自行切换主题并参考主题说明修改相应配置\n\n# command\n\n[指令 | Hexo](https://hexo.io/zh-cn/docs/commands)\n\n# release\n\n## pm2\n\n### 安装 pm2\n\n```bash\nnpm i -g pm2\n```\n\n### 编写脚本\n\n#### hexo-deploy.js\n\n```js\n//deploy\nconst { exec } = require('child_process')\nexec('hexo g -d (-w 用户监控文件改动情况自动发布)',(error, stdout, stderr) => {\n        if(error){\n                console.log('exec error: ${error}')\n                return\n        }\n        console.log('stdout: ${stdout}');\n        console.log('stderr: ${stderr}');\n})\n```\n\n#### hexo-run.js\n\n```js\n//run\nconst { exec } = require('child_process')\nexec('hexo s (-p 80 用户指定端口)',(error, stdout, stderr) => {\n        if(error){\n                console.log('exec error: ${error}')\n                return\n        }\n        console.log('stdout: ${stdout}');\n        console.log('stderr: ${stderr}');\n})\n```\n\n### 启动后台任务\n\n```bash\npm2 start hexo-deploy.js\npm2 start hexo-run.js\n```\n\n**注：使用 nginx 发布时，不需要使用 pm2 启动 hexo-run.js 或者通过 hexo s 发布**\n\n## nginx\n\n### 安装 nginx\n\n```bash\napt/yum/dnf install nginx -y\n```\n\n### 添加路由配置\n\n```nginx\nroot /opt/hexo/public;\n\nlocation / {\n   index index.html;\n}\n```\n\n### 启动 nginx\n\n```bash\nsystemctl start nginx\n```\n\n**可以使用 pm2 后台控制 hexo 的构建，nginx 配置路由及 https 证书**\n\n## github page\n\n### 生成密钥\n\n```bash\nssh-keygen -t rsa -C github邮箱地址\n\ncat id_rsa.pub\n```\n\n### 在 github 页码中配置密钥\n\n### 安装插件\n\n```bash\nnpm i hexo-deployer-git --save\n```\n\n### 修改配置\n\n```yml\ndeploy:\n  type: git\n  repo: <repository url>  # ssh地址\n  branch: <branch name>   # master\n```\n\n### 配置 git 信息\n\n```bash\ngit config --global user.name \"github用户名\"\ngit config --global user.email \"github邮箱地址\"\n```\n\n### 构建发布\n\n```bash\nhexo clean\nhexo g -d\n```\n","tags":["Hexo"],"categories":["devops"]},{"title":"手动搭建Train版OpenStack","url":"/posts/6c8009ea.html","content":"<!-- block -->\n>&emsp;&emsp;学习搭建 `OpenStack`\n<!-- block -->\n\n# Enviroment\n\n|  hostname  |    os     |                                    static ip                                     |      role      |\n| :--------: | :-------: | :------------------------------------------------------------------------------: | :------------: |\n| controller | centos7.8 | 网卡 1Nat 模式静态 10.10.10.101；网卡 2 仅主机模式不设 IP，网关地址 192.168.40.1 |      主控      |\n|  computer  | centos7.8 | 网卡 1Nat 模式静态 10.10.10.102；网卡 2 仅主机模式不设 IP，网关地址 192.168.40.1 |      计算      |\n|   block    | centos7.8 |                                   10.10.10.103                                   | 块储存，双硬盘 |\n\n# Prerequisites\n\n```shell\n#临时关闭防火墙\nsystemctl stop firewalld\n#禁用防火墙，重启生效\nsystemctl disable firewalld\n\n#临时关闭selinux\nsetenforce 0\n#禁用selinux,重启生效\nsed -i \"s/enforcing/disabled/g\" /etc/selinux/config\n\n#安装常用软件包\nyum install vim net-tools lrzsz unzip dos2unix telnet sysstat iotop pciutils lsof tcpdump psmisc bc wget socat gcc tree chrony ntpdate -y\n\n#主机名写入hosts\nsed -i '$a\\10.10.10.101 controller\\n10.10.10.102 compute\\n10.10.10.103 block' /etc/hosts\n\n#主机间配置ssh免密登录\nssh-keygen -t rsa\nfor host in master backup node ; do ssh-copy-id -i ~/.ssh/id_rsa.pub $host;done\n```\n\n## [Network Time Protoco](https://docs.openstack.org/install-guide/environment-ntp.html#)\n\n### controller\n\n```shell\nyum install chrony -y\n\nvim /etc/chrony.conf\nallow 10.10.0.0/16 (主控节点网段)\n\nfor op in enable restart status;do systemctl $op chronyd;done\n```\n\n### computer,block\n\n```shell\nyum install chrony -y\n\nvim /etc/chrony.conf\nserver controller iburst\n\nfor op in enable restart status;do systemctl $op chronyd;done\n```\n\n## [OpenStack packages](https://docs.openstack.org/install-guide/environment-packages.html#)\n\n```shell\nyum list centos-release-openstack*\nyum install centos-release-openstack-train -y\nyum upgrade -y\nyum install python-openstackclient openstack-selinux -y\n```\n\n## [SQL database](https://docs.openstack.org/install-guide/environment-sql-database.html)\n\n```shell\nyum install mariadb mariadb-server python2-PyMySQL -y\n\nvim /etc/my.cnf.d/openstack.conf\n[mysqld]\nbind-address = 10.10.10.101\n\ndefault-storage-engine = innodb\ninnodb_file_per_table = on\nmax_connections = 4096\ncollation-server = utf8_general_ci\ncharacter-set-server = utf8\n\nfor op in enable start status;do systemctl $op mariadb;done\n\n#数据库初始化，需要设置密码\nmysql_secure_installation\n```\n\n## [Message queue](https://docs.openstack.org/install-guide/environment-messaging.html)\n\n```shell\nyum install rabbitmq-server -y\n\nfor op in enable start status;do systemctl $op rabbitmq-server;done\n\nrabbitmqctl add_user openstack openstack\n\nrabbitmqctl set_permissions openstack \".*\" \".*\" \".*\"\n\n#安装可视化插件\nrabbitmq-plugins enable rabbitmq-management\nsystemctl restart rabbitmq-server\n\n#访问http://主控节点IP:15672/，登录账户guest/guest，将刚建的openstack用户加入到管理组\n```\n\n## [Memcached](https://docs.openstack.org/install-guide/environment-memcached.html)\n\n```shell\nyum install memcached python-memcached -y\n\nsed -i 's/127.0.0.1/10.10.10.101/' /etc/sysconfig/memcached\n\nfor op in enable start status;do systemctl $op memcached;done\n```\n\n## [Etcd](https://docs.openstack.org/install-guide/environment-etcd.html)\n\n```shell\nyum install etcd -y\n\nvim /etc/etcd/etcd.conf\n#[Member]\nETCD_DATA_DIR=\"/var/lib/etcd/default.etcd\"\nETCD_LISTEN_PEER_URLS=\"http://10.10.10.101:2380\"\nETCD_LISTEN_CLIENT_URLS=\"http://10.10.10.101:2379\"\nETCD_NAME=\"controller\"\n#[Clustering]\nETCD_INITIAL_ADVERTISE_PEER_URLS=\"http://10.10.10.101:2380\"\nETCD_ADVERTISE_CLIENT_URLS=\"http://10.10.10.101:2379\"\nETCD_INITIAL_CLUSTER=\"controller=http://10.10.10.101:2380\"\nETCD_INITIAL_CLUSTER_TOKEN=\"etcd-cluster-01\"\nETCD_INITIAL_CLUSTER_STATE=\"new\"\n\nfor op in enable start status;do systemctl $op etcd;done\n```\n\n# [Keystone installation for Train](https://docs.openstack.org/keystone/train/install/)\n\n## Prerequisites\n\n```shell\nmysql -u root -p123456\n\nCREATE DATABASE keystone;\n\nGRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'localhost' IDENTIFIED BY 'keystone';\nGRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'%' IDENTIFIED BY 'keystone';\n\nexit;\n```\n\n## Install and configure components\n\n```shell\nyum install openstack-keystone httpd mod_wsgi -y\n\ncp /etc/keystone/keystone.conf /etc/keystone/keystone.conf.bak\n\nvim /etc/keystone/keystone.conf\n[database]\nconnection = mysql+pymysql://keystone:keystone@controller/keystone\n[token]\nprovider = fernet\n\nsu -s /bin/sh -c \"keystone-manage db_sync\" keystone\n\nkeystone-manage fernet_setup --keystone-user keystone --keystone-group keystone\nkeystone-manage credential_setup --keystone-user keystone --keystone-group keystone\n\nkeystone-manage bootstrap --bootstrap-password admin(管理员密码) \\\n  --bootstrap-admin-url http://controller:5000/v3/ \\\n  --bootstrap-internal-url http://controller:5000/v3/ \\\n  --bootstrap-public-url http://controller:5000/v3/ \\\n  --bootstrap-region-id RegionOne\n```\n\n## Configure the Apache HTTP server\n\n```shell\nvim /etc/httpd/conf/httpd.conf\nServerName controller(主机名)\n\nln -s /usr/share/keystone/wsgi-keystone.conf /etc/httpd/conf.d/\n```\n\n## Finalize the installation\n\n```shell\nfor op in enable start status;do systemctl $op httpd;done\n\nvim admin-openrc\nexport OS_USERNAME=admin\nexport OS_PASSWORD=Aadmin(管理员密码)\nexport OS_PROJECT_NAME=admin\nexport OS_USER_DOMAIN_NAME=Default\nexport OS_PROJECT_DOMAIN_NAME=Default\nexport OS_AUTH_URL=http://controller:5000/v3\nexport OS_IDENTITY_API_VERSION=3\n\nsource admin-openrc\n```\n\n## Create a domain, projects, users, and roles\n\n```shell\nopenstack domain create --description \"An Example Domain\" example\n\nopenstack project create --domain default \\\n  --description \"Service Project\" service\n\nopenstack project create --domain default \\\n  --description \"Demo Project\" myproject\n\n#设置普通用户密码\nopenstack user create --domain default \\\n  --password-prompt myuser\n\nopenstack role create myrole\n\nopenstack role add --project myproject --user myuser myrole\n```\n\n## Verify operation\n\n```shell\nunset OS_AUTH_URL OS_PASSWORD\n\nopenstack --os-auth-url http://controller:5000/v3 \\\n  --os-project-domain-name Default --os-user-domain-name Default \\\n  --os-project-name admin --os-username admin token issue\n\nopenstack --os-auth-url http://controller:5000/v3 \\\n  --os-project-domain-name Default --os-user-domain-name Default \\\n  --os-project-name myproject --os-username myuser token issue\n\nvim demo-openrc\nexport OS_PROJECT_DOMAIN_NAME=Default\nexport OS_USER_DOMAIN_NAME=Default\nexport OS_PROJECT_NAME=myproject\nexport OS_USERNAME=myuser\nexport OS_PASSWORD=demo(普通用户密码)\nexport OS_AUTH_URL=http://controller:5000/v3\nexport OS_IDENTITY_API_VERSION=3\nexport OS_IMAGE_API_VERSION=2\n\n#查看普通用户token信息\nsource demo-openrc\nopenstack token issue\n\n#查看管理员token信息\nsource admin-openrc\nopenstack token issue\n```\n\n# [Glance installation for Train](https://docs.openstack.org/glance/train/install/)\n\n## Prerequisites\n\n### Create Database\n\n```shell\nmysql -u root -p123456\n\nCREATE DATABASE glance;\n\nGRANT ALL PRIVILEGES ON glance.* TO 'glance'@'localhost' \\\n  IDENTIFIED BY 'glance';\nGRANT ALL PRIVILEGES ON glance.* TO 'glance'@'%' \\\n  IDENTIFIED BY 'glance';\n\nexit;\n```\n\n### Configure User and Endpoints\n\n```shell\n. admin-openrc\n\n#设置glance用户密码\nopenstack user create --domain default --password-prompt glance\n\nopenstack role add --project service --user glance admin\n\nopenstack service create --name glance \\\n  --description \"OpenStack Image\" image\n\nopenstack endpoint create --region RegionOne \\\n  image public http://controller:9292\nopenstack endpoint create --region RegionOne \\\n  image internal http://controller:9292\nopenstack endpoint create --region RegionOne \\\n  image admin http://controller:9292\n```\n\n## Install and configure components\n\n```shell\nyum install openstack-glance -y\n\ncp /etc/glance/glance-api.conf /etc/glance/glance-api.conf.bak\nvim /etc/glance/glance-api.conf\n[database]\nconnection = mysql+pymysql://glance:glance@controller/glance\n[keystone_authtoken]\nwww_authenticate_uri  = http://controller:5000\nauth_url = http://controller:5000\nmemcached_servers = controller:11211\nauth_type = password\nproject_domain_name = Default\nuser_domain_name = Default\nproject_name = service\nusername = glance\npassword = glance(glance用户密码)\n[paste_deploy]\nflavor = keystone\n[glance_store]\nstores = file,http\ndefault_store = file\nfilesystem_store_datadir = /var/lib/glance/images/\n\nsu -s /bin/sh -c \"glance-manage db_sync\" glance\n\nfor op in enable start status;do systemctl $op openstack-glance-api;done\n```\n\n## Verify operation\n\n```shell\nwget http://download.cirros-cloud.net/0.4.0/cirros-0.4.0-x86_64-disk.img\n\nglance image-create --name \"cirros\" \\\n  --file cirros-0.4.0-x86_64-disk.img \\\n  --disk-format qcow2 --container-format bare \\\n  --visibility public\n\nglance image-create --name \"Centos7\" \\\n  --file CentOS-7-x86_64-GenericCloud-2003.qcow2c \\\n  --disk-format qcow2 --container-format bare \\\n  --visibility public\n\nopenstack images list\n```\n\n# [Placement installation for Train](https://docs.openstack.org/placement/train/install/)\n\n## Prerequisites\n\n### Create Database\n\n```shell\nmysql -u root -p123456\n\nCREATE DATABASE placement;\n\nGRANT ALL PRIVILEGES ON placement.* TO 'placement'@'localhost' \\\n  IDENTIFIED BY 'placement';\nGRANT ALL PRIVILEGES ON placement.* TO 'placement'@'%' \\\n  IDENTIFIED BY 'placement';\n\nexit\n```\n\n### Configure User and Endpoints\n\n```shell\n. admin-openrc\n\n#设置placement用户密码\nopenstack user create --domain default --password-prompt placement\n\nopenstack role add --project service --user placement admin\n\nopenstack service create --name placement \\\n  --description \"Placement API\" placement\n\nopenstack endpoint create --region RegionOne \\\n  placement public http://controller:8778\nopenstack endpoint create --region RegionOne \\\n  placement internal http://controller:8778\nopenstack endpoint create --region RegionOne \\\n  placement admin http://controller:8778\n```\n\n## Install and configure components\n\n```shell\nyum install openstack-placement-api -y\n\ncp /etc/placement/placement.conf /etc/placement/placement.conf.bak\nvim /etc/placement/placement.conf\n[placement_database]\nconnection = mysql+pymysql://placement:placement@controller/placement\n[api]\nauth_strategy = keystone\n[keystone_authtoken]\nauth_url = http://controller:5000/v3\nmemcached_servers = controller:11211\nauth_type = password\nproject_domain_name = Default\nuser_domain_name = Default\nproject_name = service\nusername = placement\npassword = placement(placement用户密码)\n\nsu -s /bin/sh -c \"placement-manage db sync\" placement\n\nsystemctl restart httpd & systemctl status httpd\n```\n\n## Verify operation\n\n```shell\n. admin-openrc\n\nplacement-status upgrade check\n\nyum install epel-release -y\nyum install -y python-pip\npip install --upgrade pip\npip install osc-placement\n\nopenstack --os-placement-api-version 1.2 resource class list --sort-column name\nopenstack --os-placement-api-version 1.6 trait list --sort-column name\n```\n\n# [Nova installation for Train](https://docs.openstack.org/nova/train/install/)\n\n## controller\n\n### Prerequisites\n\n#### Create Database\n\n```shell\nmysql -u root -p123456\n\nCREATE DATABASE nova_api;\nCREATE DATABASE nova;\nCREATE DATABASE nova_cell0;\n\nGRANT ALL PRIVILEGES ON nova_api.* TO 'nova'@'localhost' \\\n  IDENTIFIED BY 'nova';\nGRANT ALL PRIVILEGES ON nova_api.* TO 'nova'@'%' \\\n  IDENTIFIED BY 'nova';\n\nGRANT ALL PRIVILEGES ON nova.* TO 'nova'@'localhost' \\\n  IDENTIFIED BY 'nova';\nGRANT ALL PRIVILEGES ON nova.* TO 'nova'@'%' \\\n  IDENTIFIED BY 'nova';\n\nGRANT ALL PRIVILEGES ON nova_cell0.* TO 'nova'@'localhost' \\\n  IDENTIFIED BY 'nova';\nGRANT ALL PRIVILEGES ON nova_cell0.* TO 'nova'@'%' \\\n  IDENTIFIED BY 'nova';\n\nexit;\n```\n\n#### Configure User and Endpoints\n\n```shell\n. admin-openrc\n\n#设置nova用户密码\nopenstack user create --domain default --password-prompt nova\n\nopenstack role add --project service --user nova admin\n\nopenstack service create --name nova \\\n  --description \"OpenStack Compute\" compute\n\nopenstack endpoint create --region RegionOne \\\n  compute public http://controller:8774/v2.1\nopenstack endpoint create --region RegionOne \\\n  compute internal http://controller:8774/v2.1\nopenstack endpoint create --region RegionOne \\\n  compute admin http://controller:8774/v2.1\n```\n\n### Install and configure components\n\n```shell\nyum install openstack-nova-api openstack-nova-conductor \\\n  openstack-nova-novncproxy openstack-nova-scheduler -y\ncp /etc/nova/nova.conf /etc/nova/nova.conf.bak\nvim /etc/nova/nova.conf\n[DEFAULT]\nenabled_apis = osapi_compute,metadata\ntransport_url = rabbit://openstack:openstack@controller:5672/\nmy_ip = 10.10.10.101(主控节点IP)\nuse_neutron = true\nfirewall_driver = nova.virt.firewall.NoopFirewallDriver\n\n[api_database]\nconnection = mysql+pymysql://nova:nova@controller/nova_api\n\n[database]\nconnection = mysql+pymysql://nova:nova@controller/nova\n\n[api]\nauth_strategy = keystone\n\n[keystone_authtoken]\nwww_authenticate_uri = http://controller:5000/\nauth_url = http://controller:5000/\nmemcached_servers = controller:11211\nauth_type = password\nproject_domain_name = Default\nuser_domain_name = Default\nproject_name = service\nusername = nova\npassword = nova(nova用户密码)\n\n[vnc]\nenabled = true\nserver_listen = $my_ip\nserver_proxyclient_address = $my_ip\n\n[glance]\napi_servers = http://controller:9292\n\n[oslo_concurrency]\nlock_path = /var/lib/nova/tmp\n\n[placement]\nregion_name = RegionOne\nproject_domain_name = Default\nproject_name = service\nauth_type = password\nuser_domain_name = Default\nauth_url = http://controller:5000/v3\nusername = placement\npassword = placement(placement用户密码)\n\n[scheduler]\ndiscover_hosts_in_cells_interval = 300\n\nvim /etc/httpd/conf.d/00-nova-placement-api.conf\n<Directory /usr/bin>\n   <IfVersion >= 2.4>\n      Require all granted\n   </IfVersion>\n   <IfVersion < 2.4>\n      Order allow,deny\n      Allow from all\n   </IfVersion>\n</Directory>\n\nsu -s /bin/sh -c \"nova-manage api_db sync\" nova\nsu -s /bin/sh -c \"nova-manage cell_v2 map_cell0\" nova\nsu -s /bin/sh -c \"nova-manage cell_v2 create_cell --name=cell1 --verbose\" nova\nsu -s /bin/sh -c \"nova-manage db sync\" nova\nsu -s /bin/sh -c \"nova-manage cell_v2 list_cells\" nova\n\nfor op in enable satrt status;do for service in openstack-nova-api openstack-nova-scheduler openstack-nova-conductor openstack-nova-novncproxy;do systemctl $op $service;done;done\n```\n\n## compute\n\n### Install and configure components\n\n```shell\nyum install openstack-nova-compute libvirt -y\n\ncp /etc/nova/nova.conf /etc/nova/nova.conf.bak\nvim /etc/nova/nova.conf\n[DEFAULT]\nenabled_apis = osapi_compute,metadata\ntransport_url = rabbit://openstack:openstack@controller\nmy_ip = 10.10.10.102(计算节点IP)\nuse_neutron = true\nfirewall_driver = nova.virt.firewall.NoopFirewallDriver\n\n[api]\nauth_strategy = keystone\n\n[keystone_authtoken]\nwww_authenticate_uri = http://controller:5000/\nauth_url = http://controller:5000/\nmemcached_servers = controller:11211\nauth_type = password\nproject_domain_name = Default\nuser_domain_name = Default\nproject_name = service\nusername = nova\npassword = nova(nova用户密码)\n\n[vnc]\nenabled = true\nserver_listen = 0.0.0.0\nserver_proxyclient_address = $my_ip\nnovncproxy_base_url = http://controller:6080/vnc_auto.html\n\n[glance]\napi_servers = http://controller:9292\n\n[oslo_concurrency]\nlock_path = /var/lib/nova/tmp\n\n[placement]\nregion_name = RegionOne\nproject_domain_name = Default\nproject_name = service\nauth_type = password\nuser_domain_name = Default\nauth_url = http://controller:5000/v3\nusername = placement\npassword = placement(placement用户密码)\n\n[libvirt]\ncpu_mode = none\nvirt_type=qemu\n\n\nfor op in enable satrt status;do for service in libvirtd openstack-nova-compute;do systemctl $op $service;done;done\n```\n\n## controller\n\n### Verify operation\n\n```shell\n. admin-openrc\nopenstack compute service list\nsu -s /bin/sh -c \"nova-manage cell_v2 discover_hosts --verbose\" nova\n```\n\n# [Neutron installation for Train](https://docs.openstack.org/neutron/train/install/)\n\n## controller\n\n### Prerequisites\n\n#### Create Database\n\n```shell\nmysql -u root -p123456\n\nCREATE DATABASE neutron;\n\nGRANT ALL PRIVILEGES ON neutron.* TO 'neutron'@'localhost' \\\n  IDENTIFIED BY 'neutron';\nGRANT ALL PRIVILEGES ON neutron.* TO 'neutron'@'%' \\\n  IDENTIFIED BY 'neutron';\n\nexit;\n```\n\n#### Configure User and Endpoints\n\n```shell\n. admin-openrc\n\n#设置neutron用户密码\nopenstack user create --domain default --password-prompt neutron\n\nopenstack role add --project service --user neutron admin\n\nopenstack service create --name neutron \\\n  --description \"OpenStack Networking\" network\n\nopenstack endpoint create --region RegionOne \\\n  network public http://controller:9696\nopenstack endpoint create --region RegionOne \\\n  network internal http://controller:9696\nopenstack endpoint create --region RegionOne \\\n  network admin http://controller:9696\n```\n\n### Install and configure components\n\n```shell\nyum install openstack-neutron openstack-neutron-ml2 \\\n  openstack-neutron-linuxbridge ebtables -y\n```\n\n#### [Provider networks](https://docs.openstack.org/neutron/train/install/controller-install-option1-rdo.html)\n\n##### vim /etc/neutron/neutron.conf\n\n```properties\n[database]\nconnection = mysql+pymysql://neutron:neutron@controller/neutron\n\n[DEFAULT]\ncore_plugin = ml2\nservice_plugins =\ntransport_url = rabbit://openstack:openstack@controller\nauth_strategy = keystone\nnotify_nova_on_port_status_changes = true\nnotify_nova_on_port_data_changes = true\n\n[keystone_authtoken]\nwww_authenticate_uri = http://controller:5000\nauth_url = http://controller:5000\nmemcached_servers = controller:11211\nauth_type = password\nproject_domain_name = default\nuser_domain_name = default\nproject_name = service\nusername = neutron\npassword = neutron(neutron用户密码)\n\n[nova]\nauth_url = http://controller:5000\nauth_type = password\nproject_domain_name = default\nuser_domain_name = default\nregion_name = RegionOne\nproject_name = service\nusername = nova\npassword = nova(nova用户密码)\n\n[oslo_concurrency]\nlock_path = /var/lib/neutron/tmp\n```\n\n##### vim /etc/neutron/plugins/ml2/ml2_conf.ini\n\n```properties\n[ml2]\ntype_drivers = flat,vlan\ntenant_network_types =\nmechanism_drivers = linuxbridge\nextension_drivers = port_security\n\n[ml2_type_flat]\nflat_networks = provider\n\n[securitygroup]\nenable_ipset = true\n```\n\n##### vim /etc/neutron/plugins/ml2/linuxbridge_agent.ini\n\n```properties\n[linux_bridge]\nphysical_interface_mappings = provider:ens33(服务器网络接口名)\n\n[vxlan]\nenable_vxlan = false\n\n[securitygroup]\nenable_security_group = true\nfirewall_driver = neutron.agent.linux.iptables_firewall.IptablesFirewallDriver\n```\n\n##### edit kernel supports\n\n```shell\nmodprobe br_netfilter\n\nvim /etc/sysctl.conf\nnet.bridge.bridge-nf-call-iptables = 1\nnet.bridge.bridge-nf-call-ip6tables = 1\n\nsysctl -p\n```\n\n##### vim /etc/neutron/dhcp_agent.ini\n\n```properties\n[DEFAULT]\ninterface_driver = linuxbridge\ndhcp_driver = neutron.agent.linux.dhcp.Dnsmasq\nenable_isolated_metadata = true\n```\n\n##### vim /etc/neutron/metadata_agent.ini\n\n```properties\n[DEFAULT]\nnova_metadata_host = controller\nmetadata_proxy_shared_secret = metadata(网络共享密码)\n```\n\n##### vim /etc/nova/nova.conf\n\n```properties\n[neutron]\nauth_url = http://controller:5000\nauth_type = password\nproject_domain_name = default\nuser_domain_name = default\nregion_name = RegionOne\nproject_name = service\nusername = neutron\npassword = neutron(neutron用户密码)\nservice_metadata_proxy = true\nmetadata_proxy_shared_secret = metadata(网络共享密码)\n```\n\n### Finalize installation\n\n```shell\nln -s /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugin.ini\n\nsu -s /bin/sh -c \"neutron-db-manage --config-file /etc/neutron/neutron.conf \\\n  --config-file /etc/neutron/plugins/ml2/ml2_conf.ini upgrade head\" neutron\n\nsystemctl restart openstack-nova-api.service\n\nfor op in enable start status;do for service in neutron-server neutron-linuxbridge-agent neutron-dhcp-agent neutron-metadata-agent;do systemctl $op $servicel;done;done\n\nfor op in enable start status;do systemctl $op neutron-l3-agent;done\n```\n\n## compute\n\n### Install and configure components\n\n```shell\nyum install openstack-neutron-linuxbridge ebtables ipset -y\n```\n\n#### vim /etc/neutron/neutron.conf\n\n```properties\n[DEFAULT]\ntransport_url = rabbit://openstack:openstack@controller\nauth_strategy = keystone\n\n[keystone_authtoken]\nwww_authenticate_uri = http://controller:5000\nauth_url = http://controller:5000\nmemcached_servers = controller:11211\nauth_type = password\nproject_domain_name = default\nuser_domain_name = default\nproject_name = service\nusername = neutron\npassword = neutron(neutron用户密码)\n\n[oslo_concurrency]\nlock_path = /var/lib/neutron/tmp\n```\n\n#### [Provider networks](https://docs.openstack.org/neutron/train/install/compute-install-option1-rdo.html)\n\n##### vim /etc/neutron/plugins/ml2/linuxbridge_agent.ini\n\n```properties\n[linux_bridge]\nphysical_interface_mappings = provider:ens33(服务器网络接口名)\n\n[vxlan]\nenable_vxlan = false\n\n[securitygroup]\nenable_security_group = true\nfirewall_driver = neutron.agent.linux.iptables_firewall.IptablesFirewallDriver\n```\n\n##### edit kernel supports\n\n```shell\nmodprobe br_netfilter\n\nvim /etc/sysctl.conf\nnet.bridge.bridge-nf-call-iptables = 1\nnet.bridge.bridge-nf-call-ip6tables = 1\n\nsysctl -p\n```\n\n#### vim /etc/nova/nova.conf\n\n```properties\n[neutron]\nauth_url = http://controller:5000\nauth_type = password\nproject_domain_name = default\nuser_domain_name = default\nregion_name = RegionOne\nproject_name = service\nusername = neutron\npassword = neutron(neutron用户密码)\n```\n\n### Finalize installation\n\n```shell\nsystemctl restart openstack-nova-compute\n\nfor op in enable start status;do systemctl $op neutron-linuxbridge-agent;done\n```\n\n## Verify operation\n\n```shell\n. admin-openrc\n```\n\n### [Provider networks](https://docs.openstack.org/neutron/train/install/verify-option1.html)\n\n```shell\nopenstack extension list --network\n```\n\n### [Self-service networks](https://docs.openstack.org/neutron/train/install/verify-option2.html)\n\n```shell\nopenstack network agent list\n```\n\n# [Cinder installation for Train](https://docs.openstack.org/cinder/train/install/)\n\n## controller\n\n### Prerequisites\n\n#### Create Database\n\n```shell\nmysql -u root -p123456\n\nCREATE DATABASE cinder;\n\nGRANT ALL PRIVILEGES ON cinder.* TO 'cinder'@'localhost' \\\n  IDENTIFIED BY 'cinder';\nGRANT ALL PRIVILEGES ON cinder.* TO 'cinder'@'%' \\\n  IDENTIFIED BY 'cinder';\n\nexit;\n```\n\n#### Configure User and Endpoints\n\n```shell\n. admin-openrc\n\n#设置cinder用户密码\nopenstack user create --domain default --password-prompt cinder\n\nopenstack role add --project service --user cinder admin\n\nopenstack service create --name cinderv2 \\\n  --description \"OpenStack Block Storage\" volumev2\nopenstack service create --name cinderv3 \\\n  --description \"OpenStack Block Storage\" volumev3\n\nopenstack endpoint create --region RegionOne \\\n  volumev2 public http://controller:8776/v2/%\\(project_id\\)s\nopenstack endpoint create --region RegionOne \\\n  volumev2 internal http://controller:8776/v2/%\\(project_id\\)s\nopenstack endpoint create --region RegionOne \\\n  volumev2 admin http://controller:8776/v2/%\\(project_id\\)s\n\nopenstack endpoint create --region RegionOne \\\n  volumev3 public http://controller:8776/v3/%\\(project_id\\)s\nopenstack endpoint create --region RegionOne \\\n  volumev3 internal http://controller:8776/v3/%\\(project_id\\)s\nopenstack endpoint create --region RegionOne \\\n  volumev3 admin http://controller:8776/v3/%\\(project_id\\)s\n```\n\n### Install and configure components\n\n```shell\nyum install openstack-cinder -y\n\nvim /etc/cinder/cinder.conf\n[DEFAULT]\ntransport_url = rabbit://openstack:openstack@controller\nauth_strategy = keystone\nmy_ip = 10.10.10.101\n[database]\nconnection = mysql+pymysql://cinder:cinder@controller/cinder\n[keystone_authtoken]\nwww_authenticate_uri = http://controller:5000\nauth_url = http://controller:5000\nmemcached_servers = controller:11211\nauth_type = password\nproject_domain_name = default\nuser_domain_name = default\nproject_name = service\nusername = cinder\npassword = cinder(cinder用户密码)\n[oslo_concurrency]\nlock_path = /var/lib/cinder/tmp\n\nsu -s /bin/sh -c \"cinder-manage db sync\" cinder\n```\n\n### Configure Compute to use Block Storage[\n\n```shell\nvim /etc/nova/nova.conf\n[cinder]\nos_region_name = RegionOne\n```\n\n### Finalize installation\n\n```shell\nfor op in restart status;do systemctl $op openstack-nova-api;done\n\nfor op in enable restart status;do for service in openstack-cinder-api openstack-cinder-scheduler;do systemctl $op $service;done;done\n```\n\n## block\n\n### Prerequisites\n\n```shell\nyum install lvm2 device-mapper-persistent-data\n\nfor op in enable start status;do systemctl $op lvm2-lvmetad;done\n\npvcreate /dev/sdb\n\nvgcreate cinder-volumes /dev/sdb\n\nvim /etc/lvm/lvm.conf\ndevices {\n...\nfilter = [ \"a/sdb/\", \"r/.*/\"]\n```\n\n### Install and configure components\n\n```shell\nyum install openstack-cinder targetcli python-keystone -y\n\nvim /etc/cinder/cinder.conf\n[DEFAULT]\nauth_strategy = keystone\ntransport_url = rabbit://openstack:openstack@controller\nmy_ip = 10.10.10.103\nenabled_backends = lvm\nglance_api_servers = http://controller:9292\n[database]\nconnection = mysql+pymysql://cinder:cinder@controller/cinder\n[keystone_authtoken]\nwww_authenticate_uri = http://controller:5000\nauth_url = http://controller:5000\nmemcached_servers = controller:11211\nauth_type = password\nproject_domain_name = default\nuser_domain_name = default\nproject_name = service\nusername = cinder\npassword = cinder\n[lvm]\nvolume_driver = cinder.volume.drivers.lvm.LVMVolumeDriver\nvolume_group = cinder-volumes\ntarget_protocol = iscsi\ntarget_helper = lioadm\n[oslo_concurrency]\nlock_path = /var/lib/cinder/tmp\n```\n\n### Finalize installation\n\n```shell\nfor op in enable restart status;do for service in openstack-cinder-volume target;do systemctl $op $service;done;done\n```\n\n## Verify Cinder operation\n\n```shell\n. admin-openrc\nopenstack volume service list\n```\n\n# [Horizon installation for Train](https://docs.openstack.org/horizon/train/install/)\n\n## Install and configure components\n\n```shell\nyum install openstack-dashboard -y\n\nvim /etc/openstack-dashboard/local_settings\n\nOPENSTACK_HOST = \"controller\"\nALLOWED_HOSTS = ['*']\nSESSION_ENGINE = 'django.contrib.sessions.backends.cache'\nCACHES = {\n    'default': {\n         'BACKEND': 'django.core.cache.backends.memcached.MemcachedCache',\n         'LOCATION': 'controller:11211',\n    }\n}\nOPENSTACK_KEYSTONE_URL = \"http://%s:5000/v3\" % OPENSTACK_HOST\nOPENSTACK_KEYSTONE_MULTIDOMAIN_SUPPORT = True\nOPENSTACK_API_VERSIONS = {\n    \"identity\": 3,\n    \"image\": 2,\n    \"volume\": 3,\n}\nOPENSTACK_KEYSTONE_DEFAULT_DOMAIN = \"Default\"\nOPENSTACK_KEYSTONE_DEFAULT_ROLE = \"user\"\nOPENSTACK_NEUTRON_NETWORK = {\n    ...\n    'enable_router': False,\n    'enable_quotas': False,\n    'enable_distributed_router': False,\n    'enable_ha_router': False,\n    'enable_lb': False,\n    'enable_firewall': False,\n    'enable_vpn': False,\n    'enable_fip_topology_check': False,\n}\nTIME_ZONE = \"Asia/Shanghai\"\nWEBROOT = '/dashboard/'\n\nvim  /etc/httpd/conf.d/openstack-dashboard.conf\nWSGIApplicationGroup %{GLOBAL}\n```\n\n## Finalize installation\n\n```shell\nsystemctl restart httpd.service memcached.service\n#访问http://主控节点IP/dashboard/,域:default\n```\n\n# Launch an instance（可以通过 dashboard 实现）\n\n## Create virtual networks\n\n### Provider network\n\n```shell\n. admin-openrc\n\nopenstack network create  --share --external \\\n  --provider-physical-network provider \\\n  --provider-network-type flat provider\n\nopenstack subnet create --network provider \\\n  --allocation-pool start=192.168.40.101,end=192.168.40.250 \\\n  --dns-nameserver 8.8.8.8 --gateway 192.168.40.1 \\\n  --subnet-range 192.168.40.0/24 provider\n```\n\n### Self-service network\n\n```shell\n. demo-openrc\n\nopenstack network create selfservice\n\nopenstack subnet create --network selfservice \\\n  --dns-nameserver 8.8.4.4 --gateway 172.16.1.1 \\\n  --subnet-range 172.16.1.0/24 selfservice\n\nopenstack router create router\nopenstack router add subnet router selfservice\nopenstack router set router --external-gateway provider\n\n. admin-openrc\nip netns\nopenstack port list --router router\n```\n\n## Create m1.nano flavor\n\n```shell\nfor op in enable satrt status;do for service in libvirtd openstack-nova-compute;do systemctl $op $service;done;done\n\nopenstack flavor create --id 0 --vcpus 1 --ram 64 --disk 1 m1.nano\nopenstack flavor create --id 0 --vcpus 1 --ram 1024 --disk 10 m2.nano\n```\n\n## Generate a key pair\n\n```\n. demo-openrc\nssh-keygen -q -N \"\"\nopenstack keypair create --public-key ~/.ssh/id_rsa.pub mykey\nopenstack keypair list\n```\n\n## Add security group rules\n\n```shell\nopenstack security group rule create --proto icmp default\nopenstack security group rule create --proto tcp --dst-port 22 default\n```\n\n## Launch an instance\n\n### Provider network\n\n```shell\n. demo-openrc\nopenstack flavor list\nopenstack image list\nopenstack network list\nopenstack security group list\n\nopenstack server create --flavor m1.nano --image cirros \\\n  --nic net-id=PROVIDER_NET_ID --security-group default \\\n  --key-name mykey provider-instance\nopenstack server list\nopenstack console url show provider-instance\n```\n\n### Self-service network\n\n```shell\nopenstack server create --flavor m1.nano --image cirros \\\n  --nic net-id=b364d40b-0e57-4b32-9949-db83666f0f57 --security-group default \\\n  --key-name mykey newserver\n```\n\n## 修改实例默认密码(controller)\n\n```shell\nvim /etc/nova/nova.conf\ninject_password=True\n\nvim /etc/openstack-dashboard/local_settings\nOPENSTACK_HYPERVISOR_FEATURES = {\n'can_set_password': True\n}\n\nsystemctl restart openstack-nova-*\n\n#创建实例时写入以下脚本，并勾选配置驱动\n#!/bin/bash\npasswd root<<EOF\n123456\n123456\nEOF\n```\n","tags":["openstack","虚拟化"],"categories":["devops"]},{"title":"通过二进制方式搭建Kubernetes集群","url":"/posts/d297681f.html","content":"\n<!-- block -->\n>&emsp;&emsp;学习搭建使用二进制文件搭建 `Kubernetes` 集群\n<!-- block -->\n\n# Refence\n\nhttps://www.cnblogs.com/lizexiong/p/14882419.html#blogTitle32\n\n# Enviroment\n\n|          Role           |      IP      |                                                Component                                                | Hostname |\n| :---------------------: | :----------: | :-----------------------------------------------------------------------------------------------------: | :------: |\n| master(Kubernetes,Etcd) | 10.10.10.101 | cfssl<br/>etcd<br/>docker<br/>kube-apiserver<br/>kube-controller-manager<br/>kube-scheduler<br/>kubectl |  master  |\n| node1(Kubernetes,Etcd)  | 10.10.10.102 |                               etcd<br/>docker<br/>kubelet<br/>kube-proxy                                |  minion  |\n| node2(Kubernetes,Etcd)  | 10.10.10.103 |                               etcd<br/>docker<br/>kubelet<br/>kube-proxy                                |  slave   |\n\n# Prepare\n\n```shell\n#关闭防火墙\nsystemctl disable firewalld --now\n\n#临时关闭selinux\nsetenforce 0\n#禁用,重启生效\nsed -i \"s/enforcing/disabled/g\" /etc/selinux/config\n\n#临时关闭swap\nswapoff -a\n#永久关闭\nsed -i 's/^[^#].*swap*/#&/g' /etc/fstab\n\n#安装常用软件包\nyum install vim net-tools lrzsz unzip dos2unix telnet sysstat iotop pciutils lsof tcpdump psmisc bc wget socat gcc tree chrony ntpdate mlocate zip unzip -y\n\n#设置主机名\nhostnamectl set-hostname <hostname>\n\n#主机名写入hosts\ncat >> /etc/hosts << EOF\n10.10.10.101 master\n10.10.10.102 minion\n10.10.10.103 slave\nEOF\n\n#主机间配置ssh免密登录\nssh-keygen -t rsa\nfor host in master minion slave; do ssh-copy-id -i ~/.ssh/id_rsa.pub $host;done\n\n#内核开启网络支持\ncat > /etc/sysctl.d/kubernetes.conf << EOF\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-iptables = 1\nnet.ipv4.ip_forward = 1\nnet.ipv4.ip_nonlocal_bind = 1\nEOF\nmodprobe br_netfilter\nsysctl -p /etc/sysctl.d/kubernetes.conf\n```\n\n# Cfssl\n\n```shell\nwget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64\nwget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64\nwget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64\nchmod +x cfssl*\n\nmv cfssl_linux-amd64 /usr/bin/cfssl\nmv cfssljson_linux-amd64 /usr/bin/cfssljson\nmv cfssl-certinfo_linux-amd64 /usr/bin/cfssl-certinfo\n```\n\n# Etcd\n\n## 自签 CA\n\n```shell\nmkdir cfssl/etcd -p && cd cfssl/etcd\n\ncat > ca-config.json << EOF\n{\n  \"signing\": {\n    \"default\": {\n      \"expiry\": \"87600h\"\n    },\n    \"profiles\": {\n      \"www\": {\n         \"expiry\": \"87600h\",\n         \"usages\": [\n            \"signing\",\n            \"key encipherment\",\n            \"server auth\",\n            \"client auth\"\n        ]\n      }\n    }\n  }\n}\nEOF\n\ncat > ca-csr.json << EOF\n{\n    \"CN\": \"etcd CA\",\n    \"key\": {\n        \"algo\": \"rsa\",\n        \"size\": 2048\n    },\n    \"names\": [\n        {\n            \"C\": \"CN\",\n            \"L\": \"ShangHai\",\n            \"ST\": \"ShangHai\"\n        }\n    ]\n}\nEOF\n\n#生成ca.pem和ca-key.pem文件\ncfssl gencert -initca ca-csr.json | cfssljson -bare ca -\n```\n\n## 使用自签 CA 签发 HTTPS 证书\n\n```shell\n#hosts内为节点IP\ncat > server-csr.json << EOF\n{\n    \"CN\": \"etcd\",\n    \"hosts\": [\n    \"10.10.10.101\",\n    \"10.10.10.102\",\n    \"10.10.10.103\"\n    ],\n    \"key\": {\n        \"algo\": \"rsa\",\n        \"size\": 2048\n    },\n    \"names\": [\n        {\n            \"C\": \"CN\",\n            \"L\": \"ShangHai\",\n            \"ST\": \"ShangHai\"\n        }\n    ]\n}\nEOF\n\n#生成server.pem和server-key.pem文件\ncfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=www server-csr.json | cfssljson -bare server\n```\n\n## 手动安装\n\n```shell\n#可以在https://github.com/etcd-io/etcd/releases寻找下载\nmkdir /etc/etcd/{bin,ssl} -p\ntar -mxvf etcd-v3.5.0-linux-amd64.tar\nmv etcd-v3.5.0-linux-amd64.tar/etcd* /etc/etcd/bin/\n```\n\n## Yum 安装\n\n```shell\nyum install etcd -y\nmkdir /etc/etcd/ssl -p\n```\n\n## 修改配置文件\n\n```shell\n#注意检查各个节点的etcd.conf中主机名和IP是否和当前节点一致(没有则新建)\n#[Member]\nETCD_NAME=\"etcd-master\"\nETCD_DATA_DIR=\"/var/lib/etcd/default.etcd\"\nETCD_LISTEN_PEER_URLS=\"https://10.10.10.101:2380\"\nETCD_LISTEN_CLIENT_URLS=\"https://10.10.10.101:2379\"\n\n#[Clustering]\nETCD_INITIAL_ADVERTISE_PEER_URLS=\"https://10.10.10.101:2380\"\nETCD_ADVERTISE_CLIENT_URLS=\"https://10.10.10.101:2379\"\nETCD_INITIAL_CLUSTER=\"etcd-master=https://10.10.10.101:2380,etcd-minion=https://10.10.10.102:2380,etcd-slave=https://10.10.10.103:2380\"\nETCD_INITIAL_CLUSTER_TOKEN=\"etcd-cluster\"\nETCD_INITIAL_CLUSTER_STATE=\"new\"\n```\n\n## 拷贝证书\n\n```shell\nfor host in master minion slave; do scp cfssl/etcd/*.pem $host:/etc/etcd/ssl/;done\n```\n\n## 修改系统服务文件\n\n```shell\n#/lib/systemd/system/etcd.service(没有则新建)\n[Unit]\nDescription=Etcd Server\nAfter=network.target\nAfter=network-online.target\nWants=network-online.target\n\n[Service]\nType=notify\nEnvironmentFile=/etc/etcd/etcd.conf\nExecStart=/etc/etcd/etcd \\\n--cert-file=/etc/etcd/ssl/server.pem \\\n--key-file=/etc/etcd/ssl/server-key.pem \\\n--peer-cert-file=/etc/etcd/ssl/server.pem \\\n--peer-key-file=/etc/etcd/ssl/server-key.pem \\\n--trusted-ca-file=/etc/etcd/ssl/ca.pem \\\n--peer-trusted-ca-file=/etc/etcd/ssl/ca.pem \\\n--logger=zap\nRestart=on-failure\nLimitNOFILE=65536\n\n[Install]\nWantedBy=multi-user.target\n```\n\n## 自启启动\n\n```shell\nfor host in master minion slave;do ssh $host 'systemctl daemon-reload;systemctl enable etcd --now';done\n```\n\n## 查看集群状态\n\n```shell\nETCDCTL_API=3 /etc/etcd/etcdctl --cacert=/etc/etcd/ssl/ca.pem --cert=/etc/etcd/ssl/server.pem --key=/etc/etcd/ssl/server-key.pem --endpoints=\"https://10.10.10.101:2379,https://10.10.10.102:2379,https://10.10.10.103:2379\" endpoint health --write-out=table\n\n+-----------------------------+--------+-------------+-------+\n|          ENDPOINT           | HEALTH |    TOOK     | ERROR |\n+-----------------------------+--------+-------------+-------+\n| https://10.10.10.101:2379   |  true  | 50.191672ms |       |\n| https://10.10.10.102:2379   |  true  | 52.394036ms |       |\n| https://10.10.10.103:2379   |  true  | 46.009422ms |       |\n+-----------------------------+--------+-------------+-------+\n```\n\n# Docker\n\n## 手动安装\n\n```shell\n#可以在https://download.docker.com/linux/static/stable/x86_64/载\ntar -mxvf docker-20.10.7.tgz\nmv docker/* /usr/bin\n\n#创建系统服务文件\ncat > /usr/lib/systemd/system/docker.service << EOF\n[Unit]\nDescription=Docker Application Container Engine\nDocumentation=https://docs.docker.com\nAfter=network-online.target firewalld.service\nWants=network-online.target\n\n[Service]\nType=notify\nExecStart=/usr/bin/dockerd\nExecReload=/bin/kill -s HUP $MAINPID\nLimitNOFILE=infinity\nLimitNPROC=infinity\nLimitCORE=infinity\nTimeoutStartSec=0\nDelegate=yes\nKillMode=process\nRestart=on-failure\nStartLimitBurst=3\nStartLimitInterval=60s\n\n[Install]\nWantedBy=multi-user.target\nEOF\n```\n\n## Yum 安装\n\n```shell\n#官方脚本安装\ncurl -fsSL https://get.docker.com | bash -s docker --mirror Aliyun\n\n#手动yum安装\n#卸载原有docker\nyum remove docker*\n#安装所需的软件包。yum-utils 提供了 yum-config-manager ，并且 device mapper 存储驱动程序需要 device-mapper-persistent-data 和 lvm2\nyum install -y yum-utils device-mapper-persistent-data lvm2\n#设置阿里源地址\nyum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo\n#安装\nyum install docker-ce docker-ce-cli containerd.io -y\n```\n\n## 修改配置\n\n```shell\n#配置镜像加速地址,并修改Cgroup Driver(docker默认cgroupfs，k8s推荐使用systemd)\nmkdir /etc/docker -p\ncat > /etc/docker/daemon.json << EOF\n{\"registry-mirrors\":[\"https://docker.mirrors.ustc.edu.cn/\"],\n  \"exec-etcs\": [\"native.cgroupdriver=systemd\"]}\nEOF\n```\n\n## 自启启动\n\n```shell\nfor host in master minion slave;do ssh $host 'systemctl daemon-reload;systemctl enable docker --now';done\n```\n\n# Master\n\n## 二进制文件\n\n```shell\n#可以在https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG.md寻找下载\nmkdir -p /etc/kubernetes/{bin,conf,ssl,logs}\ntar -mxvf kubernetes-server-linux-amd64.tar.gz\ncd kubernetes/server/bin/\ncp kube-apiserver kube-scheduler kube-controller-manager /etc/kubernetes/bin/\ncp kubectl /usr/bin/\n```\n\n## 自签 CA\n\n```shell\nmkdir cfssl/kubernetes -p && cd cfssl/kubernetes\n\ncat > ca-config.json << EOF\n{\n  \"signing\": {\n    \"default\": {\n      \"expiry\": \"87600h\"\n    },\n    \"profiles\": {\n      \"kubernetes\": {\n         \"expiry\": \"87600h\",\n         \"usages\": [\n            \"signing\",\n            \"key encipherment\",\n            \"server auth\",\n            \"client auth\"\n        ]\n      }\n    }\n  }\n}\nEOF\n\ncat > ca-csr.json << EOF\n{\n    \"CN\": \"kubernetes\",\n    \"key\": {\n        \"algo\": \"rsa\",\n        \"size\": 2048\n    },\n    \"names\": [\n        {\n            \"C\": \"CN\",\n            \"L\": \"ShangHai\",\n            \"ST\": \"ShangHai\",\n            \"O\": \"Kubernetes\",\n            \"OU\": \"System\"\n        }\n    ]\n}\nEOF\n\n#生成ca.pem和ca-key.pem文件\ncfssl gencert -initca ca-csr.json | cfssljson -bare ca -\n```\n\n## kube-apiserver\n\n### 使用自签 CA 签 HTTPS 证书\n\n```shell\n#hosts内为节点IP,为了方便后期扩容可以多写几个预留的IP\ncat > server-csr.json << EOF\n{\n    \"CN\": \"kubernetes\",\n    \"hosts\": [\n      \"10.0.0.1\",\n      \"127.0.0.1\",\n      \"10.10.10.101\",\n      \"10.10.10.102\",\n      \"10.10.10.103\",\n      \"kubernetes\",\n      \"kubernetes.default\",\n      \"kubernetes.default.svc\",\n      \"kubernetes.default.svc.cluster\",\n      \"kubernetes.default.svc.cluster.local\"\n    ],\n    \"key\": {\n        \"algo\": \"rsa\",\n        \"size\": 2048\n    },\n    \"names\": [\n        {\n            \"C\": \"CN\",\n            \"L\": \"ShangHai\",\n            \"ST\": \"ShangHai\",\n            \"O\": \"Kubernetes\",\n            \"OU\": \"System\"\n        }\n    ]\n}\nEOF\n\n#生成server.pem和server-key.pem文件\ncfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes server-csr.json | cfssljson -bare server\n\n#拷贝证书\ncp cfssl/kubernetes/*.pem /etc/kubernetes/ssl\n#如果etcd和master不在一台机器部署，这里etcd的证书也要拷贝\ncp cfssl/etcd/*.pem /etc/etcd/ssl\n```\n\n### 创建配置文件\n\n```shell\n#创建token\ncat > /opt/kubernetes/conf/token.csv << EOF\nc47ffb939f5ca36231d9e3121a252940,kubelet-bootstrap,10001,\"system:node-bootstrapper\"\nEOF\n\n#token也可自行生成替换\nhead -c 16 /dev/urandom | od -An -t x | tr -d ' '\n\ncat > /opt/kubernetes/conf/kube-apiserver.conf << EOF\nKUBE_APISERVER_OPTS=\"--logtostderr=false \\\\\n--v=2 \\\\\n--log-dir=/opt/kubernetes/logs \\\\\n--etcd-servers=https://10.10.10.101:2379,https://10.10.10.102:2379,https://10.10.10.103:2379 \\\\\n--bind-address=10.10.10.101 \\\\\n--secure-port=6443 \\\\\n--advertise-address=10.10.10.101 \\\\\n--allow-privileged=true \\\\\n--service-cluster-ip-range=10.0.0.0/24 \\\\\n--enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,ResourceQuota,NodeRestriction \\\\\n--authorization-mode=RBAC,Node \\\\\n--enable-bootstrap-token-auth=true \\\\\n--token-auth-file=/opt/kubernetes/conf/token.csv \\\\\n--service-node-port-range=1-65535 \\\\\n--kubelet-client-certificate=/opt/kubernetes/ssl/server.pem \\\\\n--kubelet-client-key=/opt/kubernetes/ssl/server-key.pem \\\\\n--tls-cert-file=/opt/kubernetes/ssl/server.pem  \\\\\n--tls-private-key-file=/opt/kubernetes/ssl/server-key.pem \\\\\n--client-ca-file=/opt/kubernetes/ssl/ca.pem \\\\\n--service-account-key-file=/opt/kubernetes/ssl/ca-key.pem \\\\\n--service-account-issuer=api \\\\\n--service-account-signing-key-file=/opt/kubernetes/ssl/server-key.pem \\\\\n--etcd-cafile=/opt/etcd/ssl/ca.pem \\\\\n--etcd-certfile=/opt/etcd/ssl/server.pem \\\\\n--etcd-keyfile=/opt/etcd/ssl/server-key.pem \\\\\n--requestheader-client-ca-file=/opt/kubernetes/ssl/ca.pem \\\\\n--proxy-client-cert-file=/opt/kubernetes/ssl/server.pem \\\\\n--proxy-client-key-file=/opt/kubernetes/ssl/server-key.pem \\\\\n--requestheader-allowed-names=kubernetes \\\\\n--requestheader-extra-headers-prefix=X-Remote-Extra- \\\\\n--requestheader-group-headers=X-Remote-Group \\\\\n--requestheader-username-headers=X-Remote-User \\\\\n--enable-aggregator-routing=true \\\\\n--audit-log-maxage=30 \\\\\n--audit-log-maxbackup=3 \\\\\n--audit-log-maxsize=100 \\\\\n--audit-log-path=/opt/kubernetes/logs/k8s-audit.log\"\nEOF\n```\n\n### 创建系统服务文件\n\n```shell\ncat > /lib/systemd/system/kube-apiserver.service << EOF\n[Unit]\nDescription=Kubernetes API Server\nDocumentation=https://github.com/kubernetes/kubernetes\n\n[Service]\nEnvironmentFile=/opt/kubernetes/conf/kube-apiserver.conf\nExecStart=/opt/kubernetes/bin/kube-apiserver \\$KUBE_APISERVER_OPTS\nRestart=on-failure\n\n[Install]\nWantedBy=multi-user.target\nEOF\n```\n\n### 自启启动\n\n```shell\nsystemctl daemon-reload\nsystemctl enable kube-apiserver --now\n```\n\n## kube-controller-manager\n\n### 证书\n\n```shell\n#生成证书\ncat > kube-controller-manager-csr.json << EOF\n{\n  \"CN\": \"system:kube-controller-manager\",\n  \"hosts\": [],\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"CN\",\n      \"L\": \"ShangHai\",\n      \"ST\": \"ShangHai\",\n      \"O\": \"system:masters\",\n      \"OU\": \"System\"\n    }\n  ]\n}\nEOF\n\ncfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-controller-manager-csr.json | cfssljson -bare kube-controller-manager\n\n#拷贝证书\ncp cfssl/kubernetes/kube-controller-manager*.pem /etc/kubernetes/ssl\n```\n\n### 创建配置文件\n\n```shell\ncat > /opt/kubernetes/conf/kube-controller-manager.conf << EOF\nKUBE_CONTROLLER_MANAGER_OPTS=\"--logtostderr=false \\\\\n--v=2 \\\\\n--log-dir=/opt/kubernetes/logs \\\\\n--leader-elect=true \\\\\n--kubeconfig=/opt/kubernetes/conf/kube-controller-manager.kubeconfig \\\\\n--bind-address=127.0.0.1 \\\\\n--allocate-node-cidrs=true \\\\\n--cluster-cidr=10.244.0.0/16 \\\\\n--service-cluster-ip-range=10.0.0.0/24 \\\\\n--cluster-signing-cert-file=/opt/kubernetes/ssl/ca.pem \\\\\n--cluster-signing-key-file=/opt/kubernetes/ssl/ca-key.pem  \\\\\n--root-ca-file=/opt/kubernetes/ssl/ca.pem \\\\\n--service-account-private-key-file=/opt/kubernetes/ssl/ca-key.pem \\\\\n--cluster-signing-duration=87600h0m0s\"\nEOF\n```\n\n### 创建 kubeconfig 文件\n\n```shell\nKUBE_CONFIG=\"/opt/kubernetes/conf/kube-controller-manager.kubeconfig\"\nKUBE_APISERVER=\"https://10.10.10.101:6443\"\n\nkubectl config set-cluster kubernetes \\\n  --certificate-authority=/opt/kubernetes/ssl/ca.pem \\\n  --embed-certs=true \\\n  --server=${KUBE_APISERVER} \\\n  --kubeconfig=${KUBE_CONFIG}\nkubectl config set-credentials kube-controller-manager \\\n  --client-certificate=/opt/kubernetes/ssl/kube-controller-manager.pem \\\n  --client-key=/opt/kubernetes/ssl/kube-controller-manager-key.pem \\\n  --embed-certs=true \\\n  --kubeconfig=${KUBE_CONFIG}\nkubectl config set-context default \\\n  --cluster=kubernetes \\\n  --user=kube-controller-manager \\\n  --kubeconfig=${KUBE_CONFIG}\nkubectl config use-context default --kubeconfig=${KUBE_CONFIG}\n```\n\n### 创建系统服务文件\n\n```shell\ncat > /usr/lib/systemd/system/kube-controller-manager.service << EOF\n[Unit]\nDescription=Kubernetes Controller Manager\nDocumentation=https://github.com/kubernetes/kubernetes\n\n[Service]\nEnvironmentFile=/opt/kubernetes/conf/kube-controller-manager.conf\nExecStart=/opt/kubernetes/bin/kube-controller-manager \\$KUBE_CONTROLLER_MANAGER_OPTS\nRestart=on-failure\n\n[Install]\nWantedBy=multi-user.target\nEOF\n```\n\n### 自启启动\n\n```shell\nsystemctl daemon-reload\nsystemctl enable kube-controller-manager --now\n```\n\n## kube-scheduler\n\n### 证书\n\n```shell\n#生成证书\ncat > kube-scheduler-csr.json << EOF\n{\n  \"CN\": \"system:kube-scheduler\",\n  \"hosts\": [],\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"CN\",\n      \"L\": \"ShangHai\",\n      \"ST\": \"ShangHai\",\n      \"O\": \"system:masters\",\n      \"OU\": \"System\"\n    }\n  ]\n}\nEOF\n\ncfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-scheduler-csr.json | cfssljson -bare kube-scheduler\n\n#拷贝证书\ncp cfssl/kubernetes/kube-cheduler*.pem /etc/kubernetes/ssl\n```\n\n### 创建配置文件\n\n```shell\ncat > /opt/kubernetes/conf/kube-scheduler.conf << EOF\nKUBE_SCHEDULER_OPTS=\"--logtostderr=false \\\\\n--v=2 \\\\\n--log-dir=/opt/kubernetes/logs \\\\\n--leader-elect \\\\\n--kubeconfig=/opt/kubernetes/conf/kube-scheduler.kubeconfig \\\\\n--bind-address=127.0.0.1\"\nEOF\n```\n\n### 创建 kubeconfig 文件\n\n```shell\nKUBE_CONFIG=\"/opt/kubernetes/conf/kube-scheduler.kubeconfig\"\nKUBE_APISERVER=\"https://10.10.10.101:6443\"\n\nkubectl config set-cluster kubernetes \\\n  --certificate-authority=/opt/kubernetes/ssl/ca.pem \\\n  --embed-certs=true \\\n  --server=${KUBE_APISERVER} \\\n  --kubeconfig=${KUBE_CONFIG}\nkubectl config set-credentials kube-scheduler \\\n  --client-certificate=/opt/kubernetes/ssl/kube-scheduler.pem \\\n  --client-key=/opt/kubernetes/ssl/kube-scheduler-key.pem \\\n  --embed-certs=true \\\n  --kubeconfig=${KUBE_CONFIG}\nkubectl config set-context default \\\n  --cluster=kubernetes \\\n  --user=kube-scheduler \\\n  --kubeconfig=${KUBE_CONFIG}\nkubectl config use-context default --kubeconfig=${KUBE_CONFIG}\n```\n\n### 创建系统服务\n\n```shell\ncat > /usr/lib/systemd/system/kube-scheduler.service << EOF\n[Unit]\nDescription=Kubernetes Scheduler\nDocumentation=https://github.com/kubernetes/kubernetes\n\n[Service]\nEnvironmentFile=/opt/kubernetes/conf/kube-scheduler.conf\nExecStart=/opt/kubernetes/bin/kube-scheduler \\$KUBE_SCHEDULER_OPTS\nRestart=on-failure\n\n[Install]\nWantedBy=multi-user.target\nEOF\n```\n\n### 自启启动\n\n```shell\nsystemctl daemon-reload\nsystemctl enable kube-scheduler --now\n```\n\n## 查看集群状态\n\n### 证书\n\n```shell\n#生成证书\ncat > admin-csr.json <<EOF\n{\n  \"CN\": \"admin\",\n  \"hosts\": [],\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"CN\",\n      \"L\": \"ShangHai\",\n      \"ST\": \"ShangHai\",\n      \"O\": \"system:masters\",\n      \"OU\": \"System\"\n    }\n  ]\n}\nEOF\n\ncfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes admin-csr.json | cfssljson -bare admin\n\n#拷贝证书\ncp cfssl/kubernetes/admin*.pem /etc/kubernetes/ssl\n```\n\n### 创建 kubeconfig 文件\n\n```shell\nmkdir /root/.kube\n\nKUBE_CONFIG=\"/root/.kube/config\"\nKUBE_APISERVER=\"https://10.10.10.101:6443\"\n\nkubectl config set-cluster kubernetes \\\n  --certificate-authority=/etc/kubernetes/ssl/ca.pem \\\n  --embed-certs=true \\\n  --server=${KUBE_APISERVER} \\\n  --kubeconfig=${KUBE_CONFIG}\nkubectl config set-credentials cluster-admin \\\n  --client-certificate=//etc/kubernetes/ssl/admin.pem \\\n  --client-key=/etc/kubernetes/ssl/admin-key.pem \\\n  --embed-certs=true \\\n  --kubeconfig=${KUBE_CONFIG}\nkubectl config set-context default \\\n  --cluster=kubernetes \\\n  --user=cluster-admin \\\n  --kubeconfig=${KUBE_CONFIG}\nkubectl config use-context default --kubeconfig=${KUBE_CONFIG}\n```\n\n### 查看当前组建状态\n\n```shell\nkubectl get cs\nNAME                STATUS    MESSAGE             ERROR\nscheduler           Healthy   ok\ncontroller-manager  Healthy   ok\netcd-master         Healthy   {\"health\":\"true\"}\netcd-minio          Healthy   {\"health\":\"true\"}\netcd-slave          Healthy   {\"health\":\"true\"}\n```\n\n### 授权 kubelet-bootstrap 用户允许请求证书\n\n```shell\n#创建node必备，不然node的kubelet无法启动,就是创建一个可以申请证书的用户\nkubectl create clusterrolebinding kubelet-bootstrap \\\n--clusterrole=system:node-bootstrapper \\\n--user=kubelet-bootstrap\n\nkubectl create clusterrolebinding system:anonymous --clusterrole=cluster-admin ?--user=system:anonymous\n```\n\n# Nodes\n\n## 拷贝二进制文件和证书\n\n```shell\nssh minion \"mkdir -p /opt/kubernetes/{bin,conf,ssl,logs}\"\ncd kubernetes/server/bin/\nscp -r kubelet kube-proxy minion:/opt/kubernetes/bin/\nscp /opt/kubernetes/ssl/ca.pem minion:/opt/kubernetes/ssl/\nscp /usr/bin/kubectl minion:/usr/bin\n```\n\n## kubulet\n\n### 创建配置文件\n\n```shell\ncat > /opt/kubernetes/conf/kubelet.conf << EOF\nKUBELET_OPTS=\"--logtostderr=false \\\\\n--v=2 \\\\\n--log-dir=/opt/kubernetes/logs \\\\\n--hostname-override=(节点名) \\\\\n--network-plugin=cni \\\\\n--kubeconfig=/opt/kubernetes/conf/kubelet.kubeconfig \\\\\n--bootstrap-kubeconfig=/opt/kubernetes/conf/bootstrap.kubeconfig \\\\\n--config=/opt/kubernetes/conf/kubelet-config.yml \\\\\n--cert-dir=/opt/kubernetes/ssl \\\\\n--pod-infra-container-image=lizexiong/pause-amd64:3.0(pause镜像)\"\nEOF\n```\n\n### 配置参数文件\n\n```shell\ncat > /opt/kubernetes/conf/kubelet-config.yml << EOF\nkind: KubeletConfiguration\napiVersion: kubelet.config.k8s.io/v1beta1\naddress: 0.0.0.0\nport: 10250\nreadOnlyPort: 10255\ncgroupDriver: systemd(和docker的cgroupdriver保持一致)\nclusterDNS:\n- 10.0.0.2\nclusterDomain: cluster.local\nfailSwapOn: false\nauthentication:\n  anonymous:\n    enabled: false\n  webhook:\n    cacheTTL: 2m0s\n    enabled: true\n  x509:\n    clientCAFile: /opt/kubernetes/ssl/ca.pem\nauthorization:\n  mode: Webhook\n  webhook:\n    cacheAuthorizedTTL: 5m0s\n    cacheUnauthorizedTTL: 30s\nevictionHard:\n  imagefs.available: 15%\n  memory.available: 100Mi\n  nodefs.available: 10%\n  nodefs.inodesFree: 5%\nmaxOpenFiles: 1000000\nmaxPods: 110\nEOF\n```\n\n### 创建 kubeconfig 文件\n\n```shell\nKUBE_CONFIG=\"/opt/kubernetes/conf/bootstrap.kubeconfig\"\nKUBE_APISERVER=\"https://10.10.10.101:6443\" # apiserver IP:PORT\nTOKEN=\"c47ffb939f5ca36231d9e3121a252940\" # 与token.csv里保持一致\n\n# 生成 kubelet bootstrap kubeconfig 配置文件\nkubectl config set-cluster kubernetes \\\n  --certificate-authority=/opt/kubernetes/ssl/ca.pem \\\n  --embed-certs=true \\\n  --server=${KUBE_APISERVER} \\\n  --kubeconfig=${KUBE_CONFIG}\nkubectl config set-credentials \"kubelet-bootstrap\" \\\n  --token=${TOKEN} \\\n  --kubeconfig=${KUBE_CONFIG}\nkubectl config set-context default \\\n  --cluster=kubernetes \\\n  --user=\"kubelet-bootstrap\" \\\n  --kubeconfig=${KUBE_CONFIG}\nkubectl config use-context default --kubeconfig=${KUBE_CONFIG}\n```\n\n### 创建系统服务文件\n\n```shell\ncat > /usr/lib/systemd/system/kubelet.service << EOF\n[Unit]\nDescription=Kubernetes Kubelet\nAfter=docker.service\n\n[Service]\nEnvironmentFile=/opt/kubernetes/conf/kubelet.conf\nExecStart=/opt/kubernetes/bin/kubelet \\$KUBELET_OPTS\nRestart=on-failure\nLimitNOFILE=65536\n\n[Install]\nWantedBy=multi-user.target\nEOF\n```\n\n### 自启启动\n\n```shell\nsystemctl daemon-reload\nsystemctl enable kubelet --now\n```\n\n### master 批准 node 加入集群\n\n```shell\n#获取node-csr名称\nkubectl get csr\n#批准\nkubectl certificate approve <node-csr-name>\n#获取节点信息\nkubectl get node\n```\n\n## kube-proxy\n\n### 创建配置文件\n\n```shell\ncat > /opt/kubernetes/conf/kube-proxy.conf << EOF\nKUBE_PROXY_OPTS=\"--logtostderr=false \\\\\n--v=2 \\\\\n--log-dir=/opt/kubernetes/logs \\\\\n--config=/opt/kubernetes/conf/kube-proxy-config.yml\"\nEOF\n```\n\n### 配置参数文件\n\n```shell\ncat > /opt/kubernetes/conf/kube-proxy-config.yml << EOF\nkind: KubeProxyConfiguration\napiVersion: kubeproxy.config.k8s.io/v1alpha1\nbindAddress: 0.0.0.0\nmetricsBindAddress: 0.0.0.0:10249\nclientConnection:\n  kubeconfig: /opt/kubernetes/conf/kube-proxy.kubeconfig\nhostnameOverride: node01\nclusterCIDR: 10.0.0.0/24\nEOF\n```\n\n### master 生成证书并拷贝\n\n```shell\ncd cfssl/kubernetes\n\ncat > kube-proxy-csr.json << EOF\n{\n  \"CN\": \"system:kube-proxy\",\n  \"hosts\": [],\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"CN\",\n      \"L\": \"ShangHai\",\n      \"ST\": \"ShangHai\",\n      \"O\": \"Kubernets\",\n      \"OU\": \"System\"\n    }\n  ]\n}\nEOF\n\n#生成证书\ncfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxy\n\n#拷贝证书\nscp /cfssl/kubernetes/kube-proxy*pem minion:/opt/kubernetes/ssl\n```\n\n### 创建 kubeconfig 文件\n\n```shell\nKUBE_CONFIG=\"/opt/kubernetes/conf/kube-proxy.kubeconfig\"\nKUBE_APISERVER=\"https://10.10.10.101:6443\"\n\nkubectl config set-cluster kubernetes \\\n  --certificate-authority=/opt/kubernetes/ssl/ca.pem \\\n  --embed-certs=true \\\n  --server=${KUBE_APISERVER} \\\n  --kubeconfig=${KUBE_CONFIG}\nkubectl config set-credentials kube-proxy \\\n  --client-certificate=/opt/kubernetes/ssl/kube-proxy.pem \\\n  --client-key=/opt/kubernetes/ssl/kube-proxy-key.pem \\\n  --embed-certs=true \\\n  --kubeconfig=${KUBE_CONFIG}\nkubectl config set-context default \\\n  --cluster=kubernetes \\\n  --user=kube-proxy \\\n  --kubeconfig=${KUBE_CONFIG}\nkubectl config use-context default --kubeconfig=${KUBE_CONFIG}\n```\n\n### 创建系统服务文件\n\n```shell\ncat > /usr/lib/systemd/system/kube-proxy.service << EOF\n[Unit]\nDescription=Kubernetes Proxy\nAfter=network.target\n\n[Service]\nEnvironmentFile=/opt/kubernetes/conf/kube-proxy.conf\nExecStart=/opt/kubernetes/bin/kube-proxy \\$KUBE_PROXY_OPTS\nRestart=on-failure\nLimitNOFILE=65536\n\n[Install]\nWantedBy=multi-user.target\nEOF\n```\n\n### 自启启动\n\n```shell\nsystemctl daemon-reload\nsystemctl enable kube-proxy --now\n```\n\n## 部署网络组件\n\n### calico\n\n```shell\nhttps://blog.csdn.net/qq_31136839/article/details/100032022\n#下载yaml\nwget https://docs.projectcalico.org/manifests/calico.yaml\n\n#修改CALICO_IPV4POOL_CIDR,此处的网段地址应和kube-controller-manager.conf中的--cluster-cidr保持一致,修改后应用\nkubectl apply -f calico.yaml\n\n#查看pod直到成功运行\nkubectl get pods -n kube-system\n```\n\n### flannel\n\n```shell\n#修改Network,此处的网段地址应和kube-controller-manager.conf中的--cluster-cidr保持一致,修改后应用\nkubectl apply -f flannel.yaml\n\n#查看pod直到成功运行\nkubectl get pods -n kube-system\n\n#如果报错找不到/run/flannel/subnet.env\ncat > /run/flannel/subnet.env << EOF\nFLANNEL_NETWORK=10.244.0.0/16\nFLANNEL_SUBNET=10.244.0.1/24\nFLANNEL_MTU=1450\nFLANNEL_IPMASQ=true\nEOF\n```\n\n## 授权 apiserver\n\n```shell\ncat > apiserver-to-kubelet-rbac.yaml << EOF\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  annotations:\n    rbac.authorization.kubernetes.io/autoupdate: \"true\"\n  labels:\n    kubernetes.io/bootstrapping: rbac-defaults\n  name: system:kube-apiserver-to-kubelet\nrules:\n  - apiGroups:\n      - \"\"\n    resources:\n      - nodes/proxy\n      - nodes/stats\n      - nodes/log\n      - nodes/spec\n      - nodes/metrics\n      - pods/log\n    verbs:\n      - \"*\"\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: system:kube-apiserver\n  namespace: \"\"\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: system:kube-apiserver-to-kubelet\nsubjects:\n  - apiGroup: rbac.authorization.k8s.io\n    kind: User\n    name: kubernetes\nEOF\n\nkubectl apply -f apiserver-to-kubelet-rbac.yaml\n```\n\n## 部署 Coredns\n\n```shell\nhttps://github.com/kubernetes/kubernetes/blob/master/cluster/addons/dns/coredns/coredns.yaml.in\n\n#修改dns_memory_limit和clusterIP,clusterIP地址应和kubelet-config.yml中的clusterDNS保持一致,修改后应用\nkubelet apply -f coredns.yaml\n\n#授权\nkubectl create clusterrolebinding add-on-cluster-admin --clusterrole=cluster-admin --serviceaccount=kube-system:coredns\n\n#查看pod成功运行\nkubectl get pods -n kube-system\n```\n","tags":["K8S","容器"],"categories":["devops"]},{"title":"Commands","url":"/posts/6348717a.html","content":"<!-- block -->\n>&emsp;&emsp;总结一些常用命令\n<!-- block -->\n# Linux\n\n## History 添加时间戳\n\n```bash\necho 'export HISTTIMEFORMAT=\"`whoami` : %F %T : \"' >> /etc/profile . /etc/profile\n```\n\n## 美化 PS1\n\n```bash\n# [用户@主机:路径 时间] 上一条命令结果 >\nexport PS1=\"\\[\\e[m\\][\\[\\e[31;1m\\]\\u\\e[34;1m\\]@\\[\\e[33;1m\\]\\h\\\\e[34;1m\\]:\\e[32;1m\\]\\w \\e[36;1m\\]\\t\\e[m\\]] \\e[31;1m\\]$? \\e[34;1m\\]> \\[\\e[m\\]\"\n```\n\n## 批量卸载挂载卷\n```bash\nmount | grep '/data/kubelet' | awk '{print $3}'|xargs umount\n```\n\n## Ubuntu 修改 dash\n\n```bash\nsudo dpkg-reconfigure dash\n```\n\n## vim 添加脚本简介\n\n```bash\nvim ~/.vimrc\nautocmd BufNewFile *.py,*.sh, exec \":call SetTitle()\"\nlet $author_name=\"Felix Cheung\"\nlet $author_email=\"felix7cheung@163.com\"\n\nfunc SetTitle()\nif &filetype == 'sh'\ncall setline(1,\"\\#!/bin/bash\")\ncall append(line(\".\"),   \"\\#File Name    : \".expand(\"%\"))\ncall append(line(\".\")+1, \"\\#Author       : \".$author_name)\ncall append(line(\".\")+2, \"\\#Mail         : \".$author_email)\ncall append(line(\".\")+3, \"\\#Create Time  : \".strftime(\"%Y-%m-%d %H:%M\"))\ncall append(line(\".\")+4, \"\\#Description  : \")\ncall append(line(\".\")+5, \"\")\nelse\ncall setline(1,\"\\#!/usr/bin/env python\")\ncall append(line(\".\"),   \"\\#File Name    : \".expand(\"%\"))\ncall append(line(\".\")+1, \"\\#Author       : \".$author_name)\ncall append(line(\".\")+2, \"\\#Mail         : \".$author_email)\ncall append(line(\".\")+3, \"\\#Create Time  : \".strftime(\"%Y-%m-%d %H:%M\"))\ncall append(line(\".\")+4, \"\\#Description  : \")\ncall append(line(\".\")+5, \"\")\nendif\nendfunc\nautocmd BufNewfile * normal G\n```\n\n## centos 换源\n\n```bash\nmkdir /opt/centos-yum.bak\nmv /etc/yum.repos.d/* /opt/centos-yum.bak/\nwget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo\n\nyum clean all\nyum makecache\n```\n\n# Python\n\n## 文件下载器, 当前路径\n\n```bash\npython -m http.server（可直接加端口，不加默认8000）\n```\n\n## 升级 pip\n\n```bash\npython -m pip install --upgrade pip\n```\n\n## py 环境变量\n\n```bash\n/usr/bin/env python\n```\n\n## 换源\n\n```bash\npip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple\n```\n\n# Go\n\n## 换源\n\n```bash\ngo env -w GOPROXY=https://goproxy.cn,direct // 切换下载代理\n```\n\n## cobra\n\n```bash\ngo get -u github.com/spf13/cobra@latest\ngo install github.com/spf13/cobra-cli@latest\n\ncobra-cli init\ncobra-cli add xxxx\n```\n\n## 静态编译\n\n```bash\nCGO_ENABLED=0 GOOS=linux go build -a -ldflags '-extldflags \"-static\" -w -s' .\n```\n\n## 压缩\n\n```bash\nupx -9 xxx\n```\n\n# Jenkins\n\n## 换源\n\n```bash\nsed -i 's/http:\\/\\/updates.jenkins-ci.org\\/download/https:\\/\\/mirrors.tuna.tsinghua.edu.cn\\/jenkins/g' default.json && sed -i 's/http:\\/\\/www.google.com/https:\\/\\/www.baidu.com/g' $JENKINS_HOME/update/default.json\n\nhttps://mirrors.tuna.tsinghua.edu.cn/jenkins/updates/update-center.json\n```\n\n## 查用插件\n\n```\nChinese\nRole-based Authorization Strategy\nJob Configuration History\nWorkspace Cleanup Plugin\n```\n\n# Docker\n\n## 自动安装\n\n```bash\ncurl -fsSL https://get.docker.com | bash -s docker --mirror Aliyun\n```\n\n## 配置镜像加速地址\n\n```bash\nmkdir /etc/docker/ -p\ncat > /etc/docker/daemon.json << EOF\n{\n    \"registry-mirrors\":[\"https://docker.mirrors.ustc.edu.cn/\"],\n     \"data-root\": \"/mnt/docker\"\n}\nEOF\n```\n\n## 删除镜像\n```bash\n# 清理悬虚镜像\ndocker image prune\n\n# 普通删除镜像\ndocker rmi `docker images -q`\n\n# 遇到一些 IMAGE ID 相同但是 TAG 不同的镜像,需要将 IMAGE NAME 和 TAG 拼接再删除\ndocker images | grep <key_word> | awk '{print$1\":\"$2}' | xargs docker rmi\n```\n\n## 构建多架构镜像\n```bash\n# 在不同架构下构建镜像后推送到仓库\ndocker push registry.tecorigin.io:5443/test/teco-device-plugin:v1.3.6-sw64\ndocker push registry.tecorigin.io:5443/test/teco-device-plugin:v1.3.6-amd64\n\n# 创建清单，将多架构镜像并入一个清单列表中\ndocker manifest create --insecure \\ \n       registry.tecorigin.io:5443/test/teco-device-plugin:v1.3.6 \\\n       registry.tecorigin.io:5443/test/teco-device-plugin:v1.3.6-sw64 \\\n       registry.tecorigin.io:5443/test/teco-dev\tice-plugin:v1.3.6-amd64\n\n# 标注多架构镜像，添加元信息\ndocker manifest annotate registry.tecorigin.io:5443/test/teco-device-plugin:v1.3.6 \\\n                         registry.tecorigin.io:5443/test/teco-device-plugin:v1.3.6-sw64 \\\n                         --os linux --arch sw64\ndocker manifest annotate registry.tecorigin.io:5443/test/teco-device-plugin:v1.3.6 \\\n                         registry.tecorigin.io:5443/test/teco-device-plugin:v1.3.6-amd64 \\\n                         --os linux --arch amd64\n#查看清单信息，是否包含arch等信息\ndocker manifest inspect registry.tecorigin.io:5443/test/teco-device-plugin:v1.3.6\n\n# 推送清单\ndocker manifest push --insecure registry.tecorigin.io:5443/test/teco-device-plugin:v1.3.6\n\n# 完成之后可以在不同架构机器上使用同一个TAG拉取不同对应架构的镜像\ndocker pull registry.tecorigin.io:5443/test/teco-device-plugin:v1.3.6\n```\n>1. 普通多架构如 amd64/arm64 或者 使用同一个 dockerfile 可以使用 docker buildx 更好得完成多架构镜像的构建，但 sw64 这种比较特殊，目前发现只能用 docker manifest 的方式，比较麻烦。\n>2. 通过 docker manifest annotate 设置 os 和 arch 时，最好先用 docker verison 确认一下\n\n# Kubenetes\n\n## 命令补全\n\n```bash\nyum install -y bash-completion\nsource /usr/share/bash-completion/bash_completion\nsource <(kubectl completion bash)\n```\n\n## 格式化输出 ns 下 pvc 对应的 sc\n```bash\nkubectl get pvc -A | awk '{print $1,$2,$(NF-1)}' | column -t\n```\n\n## 删除已释放的 pv\n```bash\nkubectl get pv | grep Released | awk '{print$1}' | xargs kubectl delete pv\n```\n\n## 排序输出 ns 下的 helm 应用\n```bash\nhelm list -A | awk '{print$2,$1}' | column -t | sort\n```\n\n## 批量卸载 helm 应用\n```bash\nhelm list -A | awk '{system(\"helm uninstall \" $1 \" -n \" $2)}'\n```\n\n## 切换默认 ns\n```bash\nkubectl config set-context $(kubectl config current-context) --namespace=<work_ns>\n```\n\n## 删除 terminating 状态的 pod/pvc 等资源\n```bash\nkubectl -n <ns_name> patch <resource> <resource_name> -p '{\"metadata\":{\"finalizers\":null}}'\n```\n\n## 删除 terminal 状态的 ns\n\n```\nhttps://blog.csdn.net/qq_44273583/article/details/124594314\n```\n\n## 调整端口范围\n\n```bash\nvim /etc/kubernetes/manifests/kube-apiserver.yaml\n  -    --service-node-port-range=1-65535\n```\n\n## 证书过期\n\n```\nUnable to connect to the server: x509: certificate has expired or is not yet valid\nhttps://blog.csdn.net/swan_tang/article/details/115755311\n```\n\n# velero\n## 使用对象存储\n> 此处搭建 minio 并创建 velero 的 bucket（略）\n\n## 创建认证信息\nvim credentials-velero\n```properties\n[default]\naws_access_key_id = minio\naws_secret_access_key = Teco@135\n```\n\n## 安装velero\n> 需要下载 velero 可执行文件 <br>\n> 视情况修改 install 选项  <br>\n> 修改 s3 地址\n```bash\nvelero install \\\n    --provider aws \\\n    --plugins velero/velero-plugin-for-aws:v1.2.1 \\\n    --bucket velero \\\n    --secret-file ./credentials-velero \\\n    --use-volume-snapshots=false \\\n    --backup-location-config region=minio,s3ForcePathStyle=\"true\",s3Url=http://175.0.2.5:19000\n```\n\n## 备份\n```bash\nvelero backup create <backup_name> \\\n       --include-namespaces <ns_name>,<ns_name> \t\\\n       --exclude-resources <resource_name>,<resource_name>\n```\n\n## 还原\n```bash\nvelero restore create --from-backup <backup_name>\n```\n\n## 定时备份\n```bash\nvelero schedule create <schedule_name> --schedule=\"0 2 * * 5\" --include-namespaces=<ns_name>\n```\n\n>多个K8S集群安装velero时使用同一个s3地址，可以先在A集群中备份，再到B集群中还原，完成资源迁移，但是不包括 crd 自定义资源","tags":["linux","K8S"],"categories":["devops"]},{"title":"通过Kubeadm搭建Kubernetes高可用集群","url":"/posts/f6b6db86.html","content":"<!-- block -->\n>&emsp;&emsp;学习搭建使用 `Kubeadm` 搭建 `Kubernetes`  高可用集群\n<!-- block -->\n\n# 学习环境\n\nvmware 安装三台虚拟机，或者先安装一台，完成下面的准备操作后克隆两台虚拟机，再修改 IP；主机名可以在安装时候设置，或者装完之后通过 hostnamectl set-hostname 修改\n\n| hostname |    os     |    static ip    |      role      |\n| :------: | :-------: | :-------------: | :------------: |\n|  master  | centos7.8 | 192.168.204.141 | master1，node1 |\n|  backup  | centos7.8 | 192.168.204.142 | master2，node2 |\n|   node   | centos7.8 | 192.168.204.143 | master3，node3 |\n\n没有特殊说明，默认每台虚拟机都要操作\n\n网站文档：https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/install-kubeadm/\n\n# 准备操作\n\n```shell\n#临时关闭防火墙\nsystemctl stop firewalld\n#禁用，重启生效\nsystemctl disable firewalld\n\n#临时关闭\nsetenforce 0\n#禁用，重启生效\nsed -i \"s/enforcing/disabled/g\" /etc/selinux/config\n\n#临时关闭\nswapoff -a\n#永久关闭\nsed -i 's/^[^#].*swap*/#&/g' /etc/fstab\n\n#安装常用软件包\nyum install vim net-tools lrzsz unzip dos2unix telnet sysstat iotop pciutils lsof tcpdump psmisc bc wget socat gcc tree chrony ntpdate mlocate -y\n\n#主机名写入hosts\ncat >> /etc/hosts << EOF\n192.168.204.141 master\n192.168.204.142 backup\n192.168.204.143 node\nEOF\n\n#主机间配置ssh免密登录\nssh-keygen -t rsa\nfor host in master backup node; do ssh-copy-id -i ~/.ssh/id_rsa.pub $host;done\n\n#内核开启网络支持\ncat > /etc/sysctl.d/kubernetes.conf << EOF\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-iptables = 1\nnet.ipv4.ip_forward = 1\nnet.ipv4.ip_nonlocal_bind = 1\nEOF\nmodprobe br_netfilter\nsysctl -p /etc/sysctl.d/kubernetes.conf\n```\n\n# 安装 Docker\n\n```shell\n#官方脚本安装\ncurl -fsSL https://get.docker.com | bash -s docker --mirror Aliyun\n\n#手动yum安装\n#卸载原有docker\nyum remove docker*\n#安装所需的软件包。yum-utils 提供了 yum-config-manager ，并且 device mapper 存储驱动程序需要 device-mapper-persistent-data 和 lvm2\nyum install -y yum-utils device-mapper-persistent-data lvm2\n#设置阿里源地址\nyum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo\n#安装docker-ce\nyum install docker-ce docker-ce-cli containerd.io -y\n\n#配置镜像加速地址,并修改Cgroup Driver(docker默认cgroupfs，k8s推荐使用systemd)\nmkdir /etc/docker/ -p\ncat > /etc/docker/daemon.json << EOF\n{\"registry-mirrors\":[\"https://docker.mirrors.ustc.edu.cn/\"],\n  \"exec-opts\": [\"native.cgroupdriver=systemd\"]}\nEOF\nsystemctl daemon-reload\n#启动docker并设置自启\nfor op in enable start status;do systemctl $op docker;done\n```\n\n# Master\n\n```shell\n#配置k8s阿里源地址\ncat <<EOF > /etc/yum.repos.d/kubernetes.repo\n[kubernetes]\nname=Kubernetes\nbaseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64\nenabled=1\ngpgcheck=0\nrepo_gpgcheck=0\ngpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg\nhttp://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg\nEOF\n\n#安装k8s相关\nyum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes\nsystemctl enable --now kubelet\n```\n\n## keepalived\n\n```shell\n#安装keeplived并备份配置⽂件\nfor host in master backup node;do ssh root@$host \"yum install -y keepalived\";done\nfor host in master backup node;do ssh root@$host \"mv /etc/keepalived/keepalived.conf /etc/keepalived/keepalived.conf.bak\";done\n\n#修改keepalived配置⽂件如下，标注的地⽅需要修改\ncat > /etc/keepalived/keepalived.conf << EOF\n! Configuration File for keepalived\nglobal_defs {\n    router_id master                        #标识，可⽤机器主机名作为标识\n}\nvrrp_instance VI_1 {\n    state MASTER                             #设置⻆⾊，第⼀个master为MASTER，剩余的节点均为BACKUP\n    interface ens32                         #设置vip绑定端⼝\n    virtual_router_id 51                     #让master和backup在同⼀个虚拟路由⾥，id号必须相同\n    priority 100                             #优先级,谁的优先级⾼谁就是master，值越⼤优先级越⾼\n    advert_int 1                             #⼼跳间隔时间\n    authentication {\n        auth_type PASS                         #认证\n        auth_pass k8s                         #密码\n    }\n     virtual_ipaddress {\n        192.168.204.200                        #虚拟ip\n    }\n}\nEOF\n\n#启动keepalived并设置开机⾃启\nfor op in enable start status;do systemctl $op keepalived;done\n```\n\n## haproxy(可以不装，不装则不负载)\n\n```shell\n#安装haproxy并备份配置⽂件\nfor host in master backup node;do ssh root@$host \"yum install -y haproxy\";done\nfor host in master backup node;do ssh root@$host \"mv /etc/haproxy/haproxy.cfg /etc/haproxy/haproxy.cfg.bak\";done\n\n#修改haproxy配置⽂件如下，标注的地⽅需要修改\ncat > /etc/haproxy/haproxy.cfg << EOF\nglobal\n        chroot /var/lib/haproxy\n        daemon\n        group haproxy\n        user haproxy\n        log 127.0.0.1:514 local0 warning\n        pidfile /var/lib/haproxy.pid\n        maxconn 20000\n        spread-checks 3\n        nbproc 8\ndefaults\n        log global\n        mode tcp\n        retries 3\n        option redispatch\nlisten k8s-apiserver\n        bind 0.0.0.0:8443\n        mode tcp\n        balance roundrobin\n        timeout server 15s\n        timeout connect 15s\n        server k8sapiserver1 192.168.204.141:6443 check port 6443 inter 5000 fall 5\n        server k8sapiserver2 192.168.204.142:6443 check port 6443 inter 5000 fall 5\n        server k8sapiserver3 192.168.204.143:6443 check port 6443 inter 5000 fall 5\nEOF\n\n#启动haproxy并设置开机⾃启\nfor op in enable start status;do systemctl $op haproxy;done\n```\n\n# 初始化\n\n## 主 Master\n\n```shell\n#初始化master\nA:参数初始(可选参数较多，命令会比较长)\nkubeadm init \\\n--apiserver-advertise-address=192.168.204.141 \\\n--kubernetes-version v1.20.4 \\\n--service-cidr=192.100.0.0/16 \\\n--pod-network-cidr=192.244.0.0/16 \\\n--control-plane-endpoint 192.168.204.200:8443 \\\n--image-repository registry.cn-hangzhou.aliyuncs.com/google_containers\n\nB:配置文件初始(通过配置文件修改参数，复用性高，易修改)\n#导出默认配置文件\nkubeadm config print init-defaults > kubeadm-init.yml\nvim kubeadm-init.yml\n//\napiVersion: kubeadm.k8s.io/v1beta2\nbootstrapTokens:\n- groups:\n  - system:bootstrappers:kubeadm:default-node-token\n  token: abcdef.0123456789abcdef                        #token,默认值即可\n  ttl: 2400h0m0s                                         #token有效期，添加节点如果token过期需要重新⽣成\n  usages:\n  - signing\n  - authentication\nkind: InitConfiguration\nlocalAPIEndpoint:\n  advertiseAddress: 192.168.204.131                   #修改为本地主机IP\n  bindPort: 6443\nnodeRegistration:\n  criSocket: /var/run/dockershim.sock\n  name: master                                        #默认为本地主机名\n  taints:\n  - effect: NoSchedule\n    key: node-role.kubernetes.io/master\n---\napiServer:\n  timeoutForControlPlane: 4m0s\napiVersion: kubeadm.k8s.io/v1beta2\ncertificatesDir: /etc/kubernetes/pki\nclusterName: kubernetes\ncontrolPlaneEndpoint: \"192.168.204.200:8443\"                 #单master节点时这⾥写“本地真实IP:6443”；多master节点时写“VIP:端⼝”，端⼝要与haproxy配置中的bind字段的端⼝⼀致（如果没有装haproxy则仍然是6443）\ncontrollerManager: {}\ndns: {}\netcd:\n  local:\n    dataDir: /var/lib/etcd\nimageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers     #镜像地址，默认为k8s地址，此处已修改为阿里云\nkind: ClusterConfiguration\nkubernetesVersion: v1.20.4                                    #指定k8s版本，可以通用kubectl version查看获取\nnetworking:\n  dnsDomain: cluster.local\n  podSubnet: 192.244.0.0/16                                    #指定Pod的⽹络范围\n  serviceSubnet: 192.100.0.0/16                                #指定Service的⽹络范围\nscheduler: {}\n//\n\n#预下载镜像\nkubeadm config images pull --config kubeadm-init.yml\n\n#初始化\nkubeadm init --config kubeadm-init.yml\n\n#配置kubectl的config⽂件\nmkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n\n#安装flannel网络插件（k8s支持很多网络插件，此处以flannel为例）\ncat > /run/flannel/subnet.env << EOF\nFLANNEL_NETWORK=10.244.0.0/16\nFLANNEL_SUBNET=10.244.0.1/24\nFLANNEL_MTU=1450\nFLANNEL_IPMASQ=true\nEOF\n\nkubectl apply -f https://raw.githubusercontent.com/coreos/flannel/2140ac876ef134e0ed5af15c65e414cf26827915/Documentation/kube-flannel.yml\n\n#配置命令补全\nyum install -y bash-completion\nsource /usr/share/bash-completion/bash_completion\nsource <(kubectl completion bash)\n\n#切换默认命名空间\nkubectl config set-context $(kubectl config current-context) --namespace=kube-system\n\n#查看pod情况\nkubectl get pod -o wide\n\n#初始化失败或者反复可以重置\nkubeadm reset\n\n#复制一下脚本内容\nvim add_master.sh\n#/bin/bash\nCONTROL_PLANE_IPS=\"backup node\"                                               #多个节点以空格隔开\nfor host in ${CONTROL_PLANE_IPS}\ndo\n ssh root@${host} \"mkdir -p /etc/kubernetes/pki/etcd\"\n scp -r /etc/kubernetes/pki/ca.* root@${host}:/etc/kubernetes/pki/\n scp -r /etc/kubernetes/pki/sa.* root@${host}:/etc/kubernetes/pki/\n scp -r /etc/kubernetes/pki/front-proxy-ca.* root@${host}:/etc/kubernetes/pki/\n scp -r /etc/kubernetes/pki/etcd/ca.* root@${host}:/etc/kubernetes/pki/etcd/\n scp -r /etc/kubernetes/admin.conf root@${host}:/etc/kubernetes/\ndone\n#执行脚本，将证书信息传到其他副master\nsh add_master.sh\n```\n\n## 副 Master\n\n```shell\n#加入进已经初始化的主master(此处命令在主master初始化会有)\nkubeadm join 192.168.204.200:8443 --token abcdef.0123456789abcdef --discovery-token-ca-cert-hash sha256:4159d86c40fea91f3b7a2669eb4ca0e927466928e7df0a901c11b40dfd766dda --control-plane\n\n#配置kubectl的config⽂件\nmkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n\n#安装flannel网络插件（k8s支持很多网络插件，此处以flannel为例）\nkubectl apply -f https://raw.githubusercontent.com/coreos/flannel/2140ac876ef134e0ed5af15c65e414cf26827915/Documentation/kube-flannel.yml\n\n#查看节点情况（如果不安装网络插件，查看的STATUS会为NOT READY）\nkubectl get nodes\nNAME     STATUS   ROLES                  AGE     VERSION\nbackup   Ready    control-plane,master   9m53s   v1.20.4\nmaster   Ready    control-plane,master   32m     v1.20.4\nnode     Ready    control-plane,master   4m20s   v1.20.4\n```\n\n## 设置为 node\n\n```shell\n#查看节点容忍，当前三个节点无法调用pod\nkubectl describe node master |grep -E '(Role|Taint)'\n#取消污点后，master可以当作node\nkubectl taint nodes master node-role.kubernetes.io/master-\n```\n\n## token 过期\n\n```shell\n#列出token信息\nkubeadm token list\n#创建新的token\nkubeadm token create\n#获取ca证书sha256编码hash值\nopenssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //'\n#生成新token和获取hash后放入之前的join命令重新拼接执行\n```\n","tags":["K8S","容器"],"categories":["devops"]},{"title":"通过Kubeadm搭建Kubernetes单主多从集群","url":"/posts/688c3b79.html","content":"<!-- block -->\n>&emsp;&emsp;学习搭建使用 `Kubeadm` 搭建 `Kubernetes`  单主多从集群\n<!-- block -->\n\n# 学习环境\n\nvmware 安装三台虚拟机，或者先安装一台，完成下面的准备操作后克隆两台虚拟机，再修改 IP；主机名可以在安装时候设置，或者装完之后通过 hostnamectl set-hostname 修改；\n\n| hostname |    os     |    static ip    |  role  |\n| :------: | :-------: | :-------------: | :----: |\n|  master  | centos7.6 | 192.168.204.131 | master |\n|  minion  | centos7.6 | 192.168.204.132 | node1  |\n|  slave   | centos7.6 | 192.168.204.133 | node2  |\n\n没有特殊说明，默认每台虚拟机都要操作；\n\n网站文档：https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/install-kubeadm/；\n\n# 准备操作\n\n```shell\n#临时关闭\nsystemctl stop firewalld\n#禁用，重启生效\nsystemctl disable firewalld\n\n#临时关闭\nsetenforce 0\n#禁用,重启生效\nsed -i \"s/enforcing/disabled/g\" /etc/selinux/config\n\n#临时关闭\nswapoff -a\n#永久关闭\nsed -i 's/^[^#].*swap*/#&/g' /etc/fstab\n\n#安装常用软件包\nyum install vim net-tools lrzsz unzip dos2unix telnet sysstat iotop pciutils lsof tcpdump psmisc bc wget socat gcc tree chrony ntpdate mlocate -y\n\n#主机名写入hosts\ncat >> /etc/hosts << EOF\n192.168.204.131 master\n192.168.204.132 minion\n192.168.204.133 slave\nEOF\n\n#主机间配置ssh免密登录\nssh-keygen -t rsa\nfor host in master minion slave; do ssh-copy-id -i ~/.ssh/id_rsa.pub $host;done\n\n#内核开启网络支持\ncat > /etc/sysctl.d/kubernetes.conf << EOF\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-iptables = 1\nnet.ipv4.ip_forward = 1\nnet.ipv4.ip_nonlocal_bind = 1\nEOF\nmodprobe br_netfilter\nsysctl -p /etc/sysctl.d/kubernetes.conf\n```\n\n# Docker\n\n```shell\n#官方脚本安装\ncurl -fsSL https://get.docker.com | bash -s docker --mirror Aliyun\n\n#手动yum安装\n#卸载原有docker\nyum remove docker*\n#安装所需的软件包。yum-utils 提供了 yum-config-manager ，并且 device mapper 存储驱动程序需要 device-mapper-persistent-data 和 lvm2\nyum install -y yum-utils device-mapper-persistent-data lvm2\n#设置阿里源地址\nyum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo\n#安装\nyum install docker-ce docker-ce-cli containerd.io -y\n\n#配置镜像加速地址,并修改Cgroup Driver(docker默认cgroupfs，k8s推荐使用systemd)\nmkdir /etc/docker -p\ncat > /etc/docker/daemon.json << EOF\n{\"registry-mirrors\":[\"https://docker.mirrors.ustc.edu.cn/\"],\n  \"exec-opts\": [\"native.cgroupdriver=systemd\"]}\nEOF\nsystemctl daemon-reload\n#启动docker并设置自启\nfor op in enable start status;do systemctl $op docker;done\n```\n\n# Master\n\n```shell\n#配置k8s阿里源地址\ncat <<EOF > /etc/yum.repos.d/kubernetes.repo\n[kubernetes]\nname=Kubernetes\nbaseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64\nenabled=1\ngpgcheck=0\nrepo_gpgcheck=0\ngpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg\nhttp://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg\nEOF\n\n#安装k8s相关\nyum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes\nsystemctl enable --now kubelet\n\n#初始化master\nA:参数初始(可选参数较多，命令会比较长)\nkubeadm init \\\n--apiserver-advertise-address=192.168.204.131 \\\n--kubernetes-version v1.20.4 \\\n--service-cidr=192.100.0.0/16 \\\n--pod-network-cidr=192.244.0.0/16 \\\n--control-plane-endpoint 192.168.204.200:8443 \\\n--image-repository registry.cn-hangzhou.aliyuncs.com/google_containers\n\nB:配置文件初始(通过配置文件修改参数，复用性高，易修改)\n#导出默认配置文件\nkubeadm config print init-defaults > kubeadm-init.yml\nvim kubeadm-init.yml\n//\napiVersion: kubeadm.k8s.io/v1beta2\nbootstrapTokens:\n- groups:\n  - system:bootstrappers:kubeadm:default-node-token\n  token: abcdef.0123456789abcdef                        #token,默认值即可\n  ttl: 2400h0m0s                                         #token有效期，添加节点如果token过期需要重新⽣成\n  usages:\n  - signing\n  - authentication\nkind: InitConfiguration\nlocalAPIEndpoint:\n  advertiseAddress: 192.168.204.131                   #修改为本地主机IP\n  bindPort: 6443\nnodeRegistration:\n  criSocket: /var/run/dockershim.sock\n  name: master                                        #默认为本地主机名\n  taints:\n  - effect: NoSchedule\n    key: node-role.kubernetes.io/master\n---\napiServer:\n  timeoutForControlPlane: 4m0s\napiVersion: kubeadm.k8s.io/v1beta2\ncertificatesDir: /etc/kubernetes/pki\nclusterName: kubernetes\ncontrolPlaneEndpoint: \"192.168.204.131:6443\"                 #单master节点时这⾥写“本地真实IP:6443”；多master节点时写“VIP:端⼝”，端⼝要与haproxy配置中的bind字段的端⼝⼀致\ncontrollerManager: {}\ndns: {}\netcd:\n  local:\n    dataDir: /var/lib/etcd\nimageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers    #镜像地址，默认为k8s地址，此处已修改为阿里云\nkind: ClusterConfiguration\nkubernetesVersion: v1.20.4                                    #指定k8s版本，可以通用kubectl version查看获取\nnetworking:\n  dnsDomain: cluster.local\n  podSubnet: 192.244.0.0/16                                    #指定Pod的⽹络范围\n  serviceSubnet: 192.100.0.0/16                                #指定Service的⽹络范围\nscheduler: {}\n//\n\n#预下载镜像\nkubeadm config images pull --config kubeadm-init.yml\n\n#初始化\nkubeadm init --config kubeadm-init.yml\n\n#配置kubectl的config⽂件(此处命令在master初始化会有)\nmkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n\n#安装flannel网络插件（k8s支持很多网络插件，此处以flannel为例）\ncat > /run/flannel/subnet.env << EOF\nFLANNEL_NETWORK=10.244.0.0/16\nFLANNEL_SUBNET=10.244.0.1/24\nFLANNEL_MTU=1450\nFLANNEL_IPMASQ=true\nEOF\n\nkubectl apply -f https://raw.githubusercontent.com/coreos/flannel/2140ac876ef134e0ed5af15c65e414cf26827915/Documentation/kube-flannel.yml\n\n#配置命令补全\nyum install -y bash-completion\nsource /usr/share/bash-completion/bash_completion\nsource <(kubectl completion bash)\n\n#切换默认命名空间\nkubectl config set-context $(kubectl config current-context) --namespace=kube-system\n\n#查看pod情况\nkubectl get pod -o wide\n\n#初始化失败或者反复可以重置\nkubeadm reset\n```\n\n# Nodes\n\n```shell\n#配置k8s阿里源地址\ncat <<EOF > /etc/yum.repos.d/kubernetes.repo\n[kubernetes]\nname=Kubernetes\nbaseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64\nenabled=1\ngpgcheck=0\nrepo_gpgcheck=0\ngpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg\nhttp://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg\nEOF\n\n#安装k8s相关\nyum install -y kubelet kubeadm --disableexcludes=kubernetes\n\n#启动kubelet并设置自启\nfor op in enable start status;do systemctl $op kubelet;done\n\n#加入nodes(此处命令在master初始化会有)\nkubeadm join 192.168.204.131:6443 --token abcdef.0123456789abcdef\n    --discovery-token-ca-cert-hash sha256:724d623dc0cde1d350ebea86b6a583c3221a2bdef735db804b145eac751037b0\n#如果忘记通过以下命令查看\nkubeadm token create --print-join-command\n\n#node节点join之后，可以在master查看节点情况（如果不安装网络插件，查看的STATUS会为NOT READY）\nkubectl get nodes\nNAME     STATUS   ROLES                  AGE     VERSION\nmaster   Ready    control-plane,master   32m     v1.20.4\nminion   Ready    <none>                   9m      v1.20.4\nslave    Ready    <none>                4m20s   v1.20.4\n```\n\n# 安装 dashboard（Master）\n\n```shell\n#下载默认yaml文件\nwget https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta4/aio/deploy/recommended.yaml\n\n#修改yaml\nvim recommended.yaml修改NodePort\n//\nkind: Service\napiVersion: v1\nmetadata:\n  labels:\n    k8s-app: kubernetes-dashboard\n  name: kubernetes-dashboard\n  namespace: kubernetes-dashboard\nspec:\n  type: NodePort                              #默认没有，添加\n  ports:\n    - port: 443\n      targetPort: 8443\n      nodePort: 30001                        #默认没有，添加，30001为自定义端口\n  selector:\n    k8s-app: kubernetes-dashboard\n//\n#启动dashboard，执行命令之后可以稍微等一会\nkubectl apply -f recommended.yaml\n\n#查看dashboard的pod\nkubectl get pod -n kubernetes-dashboard | grep dashboard\n\n#生成证书\nopenssl genrsa -out dashboard.key 2048\nopenssl req -new -out dashboard.csr -key dashboard.key -subj '/CN=192.168.204.131'\nopenssl x509 -req -days 3650 -in dashboard.csr -signkey dashboard.key -out dashboard.crt\n#删除原有证书\nkubectl delete secret kubernetes-dashboard-certs -n kubernetes-dashboard\n#通过新生成的证书创建secret\nkubectl create secret generic kubernetes-dashboard-certs --from-file=dashboard.key --from-file=dashboard.crt -n kubernetes-dashboard\n#删除原有pod即可（会自动创建新的pod）\nkubectl delete pod <pod name> -n kubernetes-dashboard\n\n\n#给所有namespace授权\nkubectl create serviceaccount dashboard-serviceaccount -n kubernetes-dashboard\n#创建clusterrolebinding\nkubectl create clusterrolebinding dashboard-cluster-admin --clusterrole=cluster-admin --serviceaccount=kubernetes-dashboard:dashboard-serviceaccount\n#获取token\nkubectl get secret -n kubernetes-dashboard | grep dashboard-serviceaccount-token\nkubectl describe secret <secret name>\n\n访问https://IP:之前自定义的端口,输入上一步获取的token即可登录看到相关信息\n```\n","tags":["K8S","容器"],"categories":["devops"]},{"title":"中國風電音","url":"/posts/56d9cde9.html","content":"<!-- block -->\n>&emsp;&emsp;沉迷电音的日子\n<!-- block -->\n{% meting \"2539399807\" \"netease\" \"playlist\" \"autoplay \" \"mutex:true\" \"listmaxheight:500px\" \"preload:auto\" \"theme:#ad7a86\"  \"loop:all\" \"order:random\" %}","tags":["中國風電音"],"categories":["playlist"]},{"title":"日劇OST","url":"/posts/d4f450e9.html","content":"<!-- block -->\n>&emsp;&emsp;为什么日劇OST总这么上头？\n<!-- block -->\n{% meting \"3169187100\" \"netease\" \"playlist\" \"autoplay \" \"mutex:true\" \"listmaxheight:500px\" \"preload:auto\" \"theme:#ad7a86\"  \"loop:all\" \"order:random\" %}","tags":["日劇OST"],"categories":["playlist"]}]